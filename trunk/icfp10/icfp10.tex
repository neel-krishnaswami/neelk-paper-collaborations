\documentclass{article}
\usepackage{mathpartir}
\usepackage{amsmath}
\usepackage{amssymb}

\title{The Linear Logic of GUIs}
\author{Neelakantan R. Krishnaswami \\ Nick Benton}

\input{defs.tex}
\newcommand{\Approx}[2][n]{\left\lfloor{#2}\right\rfloor^{#1}}
\newcommand{\lolli}{\multimap}
\begin{document}

\section{Introduction}

The thesis of this paper is that functional reactive programming has not taken the 
categorical structure of causal functions seriously enough, and by doing so we win 
along many axes. 

\begin{enumerate}
\item We get a much more flexible API than arrow-style FRP interfaces
  permit, while still ruling out the possibility of space/time leaks.

\item This interface still has a very rich equational theory, and a
  semantics in terms of causal functions (generalized to higher-order). 

\item Nevertheless, we can give an efficient imperative implementation of the 
  interface, which allows using well-known techniques for ``retained-mode''
  style interfaces (i.e., DOM/scenegraph) to graphics APIs.  

\item Furthermore, we can prove the correctness of this implementation, using
  Hoare logic and a logical relation between the implementation and the semantics. 
\end{enumerate}

\section{Semantics of Causal Stream Functions}

Given a function on streams $f : \stream{A} \to \stream{B}$, we say
that $f$ is causal when it is the case that the first $n$ outputs of
$f$ are determined by the first $n$ inputs. 

What we would like to do is to generalize this definition, so that we
can freely talk about streams, values, delayed streams, and function
types over these streams, while still respecting causality.

To do this, we'll interpret streams within the category of complete
1-bounded ultrametric spaces. An \emph{ultrametric space} $(X, d)$ is
a set $X$, together with a metric function $d : X \times X \to \mathbb{R}$,
 which satisfies the following axioms:

 \begin{enumerate}
   \item $d(x, y) = 0$ if and only if $x = y$
   \item $d(x, y) = d(y, x)$
   \item $d(x, z) \leq \max \setof{d(x, y), d(y, z)}$
   \item $d(x, y) \in [0,1]$
 \end{enumerate}
 
This category is cartesian closed, with the product being given by the
product of the two underlying sets, with a distance between two points
$d_{X \times Y}((x, y), (x', y')) = \max \setof{d_X(x, x'), d_Y(y,
  y')}$. The exponential is given by function space on the underlying
sets, together with the ``sup-metric'': $d_{A \to B}(f, g) = \sup
\comprehend{d_B(f\;x, g\;x)}{x \in A}$. 

We can embed any set $X$ into this category with the trivial metric
$d_{C(X)}(x, x') = \IfThenElse{x = x'}{1}{0}$.

We can embed streams over $X$ (i.e., functions $\N \to X$) into this
category with the following ultrametric:

\begin{mathpar}
  d_{S(X)}(u, v) = \min \comprehend{2^{-i}}{d_X(u_i, v_i) > 0}
\end{mathpar}

The idea here is that the distance between two streams can be measured 
by the first time at which they differ from one another -- if they differ
at the 0-th time step, then we know they are different immediately and
the distance is 1, and if they are never different, then the distance is 0. 

Now, we'll check that this is a sensible generalization of causality,
by verifying that any $f : S(A) \to S(B)$ is nonexpansive if and
only if it is causal.

\begin{prop}{(Causality and Nonexpansiveness Coincide)}
For any $f : S(A) \to S(B)$, we have that $f$ is nonexpansive if
and only if it is causal.
\end{prop}

\begin{proof}
  Suppose that $f$ is nonexpansive. Now, suppose that we have two
  streams $(a, a')$ which vary in the $k+1$-th input.  What we want to
  do is show that the first $k$ values of $f\;a$ and $f\;a'$ are the 
  same. 

  Now, nonexpansiveness means that for all $a,a' \in S(C(A))$ we have
  that $d_{S(C(A))}(a, a') \leq r$ implies that $d_{S(C(B))}(f\;a,
  f\;a') \leq r$. Since $a_{k+1} \not= a'_{k+1}$, and the first $k$ 
  elements are the same, it follows that $d(a, a') = 2^{-(k+1)}$. 

  Therefore by nonexpansivenesss $d(f\;a, f\;a') \leq 2^{-(k+1)}$,
  which means that at least the first $k$ elements of $f\;a$ and
  $f\;a'$ must agree.

  Conversely, suppose that $f$ is causal, and that we have two streams
  $a$ and $a'$ such that $d(a, a') \leq r$. Now, the metric takes on
  discrete values $2^{-k}$ (or 0), and so it follows that there is a
  $k$ such that $d(a, a') = 2^{-k} \leq r$, or else the distance is
  $0$. If it is zero, then the case is trivial. Otherwise, we know
  from causality that at least the first $k$ values of $a$ and $a'$
  coincide, and so $d(f\;a, f\;a') \leq 2^{k}$, and so $d(f\;a, f\;a')
  \leq r$.
\end{proof}

\section{Towards the Category of Widgets}

Now, it seems like it that we ought to be able to describe GUIs using
causal stream functions -- after all, a GUI should just be something
with a type like $S(\mathrm{input}) \to S(\mathrm{window})$. However,
this description misses some of the structure of GUIs. Namely, window
elements are stateful and have identity -- two different buttons are
different, producing different click streams.

To model this, we'll use a version of the functor-category semantics
that Ian Stark introduced to study the $\nu$-calculus. The basic idea
is to start with $\mathcal{I}$, the category of finite sets and
injections between them. The idea is that the objects of $\mathcal{I}$
are sets of names, and the maps between them are embeddings of smaller
sets of names into larger ones. Using injections rather than subset
inclusion lets us model renaming of locations, which will be useful
when we try to give semantics to two copies of the same widget --- we
will be able to double the size of the state set, rename the locations 
in one of the widgets, and then put the two together, so that they 
continue with independent states. 

To model programs indexed by sets of names, we'll consider the functor
category $\Set^\I$. This category's objects are functors from $\I$ to
$\Set$, and its morphisms are natural transformations between these
functors.  A natural transformation $\eta : F \To G$ is an
$\I$-indexed family of maps in $\Set$, with $\eta_s : F(s) \to G(s)$
subject to a coherence condition as follows. If $f : s \to t$ in $\I$,
then $\eta_s; G(f) = F(f); \eta_t$. 

The category $\Set^\I$ inherits limits and colimits from set, which are
taken pointwise. It is also cartesian closed, with the exponential 
given as follows: 

\begin{mathpar}
  (A \To B)(s) = \Set^\I(A(s + -), B(s + -))
\end{mathpar}

The action on maps $f : s \to t$ for the exponential will have the 
type $(A \To B)(f : s \to t) : \Set^\I(A(s + -), B(s + -)) \to \Set^\I(A(t + -), B(t + -))$. 

To define it, we'll need to make use of sxsome of the properties of
injective functions. So if there is an injective function $f : s \to
t$, then $t$ is isomorphic to the set $f(s) + t'$, where $t' = t - f(s)$
(with $f(s)$ being the image of $s$ under $f$, and the minus sign 
meaning set difference). Given $f$, we'll take $\Iso(f)$ to be the 
pair of functions mediating between $t$ and $s + t'$. Specifically, 
we'll make the choice that $i : t \to f(s) + t' = \semfun{x}{\IfThenElse{x \in f^{-1}(s)}{\inl{x}}{\inr{x}}}$
and we'll take $i^{-1} : s + t' \to t = [f, id]$, where $id$ represents the 
identity embedding. Note that $f^{-1}$ is well-defined in the definition 
of $i$, since (a) it's only called within the image of $s$, and (b) 
the function $f$ is onto, so it has an inverse. 

\begin{specification}
\nextline $(A \To B)$\=$(f : s \to t) = $
\nextline\> $\lambda \eta:(\Pi r.\; A(s + r) \to B(s + r)).$
\nextline\> $\lambda r.$ 
\nextline\> \;\;$\letv{(i : t \to s + t', i^{-1} : s + t' \to t)}{\Iso(f)}{}$
\nextline\> \;\;$A(i); \eta_{t' + r}; B(i^{-1})$
\end{specification}

It's obvious that the action of $A \To B$ on identity functions is 
the identity. 

So, suppose that we have $(A \to B)(f; g : s \to u)$, with $f : s \to
t$ and $g : t \to u$. Now, take $i : s + t' \to t = [f, into]$, and $j :
t + u' \to u = [g, into]$, and $k : s + u'' \to u = [f; g, into]$. Notice
also that there is an iso from $h : t' + u' \to u'' =  [into, into]$. (Here, 
we take all the $into$ terms to just be identity embeddings.) 

We also have a 

\begin{mathpar}
  \begin{array}{lcl}
    
  \end{array}
\end{mathpar}


  







The category of GUI widgets(!) can be described in the following
way. Take $\tau$ to be the objects of some ambient category with enough
stuff (e.g., ultrametrics, sets, predomains, basically anything with 
enough limits and colimits).

Now, the category of widgets will have objects generated from the
following grammar:

\begin{mathpar}
  \begin{array}{lcl}
    A & ::= & 1 \bnfalt A \otimes B \bnfalt A \lolli B \bnfalt I(\tau) \bnfalt O(\tau) \bnfalt \window
  \end{array}
\end{mathpar}

The idea is that $I(\tau)$ stands for \emph{input ports}, and $O(\tau)$ stand
for \emph{output ports}. We will have an API with types like: 

\begin{mathpar}
  \begin{array}{l}
    \mathsf{button} : 1 \lolli \window \otimes I(\mathsf{string}) \otimes O(\mathsf{bool}) \\
    \mathsf{label}  : 1 \lolli \window \otimes I(\mathsf{string}) \\
    \mathsf{vstack} : \window \otimes \window \lolli \window \\
    \mathsf{hstack} : \window \otimes \window \lolli \window \\
    \mathsf{connect} : I(\alpha) \otimes O(\alpha) \lolli 1 \\
  \end{array}
\end{mathpar}

So creating a button will create a window, an input port (from which
the button will read its label), and an output port (into which the
button will emit clicks). The linear discipline of the API enforces
the property that every widget gets wired up the way it expects.

So what are the morphisms? We've made a big deal about state, so it 
should not be surprising that the morphisms will be \emph{state machines}. 
First, I'll give an interpretation of the types: 

\begin{mathpar}
  \begin{array}{lcl}
    \interp{1}           & = & \top \\
    \interp{A \otimes B} & = & \interp{A} \land \interp{B} \\
    \interp{A \lolli B}  & = & \exists L \subseteq^{\mathrm{fin}} Loc.\; \interp{A} \implies T^L \interp{B} \\
    \interp{I(\tau)}     & = & \tau \implies \exists L \subseteq^{\mathrm{fin}} Loc.\; T^L \top \\ 
    \interp{O(\tau)}     & = & \exists L \subseteq^{\mathrm{fin}} Loc.\; T^L \tau \\
    [1em]
    T^L \; \tau          & = & S^L \implies \tau \land S^L \\
  \end{array}
\end{mathpar}

So $T^L$ is our old friend, the state monad. However, today we're not
just building the Kleisli category and calling it a day. We're
explicitly tracking index sets, and interpreting types and composition
in funny ways in order to ensure that our intuition about state is 
properly respected! 

What are the identity morphisms?

\begin{mathpar}
\begin{array}{lcl}
  id_A & = & (\emptyset, \fun{a}{A}{\fun{\unit}{1}{\pair{a}{\unit}}})
\end{array}
\end{mathpar}

What is function composition?

\begin{specification}
  \nextline $(L_1, f) \circ (L_2, g) = ($\=$L_1 \cup L_2,$ 
  \nextline \> $\lambda (a,b).\;\lambda h:S^{L_1 \cup L_2}.\;$\=
                  $\letv{h_1}{h|_{L_1}}{}$
  \nextline \> \> $\letv{(c, h'_1)}{g\;a\;h'_1}{}$ 
  \nextline \> \> $\letv{h'}{[h|h'_1]}{}$ 
  \nextline \> \> $\letv{h_2}{h'|_{L_2}}{}$ 
  \nextline \> \> $\letv{(d, h'_2)}{f\;b\;h_2}{}$
  \nextline \> \> $\letv{h''}{[h'|h'_2]}{}$ 
  \nextline \> \> $((c,d), h')$)
\end{specification}

What is the tensor map?

\begin{specification}
  \nextline $(L_1, f) \otimes (L_2, g) = ($\=$L_1 + L_2, $
\end{specification}

Basically, this composes two machines sequentially, (a) taking the
internal state of the composed machine to be the union of the states
of each individual machine, and (b) threading the overlapped state
first through the first machine, and then through the second machine.

\section{Logical Relation on the Implementation}

All this builds on the stuff in our TLDI paper. In particular, I take
all the apparatus of abstract semantics of notifications and so on as 
given. 

\subsection{Something That Almost, But Doesn't Quite, Work}

The basic idea here is to start by saying how a stream is realized in 
terms of notifications, and likewise how stream transducers are realized. 

A stream $v : \stream{X}$ is realized by a cell $\hat{v} :
\celltype{X}$. The idea is that at each time step $i$, we read the
cell $\hat{v}$ to get the value of the stream $v_i$. Now, a cell reads
and modifies a bunch of state --- both other cells and local state --
so we also need to specify what it reads and how it changes it. To 
do this, we give two functions $\phi, \psi : \N \to \mathsf{formula}$. 
These two functions describe the pre- and the post-state at each 
time step $i$. 

(Now, you might ask why we don't simply have a \emph{single} function
$\psi$, and require that the post-state of $\psi_i$ be $\psi_{i+1}$,
and thereby capture the time-evolution of the system. The answer is
that this captures some, but not all, sorts of signal. For example, we
want to model events (such as button clicks) via the external event
loop modifying some cells. Having both $\psi$ and $\phi$ lets us
specify the case where we want to place the responsibility for change
outside the reevaluation mechanism. 

Furthermore, the changes to the state the cell reads can have an
impact on the rest of the heap, via the ramifications. So we need to
summarize this information, as well, which we do with a function $d :
\cellset$.  This set essentially describes the cells which both the
current stream and the rest of the network may depend on in common. 

So we can give the logical relation between the a mathematical stream and a
bit of code as follows: 

\begin{mathpar}
R_{\stream{X}}(\phi, \psi, u, v, \hat{v}) \triangleq 
  \forall i.\; \exists u.\; \astep{\phi_i}{\readcell{\hat{v}}}{\psi_i}{v_i}{\setof{\hat{v}}}{u}  
    \land u \subseteq v \cup \domain{\phi_i}
\end{mathpar}

Note that this means we are promising to read each cell on each time
step at least once. This is essential, since we can have local state
in our network -- for example, an accumulator needs to sum the value
of its input on \emph{every} time step, and if we fail to read the
cell we can fail to increment the sum on that step.

The way we will model functions $f : A \lolli B$ is via the use of an
imperative program which, when given an argument, builds a new piece of
dataflow graph for computing the function. So the representation type
is $\interp{A \lolli B} = \interp{A} \to \monad{\interp{B}}$.

\begin{mathpar}
R_{A \lolli B}(\phi, \phi, u, f, \hat{f}) \triangleq 
  \forall \alpha, \beta, a, \hat{a}.\; R_A(\alpha, \beta, u, a, \hat{a}) \implies 
     B_B(\alpha, \beta, u, f\;a, \hat{f}\;\hat{a}) 
\\ \\
B_A(\phi, \psi, u, v, c) \triangleq \forall \omega.\; \spec{H(\omega)}{c}{\mathit{a}}{\exists \theta.\; H(\omega \otimes \theta_0) \land R_A(\phi \otimes \theta, \psi \otimes \theta^+, u, v, \mathit{a})}
\end{mathpar}

First, note that we maintain the internal invariant that our functions
will never depend on the temporal behavior of their components. That
is, each call to such a function may build some new graph structure,
but it won't depend on the values in the cells of its arguments. This is
why the abstract state arguments are equal -- a function does not look
at the abstract state at all, and so it will work in any abstract state,
and won't change it. 

The purpose of this invariant is basically to let us call the function
whenever we want -- we don't want to have to put dataflow constraints
on calling functions. This may be worth relaxing later on, though.

That said, we then get to the specification. It says that given an 
input $\hat{a}$ that uses $\alpha$ and $\beta$ to realize a value 
$a$, then $\hat{f}\hat{a}$ will build a network using $\alpha$ and $\beta$
to realize $f\;a$. By ``build a network'', we mean the predicate $B_A$, 
defined on the next line. There, the command $c$ builds a network to 
realize $v$, when executing it (in any heap) results in some new 
graph structure $\theta_i$ which it will use together with $\alpha$
to compute values. Note that here is where we make use of the idea 
that evaluating the node takes the network from time $i$ to time
$i+1$ --- the constructed structure starts at state $\theta_0$, and
at each timestep it goes from $\theta_i$ to $\theta^+_{i} = \theta_{i+1}$. 

Finally, let's quickly look at how we handle pairs and units. 

A tensor type $A \otimes B$ is just realized by a pair $\interp{A}
\times \interp{B}$.

\begin{mathpar}
  R_{A \otimes B}(\phi, \psi, u, \pair{a}{b}, (\hat{a}, \hat{b})) \triangleq
    \exists \theta_A, \theta_B.\; 
      \begin{array}{lclc}
        R_A(\phi, \theta_A, u, a, \hat{a}) & \land & R_B(\theta_A, \psi, u, b, \hat{b}) & \land \\
        R_B(\phi, \theta_B, u, b, \hat{b}) & \land & R_A(\theta_B, \psi, u, a, \hat{a}) &  \\
      \end{array}
\end{mathpar}

This says that the pair $\pair{a}{b}$ is realized by a pair of values
$(\hat{a}, \hat{b})$.  The two lines of the definition assert that
regardless of the order in which we observe them, we ``end up in the
same place''. The unit is the unit of the tensor, so it is of the form: 

\begin{mathpar}
  R_{1}(\phi, \psi, u, \unit, ()) \triangleq (\phi = \psi)
\end{mathpar}

\subsubsection{Verification of Some Simple Programs}

Here's some examples. 

\begin{theorem}{(Mapping)}
  Define $\hat{map} f = \semfun{x}{\newcell (\bind (\readcell x)\; (\return \circ f))}$. 
Then we have that $R_{S(X) \lolli S(Y)}(\phi, \phi, \emptyset, \mathit{map}\;f, \hat{\mathit{map}}\;f)$,
for $f : X \to Y$. 
\end{theorem}

\begin{theorem}{(Eval)}
  Define $\hat{\mathit{eval}} = \semfun{\pair{f}{x}}{f(x)}$. Then we have that 
$R_{((A \lolli B) \otimes A) \lolli B}(\phi, \phi, \emptyset, \mathit{eval}, \hat{\mathit{eval}})$. 
\end{theorem}

\begin{proof}
  These are all just pretty much a matter of unwinding the definitions and seeing that 
everything is kosher. I'm too lazy to transcribe the proofs from the whiteboard to \LaTeX,
since we're going to chuck everything out soon. 
\end{proof}

\subsubsection{What Doesn't Work}

What we'd really like to do is to support a fixed point operator on this category. That
means that we need an operation with a type like 

\begin{mathpar}
  \mathsf{trace} : (A \otimes S(X) \lolli B \otimes S(X)) \lolli A \lolli B 
\end{mathpar}

The only trouble is that 

\end{document}
