%
% check related work (don't redo description of HTT, e.g.)
% search for Idealized ML
% search for XXX
%
%-----------------------------------------------------------------------------
%
%               Template for LaTeX Class/Style File
%
% Name:         sigplanconf-template.tex
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[preprint,natbib]{sigplanconf}

\usepackage{amsmath}
\usepackage{amssymb}
\input{commands.tex}

\begin{document}

\conferenceinfo{TLDI '09}{Savannah, GA.} 
\copyrightyear{2009} 
\copyrightdata{[to be supplied]} 

\titlebanner{Draft Copy}        % These are ignored unless
% \preprintfooter{short description of paper}   % 'preprint' option specified.

\title{Design Patterns in Separation Logic}
% \subtitle{Subtitle Text, if any}

\authorinfo{Neelakantan R. Krishnaswami \and Jonathan Aldrich}
           {Carnegie Mellon University}
           {\{neelk,jonathan.aldrich\}@cs.cmu.edu}
\authorinfo{Lars Birkedal \and Kasper Svendsen \and Alexandre Buisse}
           {IT University of Copenhagen}
           {\{birkedal,kasv,abui\}@itu.dk}

\maketitle

\begin{abstract}
Object-oriented programs are notable for making use of both
higher-order abstractions and mutable, aliased state. Either feature
alone is challenging for formal verification, and the combination
yields very flexible program designs and correspondingly difficult
verification problems. In this paper, we show how to
formally specify and verify programs that use several common design
patterns in concert.
\end{abstract}

\category{F.3}{Logics and Meanings of Programs}{Specifying and Verifying and Reasoning about Programs}
%A category including the fourth, optional field follows...
\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

\terms{Separation Logic, Design Patterns, Formal Verification}

% \keywords
% keyword1, keyword2

\section{Introduction}

The widespread use of object-oriented languages creates an opportunity
for designers of formal verification systems, above and beyond a
potential ``target market''. Object-oriented languages have been used
for almost forty years, and in that time practitioners have developed
a large body of informal techniques for structuring object-oriented
programs called \emph{design patterns}\cite{GoF}.  Design patterns
were developed to both take best advantage of the flexibility
object-oriented languages permit, and to control the potential
complexities arising from the unstructured use of these features.

This pair of characteristics make design patterns an excellent set of
benchmarks for a program logic.  First, design patterns use higher
order programs to manipulate aliased, mutable state. This is a
difficult combination for program verification systems to handle, and
attempting to verify these programs will readily reveal weaknesses or
lacunae in the program logic. Second, the fact that patterns are
intended to structure and modularize programs means that we can use
them to evaluate whether the proofs in a program logic respect the
conceptual structure of the program -- we can check to see if we need
to propagate conceptually irrelevant information out of program
modules in order to meet our proof obligations. Third, we have the
confidence that these programs, though small, actually reflect
realistic patterns of usage.

In this paper, we describe Idealized ML, a core higher-order
imperative language, and a specification language based on separation
logic for it. Then, we give good specifications for and verify the
following programs:

\begin{itemize}
\item We prove a collection and iterator implementation, which builds
  the Java aliasing rules for iterators into its specification, and
  which allows the construction of new iterators from old ones via the
  composite and decorator patterns.  

\item We prove a general version of the flyweight pattern (also known as
  hash-consing in the functional programming community), which is a
  strategy for aggressively creating aliased objects to save memory
  and permit fast equality tests. This also illustrates the use of the
  factory pattern.

\item We prove a general version of the subject-observer pattern in a
  way that supports a strong form of information hiding between the
  subject and the observers.
\end{itemize}

Finally, we give machine-verified proofs of the correctness of the
iterator and flyweight patterns in the Ynot extension of Coq, and
compare them with the paper proofs. We also see that proper treatment
of the subject-observer pattern seems to call for the use of an
impredicative type theory.


\begin{figure*}
\renewcommand{\baselinestretch}{0.9}
\begin{displaymath}
\begin{array}{llcl}  
\mbox{Types} & A & ::= & 1 \bnfalt A \to B \bnfalt \listtype{A} \bnfalt \opttype{A} \bnfalt  \N \bnfalt \reftype{A} \bnfalt \monad{A}\\ 

\mbox{Pure Terms} 
& e & ::= & \unit \bnfalt x \bnfalt e\;e' \bnfalt \fun{x}{A}{e} \bnfalt
            \ctext{Nil} \bnfalt \ctext{Cons}(e,e') \bnfalt 
            \ctext{None} \bnfalt \ctext{Some}(e) \\
&   & |   & \listcase{e}{e'}{x}{xs}{e''} \bnfalt
            \optcase{e}{e'}{x}{e''}\\ 
&   & |   & \z \bnfalt \s{e} \bnfalt \iter{e}{e_z}{x}{e_s} \bnfalt
            \fix{}{e} \bnfalt \comp{c} \\
\mbox{Computations} 
& c & ::= & e \bnfalt \letv{x}{e}{c} \bnfalt \run{e}  \bnfalt
            \newref{A}{(e)}\bnfalt !e \bnfalt e := e' \\
\end{array}
\end{displaymath}
\caption{Types and Syntax of the Programming Language}
\label{lang-syntax}
\renewcommand{\baselinestretch}{0.95}
\end{figure*}

\section{Formal System}
% \vspace{-1em}

The formal system we present has three layers. First, we have a core
programming language we call Idealized ML. This is a simply-typed
functional language which isolates all side effects inside a monadic
type~\cite{pfenning-davies}. The side effects include nontermination
and the allocation, access, and modification of general references
(including pointers to closures).  Then, we give an assertion language
based on higher-order separation logic~\cite{hosl} to describe the
state of a heap. Separation logic allows us to give a clean treatment
of issues related to specifying and controlling aliasing, and
higher-order predicates allow us to abstract over the heap. This
enables us to enforce encapsulation by hiding the exact layout of a
module's heap data structures.  Finally, we have a specification logic
to describe the effects of programs, which is a first-order logic
whose atomic propositions are Hoare triples $\spec{P}{c}{a:A}{Q}$,
which assert that if the heap is in a state described by the assertion
$P$, then executing the command $c$ will result in a postcondition
state $Q$ (with the return value of the command bound to $a$).
specifications.


\textbf{Programming Language.} The core programming language we have
formalized is an extension of the simply-typed lambda calculus with a
monadic type constructor to represent side-effecting computations.
The types of our language are the unit type $1$, the function space $A
\to B$, the natural number type $\N$, the reference type
$\reftype{A}$, as well as the option type $\opttype{A}$ and the
mutable linked list type $\listtype{A}$. In addition, we have the
monadic type $\monad{A}$, which is the type of suspended
side-effecting computations producing values of type $A$. Side effects
include both heap effects (such as reading, writing, or allocating a
reference) and nontermination.

We maintain such a strong distinction between pure and impure code for
two reasons. First, it allows us to validate very powerful equational
reasoning principles for our language: we can validate the full
$\beta$ and $\eta$ rules of the lambda calculus for each of the pure
types. This simplifies reasoning even about imperative programs,
because we can relatively freely restructure the program source to
follow the logical structure of a proof. Second, when program
expressions appear in assertions -- that is, the pre- and
post-conditions of Hoare triples -- they must be pure. However,
allowing a rich set of program expressions like function calls or
arithmetic in assertions makes it much easier to write
specifications. So we restrict which types can contain side-effects,
and thereby satisfy both requirements.
%
The pure terms of the language are typed with the typing judgement
$\judgeE{\Gamma}{e}{A}$, given in Figure~\ref{lang-typing}, and which
can be read as ``In variable context $\Gamma$, the pure expression $e$
has type $A$.'' Computations are typed with the judgement
$\judgeC{\Gamma}{c}{A}$ (also in Figure~\ref{lang-typing}), which can
be read as ``In the context $\Gamma$, the computation $c$ is
well-typed at type $A$ .''

We have $\unit$ as the inhabitant of $1$, natural numbers $\z$ and
$\s{e}$, functions $\fun{x}{A}{e}$, optional expressions
$\ctext{None}$ and $\ctext{Some }e$, and list expressions
$\ctext{Nil}$ and $\ctext{Cons}(e,e')$. $\ctext{Nil}$ is the
constructor for an empty list, and $\ctext{Cons}(e, e')$ is the
constructor for a cons-cell.  In Figure~\ref{lang-typing},
\textsc{EListCons} states the tail of a list is of \emph{reference} type
$\reftype{\listtype{A}}$, which means that our list type is a type of
\emph{mutable} lists. We also have the corresponding eliminations for
each type, including case statements for option types and list
types. For the natural numbers, we add a primitive recursion construct
$\iter{e}{e_z}{x}{e_s}$. If $e = \z$, this computes $e_z$ , and if $e
= \s{e'}$, it computes $e_s[(\iter{e'}{e_z}{x}{e_s})/x]$.  This
bounded iteration allows us to implement (for example) arithmetic
operations as pure expressions.

Suspended computations $\comp{c}$ inhabit the monadic type
$\monad{A}$.  These computations are not immediately evaluated, which
allows us to embed them into the pure part of the programming
language. Furthermore, we can take fixed points of monad-valued
functions (\textsc{EFix}), which gives us a general recursion
facility. (We restrict $\ctext{fix}$ to monad-valued domains because
of the possibility of nontermination. Also, we will write recursive
functions as syntactic sugar for $\ctext{fix}$.)  As \textsc{CPure}
shows, we can treat any pure expression of type $A$ as a computation
that coincidentally has no side-effects.  In \textsc{CRun}, a
suspended computation of type $\monad{A}$ can be forced to execute
with the $\run{e}$ command.

In \textsc{CLet}, we have a sequential composition
$\letv{x}{e}{c}$. Intuitively, the behavior of this command is as
follows. We evaluate $e$ until we get some $\comp{c'}$, and then
evaluate $c'$, modifying the heap and binding its return value to
$x$. Then, in this augmented environment, we run $c$. The fact that
monadic commands have return values explains why our sequential
composition is also a binding construct. Finally, we have computations
$\newref{A}{(e)}$, $!e$, and $e := e'$, which let us allocate, read
and write references, respectively.

This language has been given a typed denotational semantics, which for
space reasons we do not give here. The details of the semantics
(including the assertion and specification levels) can be found in the
companion tech report~\cite{tech-report}.
% % % \vspace{-2em}

\begin{figure*}
\begin{mathpar}
% \inferrule*[right=EVar]
%           {x:A \in \Gamma}
%           {\judgeE{\Gamma}{x}{A}}
% \and
% \inferrule*[right=ELam]
%           {\judgeE{\Gamma, x:A}{e}{B}}
%           {\judgeE{\Gamma}{\fun{x}{A}{e}}{A \to B}}
% \and
% \inferrule*[right=EApp]
%           {\judgeE{\Gamma}{e_1}{B \to A} \\
%            \judgeE{\Gamma}{e_2}{B}}
%           {\judgeE{\Gamma}{e_1\;e_2}{A}}
% \and
% \inferrule*[right=EUnit]
%           { }
%           {\judgeE{\Gamma}{\unit}{1}}
% \and
% \inferrule*[right=EListNil]
%           { }
%           {\judgeE{\Gamma}{\ctext{Nil}}{\listtype{A}}}
% \and
\inferrule*[right=EListCons]
          {\judgeE{\Gamma}{e}{A} \\
           \judgeE{\Gamma}{e'}{\reftype{\listtype{A}}}}
          {\judgeE{\Gamma}{\ctext{Cons}(e, e')}{\listtype{A}}}
\and
% \inferrule*[right=EListCase]
%           {\judgeE{\Gamma}{e}{\listtype{A}} \\
%            \judgeE{\Gamma}{e'}{B} \\
%            \judgeE{\Gamma, h:A, t:\reftype{\listtype{A}}}{e''}{B}}
%           {\judgeE{\Gamma}{\listcase{e}{e'}{h}{t}{e''}}{B}}
% \and
% \inferrule*[right=EOptNone]
%           { }
%           {\judgeE{\Gamma}{\ctext{None}}{\opttype{A}}}
% \and
% \inferrule*[right=EOptSome]
%           {\judgeE{\Gamma}{e}{A}}
%           {\judgeE{\Gamma}{\ctext{Some}\;e}{\opttype{A}}}
% \and
% \inferrule*[right=EOptCase]
%           {\judgeE{\Gamma}{e}{\opttype{A}} \\
%            \judgeE{\Gamma}{e'}{B} \\
%            \judgeE{\Gamma, x:A}{e''}{B}}
%           {\judgeE{\Gamma}{\optcase{e}{e'}{x}{e''}}{B}}
% \and
\inferrule*[right=EMonad]
          {\judgeC{\Gamma}{c}{A}}
          {\judgeE{\Gamma}{\comp{c}}{\monad{A}}}
\and
\inferrule*[right=EFix]
          {\judgeE{\Gamma}{e}{(A \to \monad{B}) \to (A \to \monad{B})}}
          {\judgeE{\Gamma}{\fix{}{e}}{A \to \monad{B}}}
\\
\inferrule*[right=CLet]
          {\judgeE{\Gamma}{e}{\monad{A}} \\
           \judgeC{\Gamma,x:A}{c}{B}}
          {\judgeC{\Gamma}{\letv{x}{e}{c}}{B}}
\and
\inferrule*[right=CPure]
          {\judgeE{\Gamma}{e}{A}}
          {\judgeC{\Gamma}{e}{A}}
\and
\inferrule*[right=CRun]
          {\judgeE{\Gamma}{e}{\monad{A}}}
          {\judgeC{\Gamma}{\run{e}}{A}}
\and
\inferrule*[right=CRefNew]
          {\judgeE{\Gamma}{e}{A}}
          {\judgeC{\Gamma}{\newref{A}{(e)}}{\reftype{A}}}
\and
\inferrule*[right=CRefRead]
          {\judgeE{\Gamma}{e}{\reftype{A}}}
          {\judgeC{\Gamma}{!e}{A}}
\and
\inferrule*[right=CRefWrite]
          {\judgeE{\Gamma}{e}{\reftype{A}} \\
           \judgeE{\Gamma}{e'}{A}}
          {\judgeE{\Gamma}{e := e'}{1}}
\end{mathpar}
\caption{Selected Typing Rules}
\label{lang-typing}
\end{figure*}



\textbf{Assertion Language.} The sorts and syntax of the assertion
language are given in Figure~\ref{assert-syntax}. The assertion
language is a version of separation logic, extended to higher order.

% \renewcommand{\baselinestretch}{0.95}
In ordinary Hoare logic, a predicate describes a set of program states
(in our case, heaps), and a conjunction like $p \land q$ means that a
heap in $p \land q$ is in the set described by $p$ and the described
by $q$. While this is a natural approach, aliasing can become quite
difficult to treat -- if $x$ and $y$ are pointer variables, we need to
explicitly state whether they alias or not. This means that as the
number of variables in a program grows, the number of aliasing
conditions grows quadratically. 
% Worse still, this can defeat modular
% proof, since as soon as we put a subprogram into a larger one, we need
% to add aliasing assertions describing possible interference between
% the subprogram and the larger program.
With separation logic, we add the \emph{spatial} connectives to
address this difficulty. A separating conjunction $p * q$ means that
the state can be broken into two \emph{disjoint} parts, one of which
is in the state described by $p$, and the other of which is in the
state described by $q$. The disjointness property makes the
noninterference of $p$ and $q$ implicit. This avoids the unwanted
quadratic growth in the size of our assertions. In addition to the
separating conjunction, we have its unit $\emp$, which is true of the
empty heap, and the points-to relation $e \pointsto e'$, which holds
of the one-element heap in which the reference $e$ has contents
$e'$. A heap is described by the ``magic wand'' $p \wand q$, when we
can merge it with any disjoint heap described by $p$, and the
combination is described by $q$.

% \renewcommand{\baselinestretch}{1}
The universal and existential quantifiers $\forall x:\omega.\;p$ and
$\exists x:\omega.\;p$ are higher-order quantifiers ranging over all
sorts $\omega$. The sorts include the language types $A$, the sort of
propositions $\assert$, function spaces over sorts $\omega \To
\omega'$, and mathematical sequences $\seqsort{\omega}$. Constructors
for terms of all these sorts in the syntax given in
Figure~\ref{assert-syntax}. For the function space, we include
lambda-abstraction and application. For sequence sorts, we have
sequence-formers $\epsilon$ for the empty sequence and $p \cdot ps$
for adding one element to a sequence, as well as a primitive iteration
construct over sequences,
$\iterseq{p}{p_\epsilon}{(x,acc)}{p_\cdot}$. With iteration, we can
write constructor-level map and filter functions (as in functional
programming) and use them in our specifications. Our examples will
also make use of finite sets and functions, without formalizing them
in our syntax.

Finally, we include the atomic formulas $\validprop{S}$, which are
\emph{assertions} that a \emph{specification} $S$ holds. This facility
is useful when we write assertions about pointers to code -- for
example, the assertion $r \pointsto cmd$ $\land$
$(\spec{p}{\run{cmd}}{a:A}{q})$ $\valid$ says that the reference $r$
points to a monadic term $cmd$, whose behavior is described by the
Hoare triple $\spec{p}{\run{cmd}}{a:A}{q}$.

% % \vspace{-2em}
\begin{figure}
\begin{displaymath}
\begin{array}{llcl}
\mbox{Assertion Sorts} & 
\omega & ::= & A \bnfalt \omega \To \omega \bnfalt \seqsort{\omega} 
               \bnfalt \assert 
\\[0.5em]
\mbox{Assertion} & 
p & ::= & e \bnfalt x \bnfalt \fun{x}{\omega}{p} \bnfalt p\;q \\
\mbox{Constructors}& &  |  & \iter{p}{p_{\epsilon}}{(x,acc)}{p_\cdot} 
                             \bnfalt \epsilon \bnfalt p \cdot ps \\ 
& &  |  & \top \bnfalt p \land q \bnfalt p \implies q 
          \bnfalt \bot \bnfalt p \vee q \\
& &  |  &  \emp \bnfalt p * q \bnfalt p \wand q \bnfalt e \pointsto e' \\
& &  |  & \forall x:\omega.\; p \bnfalt \exists x:\omega.\; p \bnfalt
          \validprop{S} 
\\[0.5em]
\mbox{Specifications} &
S & ::= & \spec{p}{c}{a:A}{q} \bnfalt \setof{p} \\
& &  |  & S \specand S' \bnfalt S \specimp S' \bnfalt S \specor S' \\
& &  |  & \forall x:\omega.\; S \bnfalt \exists x:\omega.\;S 
\\
\end{array}
\end{displaymath}
\caption{Syntax of Assertions and Specifications}
\label{assert-syntax}
\end{figure}
% % \vspace{-3em}
\textbf{Specification Language.} Given programs and assertions about
the heap, we need specifications to relate the two. We begin with the
Hoare triple $\spec{p}{c}{a:A}{q}$. This specification represents the
claim that if we run the computation $c$ in any heap the predicate $p$
describes, then if $c$ terminates, it will end in a heap described by
the predicate $q$. Since monadic computations can return a value in
addition to having side-effects, we add the binder $a:A$ to the third
clause of the triple to let us name and use the return value in the
postcondition.

We then treat Hoare triples as one of the atomic proposition forms of
a first-order intuitionistic logic (see
Figure~\ref{assert-syntax}). The other form of atomic proposition are
the specifications $\setof{p}$, which are $specifications$ saying that
an \emph{assertion} $p$ is true. These formulas are useful for
expressing aliasing relations between defined predicates, without
necessarily revealing the implementations. In addition, we can form
specifications with conjunction, disjunction, implication, and
universal and existential quantification over the sorts of the
assertion language. 

% For example, we give a simple specification for a hash table module
% below:
% 
% {\small
% \begin{tabbing}
% 1 \qquad \= $\exists table : A_t \times (B \to^{fin} C) \To \assert.$ \\
% 2 \> $\exists$\=$ \ctext{newtable} : 1 \to \monad{A_t}, \;\;
%               \ctext{update} : A_t \times B \times C \to \monad{1}, \;\;
%               \ctext{lookup} : A_t \times B \to \monad{(\opttype{C})}.$
% \\[0.5em]
% 5 \> $\spec{\emp}{\ctext{newtable}()}{a:A_t}{table(a, [])}$ \\
% 6 \> $\specand$ \\
% 7 \> $\forall t, f, b, c.\;$\=$\setof{table(t, f)}$ \\
% 8 \> \> $\run{\ctext{update}(t, b, c)}$ \\
% 9 \> \> $\setof{a:1. table(t, [f|b:c])}$ \\
% 10 \> $\specand$ \\
% 11 \> $\forall t, b, f.\; $\=$\setof{table(t, f)}$\\
% 12 \> \>  $\run{\ctext{lookup}(t, b)}$ \\
% 13 \> \>  $\{$\=$a:\opttype{C}.\;$ \\
% 14 \> \> \>     $table(t, fn) \land ($\=$(b \in \mbox{dom}(f) \land a = \ctext{Some }f(b)) \vee$ \\
% 15 \> \> \> \> $(b \not\in \mbox{dom}(f) \land a = \ctext{None})\}$\\
% \end{tabbing}
% }
% % % \vspace{-1.5em}
% In line 1, we assert the existence of a predicate $table(t,f)$, which
% describes the region of heap owned by the hash table $t$, and which
% currently implements the finite function $f$. Then we assert the
% existence of functions $\ctext{newtable}$, $\ctext{update}$, and
% $\ctext{lookup}$. On line 5, we give a triple asserting that calling
% $\ctext{newtable}$ produces a new, empty $table$ object (we write $[]$
% for an empty finite function). On lines 7-9, we give a triple
% asserting that $\ctext{update}(t, b, c)$ will update the finite
% function in $table(t,f)$ to one that is the same, except that it maps
% $b$ to $c$. And on lines 11-15, we give a triple asserting that
% $\ctext{lookup}(t,b)$ will return $\ctext{None}$ if $b$ is not in the
% domain of the table's finite function, and will return the appropriate
% value if it is.  All of these triples are combined together into one
% specification using conjunction of specifications $S \specand S'$.

Having a full logic of triples also lets us express program modules as
formulas of the specification logic. We can expose a module to a
client as a collection of existentially quantified functions
variables, and provide the client with Hoare triples describing the
behavior of those functions. Furthermore, modules can existentially
quantify over predicates to grant client programs access to module
state without revealing the actual implementation. A client program
that uses an existentially quantified specification cannot depend on
the concrete implementation of this module, since the existential
quantifier hides that from it -- for example, we can expose a
$table(t, map)$ predicate that does not reveal whether a hash table is
implemented with single or double hashing.
 
% Also, we can represent client dependencies within our logic using
% implication over specifications. If a client program uses a table
% module $T$, we can prove it separately from the implementation by
% proving that the client satisfies the specification $T \specimp
% S$. Then, given any implementation satisfying $T$, we can use modus
% ponens to conclude that the whole program satisfies $S$.
% 
% Specification implications also lets us more easily specify the
% behavior of higher-order imperative functions. For example, consider
% the $repeat$ function of type $(\N \times \monad{1}) \to \monad{1}$,
% which takes a natural number $n$ and a command argument $cmd$ and runs
% it $n$ times. Clearly, the specification of $repeat$ should depend on
% the specification of its argument. We can write this as:
% {\small
% \begin{tabbing}
% $\forall$\=$P, i, n, cmd.\;$\\ 
% \> $(\forall k.\; \spec{P(k)}{\run{cmd}}{a:1}{P(k+1)})$ \\
% \> $\specimp \spec{P(i)}{\run{repeat(n,cmd)}}{a:1}{P(i+n)}$ \\
% \end{tabbing}
% }
% This specification says that if $cmd$ increments a predicate $P$ by
% one step (from $P(k)$ to $P(k+1)$, then $repeat(n, cmd)$ will
% increment it by $n$ steps, which is just what we want.

% 
% \begin{figure}
% \begin{tabular}{ll}
% $\judgeE{\Gamma}{e}{A}$           & Pure Expression Typing \\
% $\judgeC{\Gamma}{c}{A}$           & Computation Typing \\
% $\judgeEqE{\Gamma}{e}{e'}{A}$     & Expression Equality \\
% $\judgeEqC{\Gamma}{c}{c'}{A}$     & Computation Equality \\
% $\judgeP{\Delta}{p}{\omega}$      & Assertion Constructor Sorting \\
% $\judgeEqP{\Delta}{p}{q}{\omega}$ & Assertion Constructor Equality \\
% $\entailsP{\Delta}{p}{p'}$        & Assertion Entailment \\
% $\judgeS{\Delta}{S}$              & Specification Well-Formedness \\
% $\entailsS{\Delta}{\Sigma}{S}$    & Specification Validity \\
% \end{tabular}
% \caption{Judgements of the Theory}
% \label{all-judgements}
% \end{figure}
% 

\section{Iterators, Composites and Decorators}
% % \vspace{-1em}
% In this section, we show how to integrate the composite, decorator and
% iterator patterns in a single module. 
% \vspace{-1em}
The iterator pattern is a design pattern for uniformly enumerating the
elements of a collection. The idea is that in addition to a
collection, we have an auxiliary data structure called the iterator,
which has an operation $\ctext{next}$. Each time $\ctext{next}$ is
called, it produces one more element of the collection, with some
signal when all of the elements have been produced. The iterators are
mutable data structures whose invariants depend on the collection,
itself another mutable data structure. Therefore, most object oriented
libraries state that while an iterator is active, a client is only
permitted to call methods on a collection that do not change the
collection state (for example, querying the size of a collection). If
destructive methods are invoked (for example, adding or removing an
element), it is no longer valid to query the iterator again.

We also support operations to create new iterators from old ones, and
to aggregate them into composite iterators. For example, given an
iterator and a predicate, we can construct a new iterator that only
returns those elements for which the predicate returns true. This sort
of decorator takes an iterator object, and \emph{decorates} it to
yield an iterator with different behavior. Likewise, we can take two
iterators and a function, and combine them into a new,
\emph{composite} iterator that returns the result of a parallel
iteration over them.  These sorts of synthesized iterators are found
in the \texttt{itertools} library in the Python programming language,
the Google Java collections library, or the C5 library~\cite{C5} for
C\#.

Aliasing enters into the picture, above and beyond the restrictions on
the underlying collections, because iterators are stateful
objects. For example, if we create a filtering iterator, and advance
the underlying iterator, then what the filtering iterator will return
may change. Even more strikingly, we cannot pass the same iterator
twice to a parallel iteration constructor -- the iterators must be
disjoint in order to correctly generate the two sequences of elements
to combine.

Below, we give a specification of an iterator pattern. We'll begin 
by describing the interface informally, in English, and then move on 
to giving formal specifications and explaining them. 

The interface consists of two types, one for collections, and one for
iterators. The operations the collection type supports are 1) creating
new mutable collections, 2) adding new elements to an existing
collection, and 3) querying a collection for its size. Adding new
elements to a collection is a detructive operation which modifies the
existing collection, whereas getting a collections size does not
modify the collection.

The interface that the iterator type supports are 1) creating a new
iterator on a collection, 2) destructively getting the next element
from an iterator (returning an error value if the iterator is
exhausted), and 3) operations to produce new iterators from old. The
iterator transformations we support are 1) a filter operation, which
takes an iterator along with a boolean predicate, and returns an
iterator which enumerates the elements satisfying the predicate, and
2) a parallel map operation, which takes two iterators and a
two-argument function, and returns an iterator which returns the
result of enumerating the elements of the two iterators in parallel,
and applying the function to each pair of elements. 

The aliasing protocol that our iterator protocol will satisfy is
essentially the same as the one the Java standard libraries specify in
their documentation.

\begin{itemize}
\item Any number of iterators can be created from a given collection.
  Each of these iterators has the collection as its underlying state.

\item An iterator constructed from other iterators has underlying 
  state consisting of its arguments, as well as their backing states. 

\item An iterator is valid as long as none of its underlying state has
  been destructively modified from the time of the iterator's creation. An
  iterators underlying state consists of any collections it depends
  on, as well as any iterators it was constructed from.

\item It is legal to call functions on an iterator only when it 
  is in a valid state. Performing a destructive operation on any 
  part of an iterator's underlying state invalidates it. For example,
  adding an element to a collection in an iterator's underlying state
  will invalidate it, as will trying to get elements from any other 
  iterators in its underlying state. 
\end{itemize}


Now, we will describe the specification in detail. The type of
collections is just the type of mutable linked lists, consisting of
pointers to list cells. (For simplicity, we only consider lists of
natural numbers.) An iterator is an element of a recursive tree
structure in the style of an ML datatype declaration. (In Java, we
would have a class hierarchy for iterators.) If it is an iterator over
a single collection, then it will be in the branch $\ctext{Coll }r$,
where $r$ is a pointer to a linked list -- a finger into the middle of
the collection. For filtering iterators, we use a constructor of the
form $\ctext{Filter}(p, i)$, where $p$ is a boolean predicate function
and $i$ is the iterator whose elements we are selectively yielding. We
give pairwise mapping iterators via a constructor $\ctext{Map2}(f,
i_1, i_2)$, which enumerates elements of $i_1$ and $i_2$ in parallel,
and applies the binary function $f$ to those pairs to produce the
yielded elements.

{\small
\begin{displaymath}
\begin{array}{llcl}
\mbox{Collection Type} & 
A_c & = & \reftype{\listtype{\N}} \\
\mbox{Iterator Type} &
A_i & = & \ctext{Coll of }\reftype{A_c} \\
&   & | & \ctext{Filter of }((\N \to \ctext{bool}) \times A_i) \\
&   & | & \ctext{Map2 of }((\N \times \N \to \N) \times A_i \times A_i) \\
\end{array}
\end{displaymath}
\begin{tabbing}
1 \qquad \= $\exists coll : A_c \times \seqsort{\N} \times \assert \To \assert.$ \\
2 \> 
$\exists iter : A_i \times \mathcal{P}^{fin}(A_c \times \seqsort{\N} \times \assert) \times \seqsort{\N} \To \assert.$ \\
3 \> 
$\exists \ctext{newcoll} : \unittype \to \monad{A_c}, \;\;
         \ctext{size} : A_c \to \monad{\N}, \;\;
         \ctext{add} : A_c \times \N \to \monad{\unittype}.$\\
4 \> 
$\exists \ctext{newiter} : A_c \to \monad{A_i}, \;\;
          \ctext{filter} : (\N \to \ctext{bool}) \times A_i  \to \monad{A_i}.$ \\
5 \> 
$\exists$\=$ \ctext{map2} : (\N \times \N \to \N) \times A_i \times A_i \to \monad{A_i}.$\\
  \>$\exists$\=$\ctext{next} : A_i \to \monad{(\opttype{\N})}.$  \\[0.5em]

6 \>
\> $\spec{\emp}{\run{\ctext{newcoll()}}}{a:A_c}{\exists P.\; coll(a, \epsilon,P)}$ \\
\>
\> $\specand$ \\
7 \>
\> $\forall c, P, xs.\;$\=
         $\setof{coll(c, xs, P)}$\\
\>\> \>  $\run{\ctext{size}(c)}$ \\
\>\> \>  $\setof{a:\N.\; coll(c, xs, P) \land a = |xs|}$ \\
\> 
\> $\specand$ \\
8 \> \> $\forall c, P, x, xs.\;$\=
               $\setof{coll(c, xs, P)}$ \\
\>\>\>         $\run{\ctext{add}(c, x)}$\\
\>\>\>         $\setof{a:1.\; \exists Q.\; coll(c, x\cdot xs, Q)}$ \\
\>
\> $\specand$ \\
9 \>
\> $\forall c, P, xs.\;$\=
      $\setof{coll(c, xs, P)}$ \\
\>\>\>$\run{\ctext{newiter}(c)}$\\
\>\>\>$\setof{a:A_i.\; coll(c, xs, P) * iter(a, \setof{(c, xs, P)}, xs)}$ \\
\>
\> $\specand$ \\
10 \>
\> $\forall p, i, S, xs.\;$\=
         $\setof{iter(i, S, xs)}$ \\
\>\>\>   $\run{\ctext{filter}(p, i)}$\\
\>\>\>   $\setof{a:A_i.\; iter(a, S, filter\; p\;xs)}$ \\
 \> 
\> $\specand$ \\
 \> 
\> $\forall f, i, S,$\=$ xs, i', S', xs'.\;$ \\
11 \> \> \> 
     $\setof{iter(i, S, xs) * iter(i', S', xs') \land S \cap S' = \emptyset}$ \\
12 \> \> \> $\run{\ctext{map2}(f, i, i')}$ \\
13 \> \> \> $\setof{a:A_i.\; iter(a, S \cup S', map\;f\;(zip\;xs\;xs'))}$ \\
\>
\> $\specand$ \\
14 \>
\> $\forall i, S.\;$\=
      $\setof{colls(S) * iter(i, S, \epsilon)}$\\  
\>\>\>$\run{\ctext{next}(i)}$ \\
\>\>\>$\setof{a:\opttype{\N}.\; colls(S) * iter(i, S, \epsilon) \land a = \ctext{None}}$ \\
\>
\> $\specand$ \\
15 \> \> $\forall i,$\=$ S, x, xs.\;$ \= 
      $\setof{colls(S) * iter(i, S, x \cdot xs)}$\\
\>\>\>$\run{\ctext{next}(i)}$\\
\>\>\>$\setof{a:\opttype{\N}.\; 
              colls(S) * iter(i, S, xs) \land a = (\ctext{Some}\;x)}$ 
\\[0.5em]
$colls(\emptyset) \qquad\qquad\qquad\;\;$ \=$\equiv\;$\= $\emp$ \\
$colls(\setof{(c,xs,P)} \cup S)$ \> $\equiv$ \> $coll(c, xs, P) * colls(S)$ \\
\end{tabbing}
}


In this specification, the predicate $coll(c, xs, P)$ (on line 1) is a
three place predicate describing the state of a collection. The first
argument $c$ names the collection object that owns this state in the
heap. The second argument, $xs$, is the abstract sequence that the
collection $c$ currently represents. As $c$ is mutated, $xs$ can
change. The final argument, $P$, represents the \emph{abstract state}
of the collection. We use this argument to track whether an operation
that uses $c$ makes any destructive changes to it. Since the iterator
protocol asks that we not call any destructive operations, this field
lets us track whether a function has made such a change, or not, without
actually revealing the internal state of the collection to a client. 

The predicate $iter(i, S, xs)$ (on line 2) is the predicate describing
the state of an iterator. The first argument $i$ is the iterator
constructor which owns this heap state. The finite set $S$ is a set of
triples describing the \emph{support} of the iterator -- the triples
$(c,xs,P)$ in this set describe all the collections the iterator will
examine in its enumeration. That is, the collections in the support
are the collections that are in $i$'s underlying state. (We track the
iterators in an underlying state by another mechanism, which we will
describe subsequently.) The third argument, $xs$, is a sequence
corresponding to the elements that the iterator has yet to produce.

$\ctext{newcoll}$ (specified on line 6) creates a new, empty
collection, unaliased with any other collection. The postcondition
specifies that the collection is empty, and that it begin in some
arbitrary abstract state. $\ctext{size}(c)$ (line 7) takes a
collection $c$, and returns the number of elements in $c$. The
abstract state $P$ of the $coll(c, xs, P)$ predicate is unchanged in
the pre- and post-conditions, indicating that this function does not
change the abstract state. $\ctext{add}(c, x)$ (line 8) takes a
collection $c$, and imperatively adds the element $x$ to the
collection. The abstract state is existentially quantified in the
postcondition, indicating that it can be modified by the call to
$\ctext{add}$. This ensures that clients cannot assume that the 
abstract state is the same before and after a call to $\ctext{add}$. 

$\ctext{newiter}(c)$ (line 9) takes a collection $c$, and returns an
iterator over it. The returned iterator predicate $iter(a,$
$\setof{(c, xs, P)}, xs)$ states that $a$ is the iterator object,
whose support is the collection $\setof{(c, xs, P)}$, and which will
produce the elements $xs$ (the same as the elements of $c$).

$\ctext{filter}(p, i)$ (line 10) takes a boolean function $p$ and an
iterator $i$, and returns a new iterator which will enumerate only
those elements which for which $p$ returns true. (We express this with
a logical function $filter$ called on the sequence $xs$.) Note that
$\ctext{filter}(p, i)$ consumes the original iterator state $iter(i,
S, xs)$ -- the postcondition state only mentions the state associated
with the return value of the the call to $\ctext{filter}$. This
reflects the fact that the filtered iterator takes ownership of the
underlying iterator, in order to prevent third parties from making
calls to $\ctext{next}(i)$ and possibly changing the state of the
filtered iterator. 

This is also why the support only needs to track the collections in
each iterator's underlying state. When we take ownership of the
argument's iterator state, we prevent third parties from being able to
call functions on the argument after creating the new iterator. This
takes advantage of the resource-conscious nature of separation logic:
a specification must have access to its footprint, and so we can hide
state inside a predicate to control which operations are allowed.  

$\ctext{map2}(f, i_1, i_2)$ (lines 11-13) takes a binary function $f$,
and two iterators $i_1$ and $i_2$. From an initial state $iter(i_1, S,
xs) * iter(i_2, S', ys)$, a call to $\ctext{map2}$ will return a new
iterator, whose support is the union of each argument's support, and
whose supply of values is the result of pairing the elements of $i_1$
and $i_2$ and applying $f$ to them. As with $\ctext{filter}$,
$\ctext{map2}$ takes ownership of the state of its argument iterators
and consumes them in the postcondition.

$\ctext{next}(i)$ (lines 14-15) takes an iterator $i$ as an argument.
In each precondition we ask for both an iterator predicate $iter(i, S,
-)$, and the collections $colls(S)$. $colls(S)$ iterates over the set
$S$, joining each $(c,xs,P)$ in $S$ into a large separated conjunction
$coll(c_1, xs_1, P_1) * \ldots * coll(c_k, xs_k, P_k)$. This expresses
the requirement that we need \emph{all} of the collections $i$ depends
on, all in the correct abstract state.

If the iterator is exhausted, $\ctext{next}(i)$ returns $\ctext{None}$
(line 14). If the iterator still has elements (i.e., is in a state
$iter(i, S, x\cdot xs)$), it returns the first element as
$\ctext{Some}\;x$, and sets the state to $iter(i, S, xs)$ in the
postcondition (line 15). We give two specifications purely for
readability; it lets us avoid giving a cumbersome combined
specification.

For space reasons, we cannot give the complete proof, so we give the
implementation of this module and its predicates below:
{\small
\begin{tabbing}
1 \qquad \= $list(c, \epsilon)$ \qquad\= $\equiv$ \= $c \pointsto \ctext{Nil}$ \\
2 \> $list(c, x\cdot xs)$ \> $\equiv$ \> 
   $\exists c'.\; c \pointsto \ctext{Cons}(x, c') * list(c', xs)$ 
\\[0.5em]
3\qquad \= $coll$\=$(c, xs, P) \equiv list(c, xs) \land P \land \mbox{exact}(P)$ \\[0.5em]
4 \> $iter$\=$(\ctext{Map2}(f, i_1, i_2), S, xs)$\=$ \equiv$ \= \\ 
\> \>         $\exists$\=$xs_1, xs_2, S_1, S_2.$ \\
\> \>\>$xs = (map\; f\; (zip\; xs_1\; xs_2)) \land (S = S_1 \uplus S_2) \;\land$\\
\> \>\>$iter(i_1, S_1, xs_1) * iter(i_2, S_2, xs_2)$ \\
5 \> $iter$\=$(\ctext{Filter}(p, i), S, xs) \equiv$ \\
\> \>\>      $\exists xs'.\;$\=$xs = filter\;p\;xs' \;\land\; iter(i, S, xs')$ \\
6 \> $iter$\=$(\ctext{Coll}(l), \setof{(c, xs, P)}, zs) \equiv$ \= \\
\> \> $\exists$\=$ c', ys.\; xs = ys \cdot zs \land l \pointsto c' * $ \\
\> \> $(coll$\=$(c, xs, P) \wand\;$\\ 
\> \> \> \!\!\!\!$[(coll(c, xs, P) \land (segment(c, c', ys) * list(c', zs)))]$
\\[0.5em]
7 \> $segment(c, c', \epsilon)$ \qquad\=$\equiv c = c' \land \emp$ \\
8 \> $seg$\=$ment(c, c', x\cdot xs) \equiv \exists c''.\; c \pointsto \ctext{Cons}(x, c'') * segment(c'', c', xs)$
\\[0.5em]

9 \> $\ctext{newcoll}() \equiv \comp{\newref{\reftype{\listtype{\N}}}{\ctext{Nil}}}$ 
\\[0.5em]
10 \> $\ctext{size}(lst) \equiv$ \=  $[$\=$ \letv{cell}{\comp{!lst}}{}$ \\
11 \> \> \> $\run{}\ctext{case}($\=$cell,$ \\
12 \> \> \> \> $\ctext{Nil} \to \comp{0}, $ \\
13 \> \> \> \> $\ctext{Cons}(h,t) \to \comp{\letv{n}{\ctext{size(t)}}{n + 1}})]$ 
\\[0.5em]

14 \> $\ctext{add}(c, x) \equiv$ \=
          $[$\= $\letv{cell}{\comp{!c}}{}$ \\
15 \> \> \> $\letv{r}{\comp{\newref{\reftype{\listtype{\N}}}{cell}}}{}$ \\
16 \> \> \> $c := \ctext{Cons}(x, r)]$
\\[0.5em]
17 \> $\ctext{new}\ctext{iter}(c) \equiv 
   \comp{\letv{r}{\comp{\newref{\reftype{\reftype{\listtype{\N}}}}{(c)}}}{\ctext{Coll}\;r}}$ 
\\[0.5em]
18 \> $\ctext{filter}(p, i) \equiv \comp{\ctext{Filter}(p, i)}$ 
\\[0.5em]
19 \> $\ctext{map2}(f, i_1, i_2) \equiv \comp{\ctext{Map2}(f, i_1, i_2)}$ 
\\[0.5em]
20 \> $\ctext{next}(\ctext{Coll}\;r) \equiv$ \=
       $[$\=$\letv{list}{\comp{!r}}{}$ \\
21 \>\>\>$\letv{cell}{\comp{!list}}{}$ \\
22 \>\> \>$\run{}\ctext{case}(cell,$\=
            $\ctext{Nil} \to $\=$\comp{\ctext{None}},$ \\
23 \>\>\>\> $\ctext{Cons}(x, t) \to $\\
   \>\>\>\>                   \> $[$\=$\letv{dummy}{\comp{r := t}}{}$ \\
   \>\>\>\>                   \>    \>${\ctext{Some}\;x}])]$ \\
24 \> $\ctext{next}(\ctext{Filter}(p, i)) \equiv$ \= 
        $[$\=$\letv{v}{\ctext{next}(i)}{}$ \\
25 \>\>\>$\run{}\ctext{case}(v,$\=
            $\ctext{None} \to \comp{\ctext{None}}$ \\
26 \>\>\>\> $\ctext{Some}$\=$\;x \to $ \\
   \>\>\>\> \>$\ctext{if}(p\;x,$\=$\comp{\ctext{Some}\; x},$ \\
   \>\>\>\> \>\>                                   $\ctext{next}(\ctext{Filter}(p, i))))]$
\\
27 \> $\ctext{next}$\=$(\ctext{Map2}(f, i_1, i_2)) \equiv$ \\
   \>\>$[$\=$\letv{v_1}{\ctext{next}\;i_1}{}$ \\
28 \>\>\>$\letv{v_2}{\ctext{next}\;i_2}{}$ \\
29 \>\>\>$\ctext{case}(v_1,$ \= 
            $\ctext{None} \to \ctext{None},$ \\
30 \>\>\>\> $\ctext{Some}\;x_1 \to \ctext{case}(v_2,$ \=
              $\ctext{None} \to \ctext{None},$ \\
31 \>\>\>\>\> $\ctext{Some}\;x_2 \to \ctext{Some}(f(x_1, x_2))))]$\\
\end{tabbing}
}

The inductive predicate $list(c, xs)$ is defined on lines 1-2. It says
that $c$ is an empty list if it is a pointer to the value
$\ctext{Nil}$, and that is a non-empty list if $c$ points to
$\ctext{Cons}(x, c')$, where $x$ is the first element of the sequence
and $c'$ disjointly represents the tail of the list.

The predicate $coll(c, xs, P)$ (line 3) is defined as the conjunction
of a linked list predicate $list(c, xs)$ and an exact predicate
$P$. An exact predicate is one that is true of exactly one heap, and
the conjunction of $P$ with $list(c,xs)$ ensures that this linked list
cannot be modified at all, without changing $P$. This is a relatively
common idiom when verifying programs whose invariants depend on a
notion of destructive update. It is necessary because the predicate
$list(c, xs)$ can be maintained even when $c$ is modified. Concretely,
suppose that $xs$ is nonempty. Then any modification of $c$ which
changes $c$'s tail to some different physical list representing the
same tail sequence will still satisfy the predicate $list(c, xs)$.
However, if a predicate is known to be exact, then there is exactly
one heap it can be, and so no other heaps can satisfy it. 

The inductive definition of $iter$ is on lines 4-6. The base case
(line 6) is the most complex. In essence, it says that the iterator's
pointer is a finger into the middle of the list, $c'$.  

We express this requirement with the clause $coll(c, xs, P) \land
(segment(c,c',ys) * list(c',zs))$, which says that heap associated
with the collection can be viewed in two ways, both as the collection
itself, and as a partial list segment representing what has already
been seen, together with the remainder of the list beginning at $c'$.
%
However, the iterator state does not own the collection; this clause
will only hold if we have the collection state in addition to the
iterator. We use the magic wand to say this: it means that when we
supply the iterator with $coll(x, xs, P)$, then we can view the
state in this way.
So as a whole, the predicate $iter(\ctext{Coll}(l), \setof{(c,xs,P)},
zs)$ can be read as ``the iterator is a pointer $l$ to some $c'$, such
that if we are given the collection state $coll(c, xs, P)$, then $c'$
is a pointer into the middle of the list, with $zs$ as the remaining
elements''. 

The reason we go to this effort is to simplify the specification and
proof of client programs -- we could eliminate the use of the magic
wand in the base case if iterators owned their collections, but this
would complicate verifying programs that use multiple iterators over
the same collection, or which want to call pure methods on the
underlying collection. In those cases, the alternative would require
us to explicitly transfer ownership in the proofs of client programs,
which is quite cumbersome, and forces clients to reason using the 
magic wand. The current approach isolates that reasoning within the 
proof of the implementation. 

On line 5, the $\ctext{Filter}(p,i)$ case says that $i$ must have some
iterator state with the same support $S$, and a sequence of elements
$xs'$ that yield $xs$ once filtered. We use the logical function
$filter$, defined at the assertion level, to describe the effect on
the iterator's logical sequence.  On line 4, we specify $iter$ for the
$\ctext{Map2}(f, i_1, i_2)$ case. We assert that the support $S$ must
be divisible into two disjoint parts, one for $i_1$ and one for $i_2$,
and that there is iterator state for $i_1$ and $i_2$, and that the
sequences $i_1$ and $i_2$ combine to yield the output sequence.

In both of these cases, we define the behavior of the imperative
linked list in terms of purely functional sequences. This is a very
common strategy in many verification efforts, but here we see that we
can use it in a local way -- in the base case, we are forced to
consider issues of aliasing and ownership, but in the inductive cases
we can largely avoid that effort.

The collection operations are mostly straightforward --
$\ctext{newcoll}$ (line 9) allocates a new linked list, $\ctext{size}$
(lines 10-13) recursively traverses the list to calculate the length,
and $\ctext{add}$ (lines 14-16) adds a cons cell to the front of the
list.  $\ctext{newiter}$ (lines 17) allocates a pointer to the front
of the list, and then wraps that in the $\ctext{Coll}$
constructor. For $\ctext{filter}$ (line 18) and $\ctext{map2}$ (line
19), we just apply the constructor to the arguments and then return
the result.

$\ctext{next}$ (lines 20-31) recursively walks down the structure of
the iterator tree, and combines the results from the leaves upwards.
The base case is the $\ctext{Coll }r$ case (lines 20-23). The iterator
pointer is doubly-dereferenced, and then the contents examined. If the
end of the list has been reached and the contents are $\ctext{Nil}$,
then $\ctext{None}$ is returned to indicate there are no more
elements. Otherwise, the pointer $r$ is advanced, and the head
returned as the observed value. The $\ctext{Filter(p,i)}$ case (lines
24-26) will return $\ctext{None}$ if $i$ is exhausted, and if it is
not, it will pull elements from $i$ until it finds one that satisfies
$p$, calling itself recursively until it succeeds or $i$ is exhausted.
Finally, in the $\ctext{Map2}(f, i_1, i_2)$ case (lines 27-31),
$\ctext{next}$ will draw a value from both $i_1$ and $i_2$, and will
return $\ctext{None}$ if either is exhausted, and otherwise it will
return $f$ applied to the pair of values.

Below, we give an example use of this module in annotated program
style: {\small
\begin{tabbing}
1 \qquad \= $\setof{\emp}$ \\
2 \> $\letv{c_1}{\ctext{newcoll}()}{}$ \\
3 \> $\setof{\exists P'_1.\; coll(c_1, \epsilon, P'_1)}$ \\
4 \> $\setof{coll(c_1, \epsilon, P_1)}$ \\
5 \> $\letv{()}{\ctext{add}(c_1, 4)}{}$ \\
6 \> $\setof{\exists P'_2.\; coll(c_1, 4\cdot\epsilon, P'_2)}$ \\
7 \> $\setof{coll(c_1, 4\cdot\epsilon, P_2)}$ \\
8 \> $\letv{()}{\ctext{add}(c_1, 3)}{}$ \\
9 \> $\letv{()}{\ctext{add}(c_1, 2)}{}$ \\
10 \> $\setof{coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)}$ \\
11 \> $\letv{c_2}{\ctext{newcoll}()}{}$ \\
12 \> $\letv{()}{\ctext{add}(c_2, 3)}{}$ \\
13 \> $\letv{()}{\ctext{add}(c_2, 5)}{}$ \\
14\> $\setof{coll(c_2, 5\cdot3\cdot\epsilon, Q_2) * coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)}$ \\
15\> $\letv{i_1}{\ctext{newiter}(c_1)}{}$ \\
16\> $\{$\=$iter(i_1, \setof{(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)}, 2\cdot3\cdot4\cdot\epsilon)$
\\ 
  \>\>$*\; coll(c_2, 5\cdot3\cdot\epsilon, Q_2) * coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)\}$ \\
17 \> $\letv{i'_1}{\ctext{filter}(even?, i_1)}{}$ \\
18 \> $\{$\=$iter(i'_1, \setof{(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)}, 2\cdot4\cdot\epsilon)$ \\
   \>\>$*\; coll(c_2, 5\cdot3\cdot\epsilon, Q_2) * coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)\}$\\
19 \> $\letv{i_2}{\ctext{newiter}(c_2)}{}$ \\
20 \> $\{$\=$iter(i'_1, \setof{(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)}, 2\cdot4\cdot\epsilon)$\\
  \>\>$*\;iter(i_2, \setof{(c_2, 5\cdot3\cdot\epsilon, Q_2)}, 5\cdot3\cdot\epsilon)$ \\
  \>\>$*\;coll(c_2, 5\cdot3\cdot\epsilon, Q_2) * coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)\}$ \\
21 \> $\letv{i}{\ctext{map2}(plus, i'_1, i_2)}{}$ \\
22 \> $\{$\=$iter(i, \setof{(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4), (c_2, 5\cdot3\cdot\epsilon, Q_2)}, 7\cdot7\cdot\epsilon)$ \\
\> \> $*\; coll(c_2, 5\cdot3\cdot\epsilon, Q_2) * coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)\}$ \\
23 \> $\letv{n}{\ctext{size}(c_2)}{}$ \\
24 \> $\{$\=$n = 2 \;\land
iter(i, \setof{(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4), (c_2, 5\cdot3\cdot\epsilon, Q_2)}, 7\cdot7\cdot\epsilon)$\\
\>\>$*\;coll(c_2, 5\cdot3\cdot\epsilon, Q_2) * coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)\}$ \\

25 \> $\letv{x}{\ctext{next}(i)}{}$ \\
26 \> $\{$\= $n = 2 \land x = \ctext{Some }7 \;\land $\\
\> \> $iter(i, \setof{(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4), (c_2, 5\cdot3\cdot\epsilon, Q_2)}, 7\cdot\epsilon)$ \\
\> \> $*\; coll(c_2, 5\cdot3\cdot\epsilon, Q_2) * coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)\}$ \\
27 \> $\ctext{add}(c_2, 17)$ \\
28 \> $\{$\= $n = 2 \land x = \ctext{Some }7 \;\land $\\
\> \> $iter(i, \setof{(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4), (c_2, 5\cdot3\cdot\epsilon, Q_2)}, 7\cdot\epsilon)$ \\ 
\> \> $* \; (\exists Q_3.\; coll(c_2, 17\cdot5\cdot3\cdot\epsilon, Q_3)) * coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)\}$ \\
\end{tabbing}
}

In line 1 of this example, we begin in an empty heap. In line 2, we
create a new collection $c_1$, which yields us the state $\exists
P'_1.\; coll(c_1, \epsilon, P'_1)$, with an existentially quantified
abstract state.

Because $P'_1$ is existentially quantified, we do not know what value
it actually takes on. However, if we prove the rest of the program
using a freshly-introduced variable $P_1$, then we know that the rest
of the program will work for \emph{any} value of $P_1$, because free
variables are implicitly universally quantified.  So it will work with
whatever value $P'_1$ had. So we drop the quantifier on line 4, and
try to prove this program with the universally-quantified
$P_1$.\footnote{A useful analogy is the existential elimination rule
  in the polymorphic lambda calculus: we prove that we can use an
  existential by showing that our program is well-typed no matter what
  the contents of the existential are.}

This permits us to $\ctext{add}$ the element 4 to $c_1$ on line 5. Its
specification puts the predicate $coll()$ on line 6 again into an
existentially quantified state $P'_2$. So we again replace $P'_2$ with
a fresh variable $P_2$ on line 7, and will elide these existential
introductions and unpackings henceforth.

In lines 8-9, we add two more elements to $c_1$, and on lines 11-13,
we create another collection $c_2$, and add $3$ and $5$ to it, as can
be seen in the state predicate on line 14. On line 15, we create the
iterator $i_1$ on the collection $c_1$. The $iter$ predicate on line
16 names $i_1$ as its value, and lists $c_1$ in state $P_4$ as its
support, and promises to enumerate the elements 2, 3, and 4.

On line 17, $\ctext{filter}(even?, i_1)$ creates the new iterator
$i'_1$. This iterator yields only the even elements of $i_1$, and so
will only yield 2 and 4. On line 18, $i_1$'s iterator state has been
consumed to make $i'_1$'s state. We can no longer call
$\ctext{next}(i_1)$, since we do not have the resource invariant
needed to prove anything about that call. Thus, we cannot write a
program that would break $i'_1$'s representation invariant.

On line 19, we create a third iterator $i_2$ enumerating the elements
of $c_2$. The state on line 20 now has predicates for $i'_1$, $i_2$,
$c_1$ and $c_2$. On line 21, $\ctext{map2}(plus, i'_1, i_2)$ creates a
new iterator $i$, which produces the pairwise sum of the elements of
$i'_1$ and $i_2$, and consumes the iterator states for $i'_1$ and
$i_2$ to yield the state for the new iterator $i$. Note that the 
invariant for $i$ does not make any mention of what it was constructed
from, naming only the collections it needs as support. 
%Furthermore, the support of $i$ is the union of the
%supports of $i'_1$ and $i_2$ -- namely, the two collections $c_1$ and
%$c_2$.

On line 23, the $\ctext{size}$ call on $c_2$ illustrates that we can
call non-destructive methods while iterators are active. The call to
$\ctext{next}(i)$ on line 24 binds $\ctext{Some }7$ to $x$, and the
the iterator's sequence argument (line 27) shrinks by one element. On
line 28, we call $\ctext{add}(c_2, 17)$ the state of $c_2$ changes to
$\exists Q_3.\; coll(c, 17\cdot 5 \cdot 3\cdot\epsilon, Q_3)$ (line
27). So we can no longer call $\ctext{next}(i)$, since it needs $c_2$
to be in the state $Q_2$.

\textbf{Discussion.} This example shows a pleasant synergy between
higher-order quantification and separation logic. We can give a
relatively simple specification to the clients of the collection
library, even though the internal invariant is quite subtle (as the
use of the magic wand suggests). Higher-order logic also lets us
freely define new data types, and so our specifications can take
advantage of the pure, non-imperative nature of the mathematical
world, as can be seen in the specifications of the $\ctext{filter}$
and $\ctext{map2}$ functions -- we can use equational reasoning on
purely functional lists in our specifications, even though our
algorithms are imperative.

\section{The Flyweight and Factory Patterns}

The flyweight pattern is a style of cached object creation. Whenever a
constructor method is called, it first consults a table to see if an
object corresponding to those arguments has been created. If it has,
then the preexisting object is returned.  Otherwise, it allocates a
new object, and updates the table to ensure that future calls with the
same arguments will return this object. Because objects are re-used,
they become pervasively aliased, and must be used in an immutable
style to avoid surprising updates. (Functional programmers call this
style of value creation ``hash-consing''.)

This is an interesting design pattern to verify, for two reasons.
First, the constructor has a memo table to cache the result of
constructor calls, which needs to be hidden from clients. Second, this
pattern makes pervasive use of aliasing, in a programmer-visible
way. In particular, programmers can test two references for identity
in order to establish whether two values are equal or not. This allows
constant-time equality testing, and is a common reason for using this
pattern. Therefore, our specification has to be able to justify this
reasoning.

Below, we specify a program that uses the flyweight pattern to create
and access glyphs (i.e., refs of pairs of characters and fonts) of a
particular font $f$. We have a function $\ctext{newglyph}$ to create
new glyphs, which does the caching described above, using a predicate
variable $I$ to refer to the table invariant; and a function
$\ctext{getdata}$ to get the character and font information from a
glyph.

Furthemore, these functions will be created by a call to another
function, $\ctext{make\_flyweight}$, which receives a font as an
argument and will return appropriate $\ctext{newglyph}$ and
$\ctext{getdata}$ functions. 

{\small
\begin{tabbing}
Flyweight$($\=$I : \assert,\;\; $\\
\> $\ctext{newglyph} : \chartp \to \monad{\ctext{glyph}},$ \\
\> $\ctext{getdata} : \ctext{glyph} \to \monad{(\chartp \times \fonttp)},$ \\
\> $f:\fonttp) \equiv$ \\
1 \qquad \=$\exists $\=$glyph : \ctext{glyph} \times \chartp \times \fonttp \To \assert.$ 
\\[0.5em]

2  \> \> $\forall c, S.\;$\=
         $\{I \land chars(S)\}$ \\
   \>\>\>$\run{\ctext{newglyph}(c)}$ \\
   \>\>\>$\setof{a:\ctext{glyph}.\; 
                 I \land chars(\setof{(a, (c,f))} \cup S)}$ \\
  \> \!$\specand$ \\
3 \> $\forall l, c, f, P.\;$\=
     $\setof{glyph(l, c, f) \land P}$ \\
\>\> $\run{\ctext{getdata}(l)}$ \\
\>\> $\setof{a:\chartp \times \fonttp.\; glyph(l, c, f) \land P \land a = (c,f)}$
\\
  \> \!$\specand$ \\
4 \> $\{\forall l, l', c, c'.\;$\=$I \land glyph(l,c,f) \land glyph(l',c',f')
 \implies $ \\
\>\>  $\;\;\;\left(l = l' \iff (c = c'\land f=f')\right)\}$ \\[0.5em]

$chars(\emptyset)$ \qquad\qquad\qquad \;\;\= $\equiv$ \= $\top$ \\
$chars(\setof{(l,(c,f))} \cup S)$ \> $\equiv$ \> $glyph(l,c,f) \land chars(S)$ \\
\end{tabbing}
}

In the opening , we informally parameterize our specification over the
predicate variable $I$, the function variable $\ctext{newglyph}$, the
function variable $\ctext{getdata}$, and the variable $f$ of $\fonttp$
type. The reason we do this instead of existentially quantifying over
them will become clear shortly, once we see the factory function that
creates flyweight constructors.

On line 1, we assert the existence of a three-place predicate
$glyph(l, c, f)$, which is read as saying the glyph value $l$ is a
glyph of character $c$ and font $f$. 

On line 2, we specify the $\ctext{newglyph}$ procedure. Its
precondition says the pre-state must be the private flyweight state
$I$, and that this state overlaps with the character state for the
glyph/data pairs in $S$. The definition of $chars$ takes a set of
glyph/data pairs and produces the conjunction of $glyph(l,c,f)$ for
all the $(l,(c,f)) \in S$. Running $\ctext{newglyph}(c)$ will yield a
postcondition state in which $(a, (c,f))$ is added to the set $S$ --
that is, the postcondition state is $I \land chars(S) \land
glyph(a,c,f)$.

The intuition for this specification is that $I$ represents the
private state of the memo table. We use an ordinary conjunction
instead of the separating conjunction in the definition of $chars$ to
say that the different glyphs may alias with each other, and with the
private state $I$. In other words, even though the $\ctext{newglyph}$
function returns a new glyph value, the ownership of the state
associated with that value is not transferred -- it remains with the
constructor. All the client can know is that some glyph state exists
for the value it created, and that the glyph state is owned by the
constructor.

On line 3, we specify the $\ctext{getdata}$ function. If the predicate
$glyph(r, c, f)$ is in the precondition, then $\ctext{getdata}(r)$
will return $(c, f)$.  To enforce the flyweight invariant that the
glyph objects are read-only, we conjoin the pre- and post-conditions
with an arbitrary predicate variable $P$. Since $P$ must be preserved
for any instantiation, we know that $\ctext{getdata}$ cannot make any
changes to the underlying data. (It might be possible to internalize
this style of argument into the logic via some sort of relational
parametricity proof. However, we have not yet done so.)

Finally, on lines 4, we give an axiom about the interaction of $I$ and
$glyph(l,c,f)$, which says that if we know that $I \land glyph(l,c,f)
\land glyph(l',c', f')$ holds, then $l = l'$ holds if and only $c =
c'$ and $f = f'$. This axiom gives clients the ability to take
advantage of the fact that we are caching object creation and conclude
that two calls to $\ctext{newglyph}$ with the same arguments will
yield the same result. 

Requiring an axiom of separation hold of the abstract predicates is
how we state reasoning principles about aliasing as part of the
interface of a module. When we verify the implementation, we will need
to give concrete definitions of $I$ and $glyph$, and show that the
formula is actually a tautology of separation logic.

The specification of the flyweight factory looks like this:

{\small
\begin{tabbing}
1 \qquad \= $\exists \ctext{make\_flyweight} :
\fonttp \to \bigcirc($\=$(\chartp \to \monad{\ctext{glyph}}) \times$ \\
\> \> $(\ctext{glyph} \to \monad{(\chartp \times \fonttp)})).$\\
2 \> \;\;\= $\forall f.\;$\=$\setof{\emp}$ \\
  \>\> \> $\run{\ctext{make\_flyweight}(f)}$ \\
  \>\> \> $\setof{a.\; \exists I:\assert.\; I \land \validprop{\mbox{Flyweight}(I, \fst{a}, \snd{a}, f)}}$ \\
\end{tabbing}
}

Here, we assert the existence of a function $\ctext{make\_flyweight}$,
which takes a font $f$ as an input argument, and returns two functions
to serve as the $\ctext{getchar}$ and $\ctext{getdata}$ functions of
the flyweight. In the postcondition, we assert the existence of some
private state $I$, which contains the table used to cache glyph
creations. 

This pattern commonly arises when encoding aggressively
object-oriented designs in a higher-order style --- we call a
function, which creates a hidden state, and returns other procedures
which are the only way to access that state. This style of
specification resembles the existential encodings of objects into type
theory. The difference is that instead of making the fields of an
object an existentially quantified \emph{value}~\cite{pierce-turner}, we
make use of existentially-quantified \emph{state}.

Below, we define $\ctext{make\_flyweight}$ and its predicates:


{\small
\begin{tabbing}
1 \qquad \= $\ctext{m}$\=$\ctext{ake\_flyweight} \equiv$ \\
   \> \>     $\lambda f:$\=$\fonttp.\;$\\
2  \> \> $[$\=$\letv{t}{\ctext{newtable}()}{}$ \\
3  \> \> \> $\ctext{letv }$\=$newglyph =$ \\
4  \> \> \> \> \!\!$[\lambda c.[$\=$\ctext{letv }x = \ctext{lookup}(t, c)\ctext{ in}$\\
5  \> \> \>\>\> $\ctext{run }\ctext{case}(x,$\=
$\ctext{None} \to [$\=$\ctext{letv } r = [\newref{\ctext{glyph}}(c,f)] \ctext{ in}$\\
6  \> \> \>\>\>\>\> $\ctext{letv } \_ = \ctext{update}(t, c, r) \ctext{ in }r],$ \\
7 \> \> \>\>\>\> $\ctext{Some }r \to [r])] \ctext{ in}$ \\
8 \> \> \> $\ctext{letv }getdata = [\lambda r.\; [!r]] \ctext{ in}$ \\
9 \> \> \> $(newglyph, getdata)]$
\\[0.5em]

$glyph(r, c, f) \equiv r \pointsto (c,f) * \top$ \\[0.5em]

$I \equiv table(t,mapping) * refs(mapping, \mbox{dom}(mapping))$ \\[0.5em]

$refs(mapping, \emptyset) \qquad\qquad  $ \= $\equiv$\;\;\=$\emp$ \\
$refs(mapping, \setof{c} \cup D)$ \> $\equiv$\> $mapping(c) \pointsto (c,f) * refs(f, D)$ \\
 
\end{tabbing}
}

In this implementation we have assumed the existence of a hash table
implementation with operations $\ctext{newtable}$, $\ctext{lookup}$,
and $\ctext{update}$, whose specifications we omit for space
reasons. The $\ctext{make\_flyweight}$ function definition takes a
font argument $f$, and then in its body it creates a new table $t$. It
then constructs two functions as closures which capture this state
(and the argument $f$) and operate on it. In lines 4-7, we define
$newglyph$, which takes a character and checks to see (line 5) if it is
already in the table. If it is not (lines 5-6), it allocates a new
glyph reference, stores it in the table, and returns the
reference. Otherwise (line 7), it returns the existing reference from
the table.  On lines 8, we define $getdata$, which dereferences
its pointer argument and returns the result. This implementation does
no writes, fulfilling the promise made in the specification. The
definition of the invariant state $I$ describes the state of the table
$t$ (and $mapping$), which are hidden from clients.

Observe how the post-condition to $\ctext{make\_flyweight}$ nests the
existential state $I$ with the validity assertion to specialize the
flyweight spec to the \emph{dynamically} created table. Each created
flyweight factory receives its own private state, and we can reuse
specifications and proofs with no possibility that the wrong
$\ctext{getdata}$ will be called on the wrong reference, even though
they have compatible types.

% It is also worth noting that our factory here does not correspond
% precisely to the most general form of the factory pattern. While we
% take an argument (the font $f$) and return a flyweight interface
% specialized to $f$, we do not change the implementations of $newchar$
% and $getdata$ based beyond closing over the newly allocated memo table
% and $f$.


% While this is technically possible, our language is simply-typed --
% which means that we cannot readily change data representations, and
% hence there is little reason to change the function
% implementations. If we added polymorphism to our language, we would
% be able to use existential quantification to more closely match
% object-oriented style.
% \vspace{-1em}
\section{Subject-Observer}

The subject-observer pattern is one of the most characteristic
patterns of object-oriented programming, and is extensively used in
GUI toolkits. This pattern features a mutable data structure called
the \emph{subject}, and a collection of data structures called
\emph{observers} whose invariants depend on the state of the
subject. Each observer registers a callback function with the subject
to ensure it remains in sync with the subject. Then, whenever the
subject changes state, it iterates over its list of callback
functions, notifying each observer of its changed state. While
conceptually simple, this is a lovely problem for verification, since
every observer can have a different invariant from all of the others,
and the implementation relies on maintaining lists of callback
functions in the heap.  

In our example, we will model this pattern with one type of subjects,
and three functions. A subject is simply a pair, consisting of a
pointer to a number, the subject state; and a list of observer
actions, which are imperative procedures to be called with the new
value of the subject whenever it changes. There is a function
$\ctext{newsub}$ to create new subjects; a function
$\ctext{register}$, which attaches observer actions to the subject;
and finally a function $\ctext{broadcast}$, which updates a subject
and notifies all of its observers of the change. 


We give a specification for the subject-observer pattern below:
%
{\small
\begin{tabbing}
1 \qquad \= $\exists sub : A_s \times \N \times \seqsort{((\N \To \assert) \times (\N \to \monad{1}))}.$ \\
2 \> $\exists \ctext{newsub} : \N \to \monad{A_s},$ \\ 
3 \> $\exists \ctext{register} : A_s \times (\N \to \monad{1}) \to \monad{1},$ \\
4 \> $\exists \ctext{broadcast} : A_s \times \N \to \monad{1}.$ \\
\\[0.5em]
5 \>$\forall n.\; \spec{\emp}{\run{\ctext{newsub}(n)}}{a:A_s}{sub(a, n, \epsilon)}$ \\
\> $\specand$ \\
6 \> $\forall f, O, s, n, os. $\=$(\forall i, k. \spec{O(i)}{\run{f(k)}}{a:1}{O(k)})$ \\
7\> \>$\specimp$\=$\setof{sub(s, n, os)}$ \\
8\> \>          \>$\run{\ctext{register}(s, f)}$ \\
9 \> \>          \>$\setof{a:1.\; sub(s, n, (O,f)\cdot os)}$ \\
\> $\specand$ \\
10 \> $\forall s,i,os,k.\; $\=$\setof{sub(s, i, os) * obs(os)}$ \\
  \>                       \>$\run{\ctext{broadcast}}(s,k)$ \\
  \>                       \>$\setof{a:1.\; sub(s, k, os) * obs\_at(os, k)}$ 
\\[0.5em]
$obs(\epsilon) \;\qquad\qquad $\=$\equiv \emp$ \\
$obs((O,f)\cdot os) $\>$\equiv (\exists i.\; O(i)) * obs(os)$ 
\\[0.5em]
$obs\_at(\epsilon, k) \;\qquad\qquad $\=$\equiv \emp$ \\
$obs\_at((O,f)\cdot os, k) $\>$\equiv O(k) * obs\_at(os, k)$ 
\\
\end{tabbing}
}
%
On line 1 we assert the existence of a three-place predicate $sub(s,
n, os)$. The first argument is the subject $s$'s whose state this
predicate represents. The second argument $n$ is the data the
observers depend on, and the field $os$ is a sequence of callbacks
paired their invariants. That is, $os$ is a sequence of pairs,
consisting of the observer functions which act on a state, along with
the predicate describing what that state should be.

On lines 2-4, we assert the existence of $\ctext{newsub}$,
$\ctext{register}$ and $\ctext{broadcast}$, which create a new
subject, register a callback, and broadcast a change, respectively.

$\ctext{register}$ is a higher order function, which takes a subject
and an observer action its two arguments. The observer action is a
function of type $\N \to \monad{1}$, which can be read as saying it
takes the new value of the subject and performs a side-effect. Because
$\ctext{register}$ depends on code, its specification must say how
this observer action should behave. $\ctext{register}$'s specification
on lines 6-9 accomplishes this via an implication over Hoare
triples. It says that \emph{if} the function $f$ is a good observer
callback, \emph{then} it can be safely registered with the
subject. Here, a ``good callback'' $f$ is one that takes an argument
$k$ and sends an observer state to $O(k)$. If this condition is
satisfied, then $\ctext{register}(s, f)$ will add the pair $(O,f)$ to
the sequence of observers in the $sub$ predicate.

$\ctext{broadcast}$ updates a subject and all its interested
observers.  The precondition state of $\ctext{broadcast}(s,k)$
requires the subject state $sub(s,n,os)$, and all of the observer
states $obs(os)$. The definition $obs(os)$ takes the list of observers
and yields the separated conjunction of the observer states. So when
$\ctext{broadcast}$ is invoked, it can modify the subject and any of
its observers. Then, after the call, the postcondition puts the $sub$
predicate and all of the observers in the same state $k$. The
$obs\_at(os,k)$ function generates the separated conjunction of all
the $O$ predicates, all in the same state $k$.

The implementation follows:
{\small
\begin{tabbing}
1 \qquad \= $A_s \equiv \reftype{\N} \times \reftype{\listtype{(\N \to \monad{1})}}$
\\[0.5em]
2 \> $sub(s, n, os) \equiv$\=$ \fst{s} \pointsto n * 
              list(\snd{s}, map\; \snd{} os) \land Good(os)$ 
\\[0.5em]
3 \> $Good(\epsilon) \!\qquad\qquad \equiv \top$ \\
4 \> $Good((O,f)\cdot os) \equiv\; $\=
   $\validprop{(\forall i,k.\; \spec{O(i)}{\run{f(k)}}{a:1}{O(k)})}$ \\
  \> \>      $\land Good(os)$ 
\\[0.5em]
5 \> $\ctext{register}(s, f) \equiv$ \=
         $[$\= $\letv{cell}{\comp{!(\snd{s})}}{}$ \\
6 \> \> \> $\letv{r}{\comp{\newref{\reftype{\listtype{(\N \to \monad{1})}}}{cell}}}{}$ \\
7 \> \> \> $\snd{s} := \ctext{Cons}(f, r)]$
\\[0.5em]

8  \> $\ctext{broad}$\=$\ctext{cast}(s, k) \equiv$ \\
9  \>  \> $[$\=$\letv{dummy}{[\fst{s} := k]}{\ctext{loop}(k, \snd{s})}]$ \\


10 \> $\ctext{loop}$\=$(k, list) \equiv $\\
   \>         \>$[$\=$\letv{cell}{[!list]}{}$ \\
11 \>\>\> $\run{}\ctext{case}(cell,$\= 
            $\ctext{Nil} \to [()],$ \\
12 \>\>\>\> $\ctext{Cons}(f, tl) \to [$\=$\letv{dummy}{f(k)}{}$ \\
13  \>\>\>\> \> $\run{\ctext{loop}(k,tl)}])$ \\[0.5em]
14 \> $\ctext{new}\ctext{sub}(n) \equiv$ \=
          $[$\=$\letv{data}{\newref{\N}{n}}{}$ \\
15 \> \> \> $\letv{callbacks}{\newref{\listtype{(\N \to \monad{1})}}{\ctext{Nil}}}{}$ \\
16 \> \> \> $(data, callbacks)]$
\end{tabbing}
}

In line 1, we state concrete type of the subject $A_s$ is a pair of a
pointer to a reference, and a pointer to a list of callback
functions. (This is \emph{not} an existential quantifier.  Since our
language is simply typed, we have no form of type abstraction and
simply use $A_s$ as an abbreviation.)  On line 2, we define the
three-place subject predicate, $sub(s,n,os)$. The first two subclauses
of the predicate's body describe the physical layout of the subject,
and assert that the first component of $s$ should point to $n$, and
that the second component of $s$ should be a linked list containing
the function pointers in $os$. (The $list$ predicate is described in
Section 3, when we give the definition of the iterator predicates.)

Then we require that $os$ be ``Good''. $Good$-ness is defined on lines
3 and 4, and says a sequence of predicates and functions is good when
every $(O,f)$ pair in the sequence satisfies the same validity
requirement the specification of $\ctext{register}$ demanded -- that
is, that each observer function $f$ update $O$ properly.  Note that we
interleave assertions and specifications to constrain the behavior of
code stored in the heap.

Next, we give the implementations of $\ctext{register}$ and
$\ctext{broadcast}$. $\ctext{register}$, on lines 5-7, adds its
argument to the list of callbacks. Though the code is trivial, its
correctness depends on the fact the $Good$ predicate holds for the
extended sequence --- we use the fact that the argument $f$ updates 
$O$ properly to establish that the extended list remains $Good$. 

$\ctext{broadcast}$, on lines 8-9, updates the subject's data field
(the first component), and then calls $\ctext{loop}$ (on lines 10-13)
to invoke all the callbacks. $\ctext{loop}(k, \snd{s})$ just recurs
over the list and calls each callback with argument $k$. The
correctness of this function also relies on the $Good$ predicate --
each time we call one of the functions in the observer list, we use
the hypothesis of its behavior given in $Good(os)$ to be able to make
progress in the proof.


Below, we give a simple piece of client code using this interface.

{\small
\begin{tabbing}
1 \qquad \= 
$\setof{\emp}$ \\
2 \> 
$\letv{s}{\ctext{newsub}(0)}{}$ \\
3 \> $\setof{sub(s, 0, \epsilon)}$ \\
4 \> $\letv{d}{\newref{\N}{(0)}}{}$ \\
5 \> $\letv{b}{\newref{\ctext{bool}}{(\ctext{true})}}{}$ \\
6 \> $\setof{sub(s, 0, \epsilon) * d \pointsto 0 * b \pointsto \ctext{true}}$\\
7 \> $\letv{()}{\ctext{register}(s, f)}{}$\\
8 \> $\setof{sub(s, 0, (double, f)\cdot\epsilon) * double(0) * b \pointsto \ctext{true}}$ \\
9 \> $\letv{()}{\ctext{register}(s, g)}{}$\\
10 \> $\setof{sub(s, 0, (even, g)\cdot(double, f)\cdot\epsilon) * double(0) * even(0)}$ \\
11 \> $\ctext{broadcast}(s, 5)$ \\
12 \> $\setof{sub(s, 5, (even, g)\cdot(double, f)\cdot\epsilon) * double(5) * even(5)}$ \\
13 \> $\setof{sub(s, 5, (even, g)\cdot(double, f)\cdot\epsilon) * d \pointsto 10 * b \pointsto \ctext{false}}$ 
\\[0.5em]
14 \> $f \qquad \qquad $\=$\equiv \lambda n:\N.\; [d := 2 \times n]$ \\
15 \> $double(n)$ \> $\equiv d \pointsto (2 \times n)$ \\
16 \> $g$ \> $\equiv \lambda x:\N.\; [b := even?(x)]$ \\
17 \> $even(n)$ \> $\equiv b \pointsto even?(n)$ \\
\end{tabbing}
}
% \vspace{-1em}
We start in the empty heap, and create a new subject $s$ on line 2.
On line 4, we create a new reference to $0$, and on line 5, we create
a reference to $\ctext{true}$. So on line 6, the state consists of a
subject state, and two references.  On line 7, we call
$\ctext{register}$ on the function $f$ (defined on line 14), which
sets $d$ to twice its argument. To the observer list in sub, we add
$f$ and the predicate $double$ (defined on line 15), which asserts
that indeed, $d$ points to two times the predicate argument. On line
8, we call $\ctext{register}$ once more, this time with the function
$g$ (defined on line 16) as its argument, which stores a boolean
indicating whether its argument was even into the pointer $b$. Again,
the state of $sub$ changes, and we equip $g$ with the $even$ predicate
(defined on line 17) indicating that $b$ points to a boolean
indicating whether the predicate argument was even or not. Since $d
\pointsto 0$ and $b \pointsto \ctext{true}$ are the same as
$double(0)$ and $even(0)$, so we can write them in this form on line
10.  We can now invoke $\ctext{broadcast}(s, 5)$ on line 11, and
correspondingly the states of all three components of the state shift
in line 12.  In line 13, we expand $double$ and $even$ to see $d$
points to 10 (twice 5), and $b$ points to $\ctext{false}$ (since 5
is odd).

\textbf{Discussion.} One nice feature of the proof of the
subject-observer implementation is that the proofs are totally
oblivious to the concrete implementations of the notification
callbacks, or to any details of the observer invariants. Just as
existential quantification hides the details of a module
implementation from the clients, the universal quantification in the
specification of $\ctext{register}$ and $\ctext{broadcast}$ hides all
details of the client callbacks from the proof of the implementation
-- since they are free variables, we are unable to make any
assumptions about the code or predicates beyond the ones explicitly
laid out in the spec. Another benefit of the passage to higher-order
logic is the smooth treatment of observers with differing invariants;
higher-order quantification lets us store and pass formulas around,
making it easy to allow each callback to have a totally different
invariant. 


\input{htt-experiments}

\section{Related Work}
% \vspace{-0.5em}
The proof system is a synthesis of O'Hearn and
Reynolds's~\cite{sep-logic} work on separation logic, with Reynolds's
system of specification logic~\cite{spec-logic} for Algol, which
introduced the idea of turning Hoare triples into the atomic formulae
of a program logic. Birkedal, Biering, and Torp-Smith~\cite{hosl}
first extended separation logic to higher-order.

Parkinson developed a version of separation logic for Java in his
doctoral dissertation~\cite{parkinson-thesis}. His logic does not have
a notion of implications over specifications, instead using behavioral
subtyping to determine what specification dynamically dispatched
method calls could have. Parkinson and Bierman have also introduced a
notion of abstract predicate family~\cite{parkinson-bierman-05}
related to the higher-order quantification of Birkedal \emph{et al}.

% Nanevski, Morisett and Birkedal have developed Hoare Type
% Theory~\cite{htt}, which is a sophisticated dependently-typed
% functional language that, like our system, uses a monadic discipline
% to control effects.  Unlike our work, HTT takes advantage of type
% dependency to directly integrate specifications into the types of
% computation terms. Nanevski, Ahmad, Morisett and
% Birkedal~\cite{abstract-htt} have proposed using the existential
% quantification of their type theory to hide data representations,
% giving examples such as a malloc/free style memory allocator.

In addition to systems based on separation, there is also a line of
research based on the concept of object invariants and ownership.  The
Java modelling language (JML)~\cite{jml} and the Boogie
methodology~\cite{boogie} are two of the most prominent systems based
on this research stream. In Boogie, each object tracks its owner
object with a ghost field, and the ownership discipline enforces that
the heap have a tree structure. This allows the calculation of frame
properties without explosions due to aliasing, even though the
specification language remains ordinary first-order logic.

In his dissertation, Parkinson gave as an example a simple iterator
protocol, lacking the integration with composites we have exhibited.
Subsequently, we formalized a similar account of
iterators~\cite{iterator}, again lacking the integration with
composites. Jacobs, Meijer, Piessens and
Schulte~\cite{iterators-revisited} extend Boogie with new rules for
the coroutine constructs C\# uses to define iterators. Their solution
typifies the difficulties ownership-based approaches face with
iterators, which arise from the fact that iterators must have access
to the private state of a collection but may have differing
lifetimes. This work builds on Barnett and Naumann's generalization of
ownership to friendship~\cite{friends}, which allows object invariants
to have some dependency on non-owned objects.

The subject-observer pattern has been the focus of a great deal of effort,
given its prominence in important applications. Simultaneously with our own
initial formulation, Parkinson gave an example of verifying the
subject-observer protocol~\cite{parkinson-so}. Recently, Parkinson and
Distefano~\cite{jstar-parkinson-distefano} have implemented a tool to verify
these programs, and have demonstrated several examples including a verification
of a subject-observer pattern specified along these lines. The tool includes
automatic generation of loop invariants. 

The work of Barnett and Naumann is also capable of reasoning about the
subject-observer pattern, but only if all of the possible observers
are known at verification.  Leino and Schulte~\cite{boogie-sub-obs}
made use of Liskov and Wing's concept of history invariants or
monotonic predicates~\cite{liskov-wing} to give a more modular
solution. More recently, Shaner, Naumann and Leavens~\cite{ShanerLN07}
gave a ``grey-box'' treatment of the subject-observer pattern.
Instead of tracking the specifications of the observers in the
predicate, they give a model program that should approximates the
behavior of any actual notification method.

Pierik, Clarke and de Boer~\cite{creational-invariants} formalize another
extension to the Boogie framework which they name \emph{creation
  guards}, specifically to handle flyweights. They consider flyweights
an instance of a case where object invariants can be invalidated by
the allocation of new objects, and add guards to their specifications
to control allocation to the permitted cases. 



% \appendix
% \section{Appendix Title}

% This is the text of the appendix, if you need one.

\acks

This work was supported in part by NSF grant
CCF-0541021, NSF grant CCF-0546550, DARPA contract HR00110710019, and
the United States Department of Defense.

\bibliographystyle{abbrv}
\bibliography{patterns}


% \bibliographystyle{plainnat}
% 
% \begin{thebibliography}{}
% 
% \bibitem{smith02}
% Smith, P. Q. reference text
% 
% \end{thebibliography}

\end{document}

