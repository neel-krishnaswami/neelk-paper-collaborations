\chapter{Proving the Correctness of Design Patterns}

\section{Introduction}

The widespread use of object-oriented languages creates an opportunity
for designers of formal verification systems, above and beyond a
potential ``target market''. Object-oriented languages have been used
for almost forty years, and in that time practitioners have developed
a large body of informal techniques for structuring object-oriented
programs called \emph{design patterns}\cite{gof}.  Design patterns
were developed to both take best advantage of the flexibility
object-oriented languages permit, and to control the potential
complexities arising from the unstructured use of these features.

This pair of characteristics make design patterns an excellent set of
benchmarks for a program logic.  First, design patterns use higher
order programs to manipulate aliased, mutable state. This is a
difficult combination for program verification systems to handle, and
attempting to verify these programs will readily reveal weaknesses or
lacunae in the program logic. Second, the fact that patterns are
intended to structure and modularize programs means that we can use
them to evaluate whether the proofs in a program logic respect the
conceptual structure of the program -- we can check to see if we need
to propagate conceptually irrelevant information out of program
modules in order to meet our proof obligations. Third, we have the
confidence that these programs, though small, actually reflect
realistic patterns of usage.

In this chapter, we give good specifications for and verify the
following programs:

\begin{itemize}
\item We prove a collection and iterator implementation, which builds
  the Java aliasing rules for iterators into its specification, and
  which allows the construction of new iterators from old ones via the
  composite and decorator patterns.  

\item We prove a general version of the flyweight pattern (also known as
  hash-consing in the functional programming community), which is a
  strategy for aggressively creating aliased objects to save memory
  and permit fast equality tests. This also illustrates the use of the
  factory pattern.

\item We prove a general version of the subject-observer pattern in a
  way that supports a strong form of information hiding between the
  subject and the observers.
\end{itemize}

% Finally, we give machine-verified proofs of the correctness of the
% iterator and flyweight patterns in the Ynot extension of Coq, and
% compare them with the paper proofs. We also see that proper treatment
% of the subject-observer pattern seems to call for the use of an
% impredicative type theory.

\section{Iterators, Composites and Decorators}

The iterator pattern is a design pattern for uniformly enumerating the
elements of a collection. The idea is that in addition to a
collection, we have an auxiliary data structure called the iterator,
which has an operation $\nextiter$. Each time $\nextiter$ is
called, it produces one more element of the collection, with some
signal when all of the elements have been produced. The iterators are
mutable data structures whose invariants depend on the collection,
itself another mutable data structure. Therefore, most object oriented
libraries state that while an iterator is active, a client is only
permitted to call methods on a collection that do not change the
collection state (for example, querying the size of a collection). If
destructive methods are invoked (for example, adding or removing an
element), it is no longer valid to query the iterator again.

We also support operations to create new iterators from old ones, and
to aggregate them into composite iterators. For example, given an
iterator and a predicate, we can construct a new iterator that only
returns those elements for which the predicate returns true. This sort
of decorator takes an iterator object, and \emph{decorates} it to
yield an iterator with different behavior. Likewise, we can take two
iterators and a function, and combine them into a new,
\emph{composite} iterator that returns the result of a parallel
iteration over them both.  These sorts of synthesized iterators are found
in the \texttt{itertools} library in the Python programming language,
the Google Java collections library, or the C5 library~\cite{C5} for
C\#.

Aliasing enters into the picture, above and beyond the restrictions on
the underlying collections, because iterators are stateful
objects. For example, if we create a filtering iterator, and advance
the underlying iterator, then what the filtering iterator will return
may change. Even more strikingly, we cannot pass the same iterator
twice to a parallel iteration constructor -- the iterators must be
disjoint in order to correctly generate the two sequences of elements
to combine.

Below, we give a specification of an iterator pattern. We'll begin 
by describing the interface informally, in English, and then move on 
to giving formal specifications and explaining them. 

The interface consists of two types, one for collections, and one for
iterators. The operations the collection type supports are 1) creating
new mutable collections, 2) adding new elements to an existing
collection, and 3) querying a collection for its size. Adding new
elements to a collection is a destructive operation which modifies the
existing collection, whereas getting a collection's size does not
modify the collection.

The interface that the iterator type supports includes:
\begin{enumerate}
\item creating a new iterator on a collection,
\item destructively getting the next element from an iterator
  (returning an error value if the iterator is exhausted), and
\item transformations producing new iterators from old. The 
  transformations we support are:
  \begin{enumerate}
  \item a filter operation, which takes an iterator along with a
    boolean predicate, and returns an iterator which enumerates the
    elements satisfying the predicate, and
  \item a parallel map operation, which takes two iterators and a
    two-argument function, and returns an iterator which returns the
    result of enumerating the elements of the two iterators in
    parallel, and applying the function to each pair of elements.
  \end{enumerate}
\end{enumerate}

The aliasing protocol that our iterator protocol will satisfy is
essentially the same as the one the Java standard libraries specify in
their documentation.

\begin{itemize}
\item Any number of iterators can be created from a given collection.
  Each of these iterators depends on the state of the collection at
  its time of creation, and is only valid as long as the collection
  state does not change.

  Each iterator also has its own traversal state, which tracks how 
  many of the collection's elements it has yet to yield. 

\item Iterator can be constructed from other iterators, and these
  composite iterators depend on the traversal states of all of the
  iterators they are constructed from. As a result, they also depend
  on the state of each collection each constituent iterator depends
  upon. 

\item An iterator is valid only as long as none of the collections it
  depends on have been modified, and it is the only thing that has the
  right to modify the traversal state of the iterators it depends on.

  It is only legal to call functions on an iterator when it is in a
  valid state. Performing a destructive operation on any collection an
  iterator depends upon invalidates it. For example, adding an element
  to any collection an iterator depends on will invalidate the
  iterator, as will enumerating the elements from any of the iterators
  that it depends on.

  Likewise, only the iterator itself may modify the traversal state of
  any iterator it depends upon. Any other modification may invalidate
  it.
\end{itemize}

\subsection{The Iterator Specification}

Now, we will describe the specification, given in
Figure~\ref{iterator-interface}, in detail. This whole specification
follows the usual pattern of introducing abstract types, predicates,
and operations with existential quantification, and then specifying
the behavior of the operations with a conjunction of Hoare triples.

In lines \ref{decl:colltype} and \ref{decl:itertype}, we introduce two
abstract type constructors, $\colltype$ and $\itertype$. These are
both type constructors of kind $\star \to \star$, and take an argument
which describes their element type. So $\colltype(\N)$ represents a
collection of natural numbers, and $\itertype(\bool)$ represents an
iterator which will produce a sequence of boolean values.

In lines \ref{decl:collpred} and \ref{decl:iterpred}, we give two
abstract predicates $\collpred$ and $\iterpred$, to represent the
state associated with a collection and iterator, respectively. The
sort of the collection predicate is $\Pi \alpha:\star.\;
\colltype(\alpha) \To \seqsort{\alpha} \To \assert \To \assert$, which
we will write using an expression like $\collpred_\alpha(c, xs, P)$.

The first argument is a type argument (e.g., $\alpha$), indexing the
whole predicate by the element type of the collection. (We will often
suppress this type argument when it is obvious from context.) The
second argument is an argument of collection type (in our example,
$c$, of type $\colltype(\alpha)$) which indicates the value with which
our state is associated. The third argument (here, $xs$) is the purely
mathematical sequence (i.e., an element of the free monoid over
$\alpha$) the collection $c$ represents.  The fourth, and final,
argument is a proposition-valued \emph{abstract state} of the
collection, which we use to track whether or not the collection has
been modified or not.

The appearance of this argument might be a little bit surprising:
naively, we might suppose the mathematical sequence that collection
represents constitutes a sufficient description of the collection, and
so we might expect our predicates to take on the form
$\collpred_\alpha(c, xs)$, with no state argument. However, this is
\emph{not} sufficient. The iterator contract forbids modifying the
collection at all, while iterators are active upon it, and the
mathematical sequence a collection represents is not enough
information to decide whether a collection has been modified or not.

For example, suppose we have a collection $c$ representing the
mathematical sequence $\left<2, 3, 4, 5\right>$, which might have the
predicate $\collpred_\N(c, \left<2, 3, 4, 5\right>, P)$.  Now, suppose
we first add the element $1$ to the front of the sequence (so that the
collection $c$ now represents $\left<1, 2, 3, 4, 5\right>$), and then
immediately remove the first element. Then, the collection $c$ will
still represent the sequence $\left<2, 3, 4, 5\right>$, but it will
have suffered an intervening modification.

This kind of change can be catastrophic for iterator implementations.
As a concrete example, suppose that we had represented our collection
with a balanced binary tree, and represent the iterator as a pointer
into the middle of that tree. Adding and removing elements from the
collection can cause a tree rebalancing, which can potentially leave
the iterator pointer with a stale or dangling pointer.

As a result, our specification must track whether a collection has
been modified or not, and this is what the abstract state field on the
predicate does. Operations on a collection which do not change its
underlying state will leave the abstract state unchanged from pre- to
post-condition, whereas destructive operations (such as adding or
removing elements) do change the abstract state.

On line \ref{decl:iterpred}, we assert the existence of the iterator
predicate $\iterpred$.  It is also a four-place predicate, and has
sort $\Pi \alpha:\star.\; \itertype(\alpha) \To \seqsort{\alpha} \To
\powerset{\Sigma \beta:\star.\;\colltype(\beta) \times \seqsort{\beta}
  \times \;\assert} \To \assert$, which we will write using an
expression like $\iterpred_\alpha(i, xs, S)$.

The first argument (in our example, $\alpha$) is a type argument
describing the type of elements the iterator will produce. The second
argument (here, $i$) is the concrete iterator value to which the
predicate is associated. Then, we have a sequence argument (here,
$xs$) which describes the elements yet to be produced from this
iterator -- if $xs = \left<5, 7, 9\right>$, then the next three
elements the iterator will yield are 5, 7, and 9, in that order.
Subsequently the iterator will be empty and unable to produce any
more elements.

Finally, we have a state argument for iterators, as well. In contrast
to the case for collections, this argument is a set of propositions,
representing an entire set of collection states. Since our interface
supports operations which allow building new iterators from old, an
iterator may access many different collections to produce a single new
element. As a result, we have to track the state of each collection
the iterator depends on, so that we can verify that we do not ever
need to read a modified collection.

\begin{figure}
\mbox{}
\begin{specification}
\nextlinelabel{decl:colltype}
$\exists \colltype : \star \to \star$  
\nextlinelabel{decl:itertype}
$\exists \itertype : \star \to \star$  
\nextlinelabel[0.5em]{decl:collpred}
$\exists \collpred : 
        \Pi \alpha:\star.\; \colltype(\alpha) \To \seqsort{\alpha} \To \assert \To \assert.$ 
\nextlinelabel{decl:iterpred}
$\exists \iterpred : 
        \Pi \alpha:\star.\; \itertype(\alpha) \To \seqsort{\alpha} \To \powerset{\Sigma \alpha:\star.\; \colltype(\alpha) \times \seqsort{\alpha} \times \assert} \To \assert.$ 
\nextlinelabel[0.5em]{decl:newcoll-type}
$\exists \newcoll : 
         \forall \alpha:\star.\; \monad{(\colltype(\alpha))}.$
\nextlinelabel{decl:size-type}
 $\exists \sizecoll : 
         \forall \alpha:\star.\; \colltype(\alpha) \to \monad{\N}.$ 
\nextlinelabel{decl:add-type}
 $\exists \addcoll : 
         \forall \alpha:\star.\; \colltype(\alpha) \times \alpha \to \monad{\unittype}.$
\nextlinelabel{decl:remove-type}
 $\exists \removecoll :
         \forall \alpha:\star.\; \colltype(\alpha) \to \monad{(\opttype{(\alpha)})}$. 
\nextlinelabel[0.5em]{decl:newiter-type}
 $\exists \newiter : 
         \forall \alpha:\star.\; \colltype(\alpha) \to \monad{(\itertype(\alpha))}.$ 
\nextlinelabel{decl:filter-type}
 $\exists \filteriter : 
         \forall \alpha:\star.\; (\alpha \to \ctext{bool}) \times \itertype(\alpha) 
                                 \to \monad{(\itertype(\alpha))}.$ 
\nextlinelabel{decl:zip-type}
$\exists$\=$ \mergeiter : 
         \forall \alpha,\beta,\gamma:\star.\; (\alpha \to \beta \to \gamma) \times
                                 \itertype(\alpha) \times \itertype(\beta) 
                                   \to \monad{(\itertype(\gamma))}.$
\nextlinelabel{decl:next-type}
$\exists$\=$\nextiter : 
         \forall \alpha:\star.\; \itertype(\alpha) \to \monad{(\opttype{(\alpha)})}.$  
\nextlinelabel[0.5em]{decl:newcoll-spec}

\> $\forall \alpha.\; \mspec{\emp}{{\newcoll_\alpha}}
                                 {a:\colltype(\alpha)}{\exists P.\; \collpred_\alpha(a, \epsilon,P)}$ $\specand$ 
\nextlinelabel[0.5em]{decl:size-spec}

\> $\forall \alpha, c, P, xs.\;$\=
         $\angles{\collpred_\alpha(c, xs, P)}$
\nextline
\> \>  ${\sizecoll_\alpha(c)}$ 
\nextline
\> \>  $\angles{a:\N.\; \collpred_\alpha(c, xs, P) \land a = |xs|}$  $\specand$ 
\nextlinelabel[0.5em]{decl:add-spec}

 \> $\forall \alpha, c, P, x, xs.\;$\=
               $\angles{\collpred_\alpha(c, xs, P)}$ 
\nextline
\>\>         ${\addcoll_\alpha(c, x)}$
\nextline
\>\>         $\angles{a:1.\; \exists Q.\; \collpred_\alpha(c, x\cdot xs, Q)}$ $\specand$ 

\nextlinelabel[0.5em]{decl:remove-spec-1}

\> $\forall \alpha, c, P.\; \spec{\collpred_\alpha(c, \epsilon, P)}
                                        {{\removecoll_\alpha(c)}}
                                        {a:\opttype{(\alpha)}}
                                        {\collpred_\alpha(c, \epsilon, P) \land a = \None}$
$\specand$ 
\nextlinelabel{decl:remove-spec-2}
\> $\forall \alpha, c, x, xs, P.$\=
          $\angles{\collpred_\alpha(c, x\cdot xs, P)}$ 
\nextline
\>\> ${\removecoll_\alpha(c)}$ 
\nextline
\>\> $\angles{a:\opttype{(\alpha)}.\;
             \exists Q.\;\collpred_\alpha(c, xs, Q) \land a = \Some(x)}$ 
\nextlinelabel[0.5em]{decl:newiter-spec}

\> $\specand \forall \alpha, c, P, xs.\;$\=
            $\angles{\collpred_\alpha(c, xs, P)}$ 
\nextline
\>\>${\newiter_\alpha(c)}$
\nextline
\>\>$\angles{a:\itertype(\alpha).\; \collpred(c, xs, P) * \iterpred(a, xs, \setof{(\alpha, c, xs, P)})}$ $\specand$ 
\nextlinelabel[0.5em]{decl:filter-spec}

 \> $\forall \alpha, p, i, S, xs.\;$\=
         $\angles{\iterpred_\alpha(i, xs, S)}$ 
\nextline
\>\>   ${\filteriter_\alpha(p, i)}$
\nextline
\>\>   $\angles{a:\itertype(\alpha).\; \iterpred_\alpha(a, \mathit{filter}\; p\;xs, S)}$ $\specand$ 
\nextlinelabel[0.5em]{decl:zip-spec}

 \> $\forall \alpha, f, i, S,$\=$ xs, i', S', xs'.\;$ 
\nextline
 \> \> 
     $\angles{\iterpred_\alpha(i, xs, S) * \iterpred_\beta(i', xs', S') \land S \cap S' = \emptyset}$ 
\nextline
 \> \> ${\mergeiter_{\alpha\;\beta\;\gamma}(f, i, i')}$ 
\nextline
 \> \> $\angles{a:\itertype(\gamma).\; \iterpred_{\gamma}(a, \mathit{map2}\;f\;xs\;xs', S \cup S')}$ $\specand$ 
\nextlinelabel[0.5em]{decl:next-spec-1}

 \> $\forall i, S.\;$\=
      $\angles{\mathit{colls}(S) * \iterpred_\alpha(i, \epsilon, S)}$
\nextline  
\>\>${\nextiter_\alpha(i)}$ 
\nextline
\>\>$\angles{a:\opttype{(\alpha)}.\; \mathit{colls}(S) * \iterpred_\alpha(i, \epsilon, S) \land a = \None}$ $\specand$ 
\nextlinelabel[0.5em]{decl:next-spec-2}

 \> $\forall i, $\=$ S, x, xs.\;$ \= 
      $\angles{\mathit{colls}(S) * \iterpred_\alpha(i, x \cdot xs, S)}$
\nextline
\>\>\>${\nextiter(i)}$
\nextline
\>\>\>$\angles{a:\opttype{(\alpha)}.\; 
              \mathit{colls}(S) * \iterpred_\alpha(i, xs, S) \land a = (\Some\;x)}$ 

\end{specification}
\caption{Interface to the Iterator Library}
\label{iterator-interface}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{figure}
\mbox{}
\begin{specification}
\nextline
$\mathit{colls}(\emptyset) \qquad\qquad\qquad\qquad\;\;$ \=$\equiv\;$\= $\emp$ 
\nextline
$\mathit{colls}(\setof{(\alpha, c,xs , P)} \uplus S)$ \> $\equiv$ \> $\collpred_\alpha(c, xs, P) * \mathit{colls}(S)$ 

\nextlinelabel[0.5em]{decl:mathfilter-impl}
$\mathit{filter}\;p\;\epsilon \qquad\quad$\=$\equiv \epsilon$
\nextline
$\mathit{filter}\;p\;(x\cdot xs)$ \> $\equiv \ctext{if}\;p\;x = \ctext{true}\;\ctext{then}\;
                                         x\cdot(\mathit{filter}\;p\;xs)\;
                                      \ctext{else}\; \mathit{filter}\;p\;xs$ 

\nextlinelabel[0.5em]{decl:map2-impl}
$\mathit{map2}\;f\;\epsilon\;ys \qquad\qquad\qquad $\=$= \epsilon$ 
\nextline
$\mathit{map2}\;f\;xs\;\epsilon $\>$= \epsilon$ 
\nextline
$\mathit{map2}\;f\;(x\cdot xs)\;(y\cdot ys)$ \>$= (f\;x\;y)\cdot(\mathit{map2}\;f\;xs\;ys)$
    
\end{specification}
\caption{Auxilliary Functions Used in the Iterator Specification}
\label{iterator-interface-aux}
\end{figure}

The operation $\newcoll$ is declared on line \ref{decl:newcoll-type},
and specified on line \ref{decl:newcoll-spec}. The specification
asserts that the call $\newcoll_\alpha$ may happen from any
precondition state, and that it creates and adds a new, empty
collection to the program state. The postcondition assertion $\exists
P.\; coll_\alpha(a, \epsilon, P)$ says that the return value $a$ is a
collection representing the empty sequence $\epsilon$, and that the
collection begins its life in some arbitrary abstract state $P$.

The $\sizecoll_\alpha(c)$ function, which is declared on line
\ref{decl:size-type} and specified on line \ref{decl:size-spec}, takes
a type argument $\alpha$ and a collection $c$, and returns the number
of elements in $c$. To call this function, we must have access to the
collection $\collpred_\alpha(c, xs, P)$ in our precondition, and it is
returned to us unchanged in the postcondition, with the return value
$a$ equal to the length of $xs$, the sequence $c$ represents. In
particular, note that the abstract state $P$ of the $coll(c, xs, P)$
predicate remains unchanged in the pre- and post-conditions,
indicating that this function does not change the abstract state.

The function call $\addcoll_\alpha(c, x)$, which adds an element $x$
to a collection $c$, is declared on line \ref{decl:add-type} and is
specified on line \ref{decl:add-spec}. We start with a precondition
$\collpred_\alpha(c, xs, P)$ and move to a postcondition state
$\exists Q.\; \collpred_\alpha(c, x\cdot xs, Q)$. Because we
destructively modify the collection $c$ when we add $x$ to it, we also
specify that the abstract state in the postcondition is existentially
quantified. This ensures that clients cannot assume that the abstract
state remains the same after a call to $\addcoll$ has been made. In
this way, the abstract state behaves a bit like a time stamp, changing
to some new state whenever a modification is made to the collection. 

Similarly, the function call $\removecoll_\alpha(c)$ (declared on
line \ref{decl:remove-type}) removes an element from the collection
$c$. We give this procedure two specifications, on lines
\ref{decl:remove-spec-1} and \ref{decl:remove-spec-2}, corresponding
to when the collection is empty, or not.  In the first specification
(on line \ref{decl:remove-spec-1}), we begin with the precondition
$\collpred_\alpha(c, \epsilon, P)$, and end in the postcondition
$\collpred_\alpha(c, \epsilon, P) \land a = \None$. The fact that the
abstract state remains $P$ means the collection is unchanged, and the
return value $a$ equals $\None$, an element of option type, indicating
that there was no element to remove.  In the second specification (on
line \ref{decl:remove-spec-2}), we begin with the precondition
$\collpred_\alpha(c, x\cdot xs, P)$, from which we can see that the
collection is nonempty. Then, the postcondition is $\exists Q.\;
\collpred_\alpha(c, xs, Q) \land a = \Some(x)$.  The value $\Some(x)$
is returned as the return value of the function, and the state of the
collection changes to reflect that the element $x$ has been removed
--- including a change to the abstract state of the collection.

As an aside, in practice it is usually more convenient to specify a
procedure with a single Hoare triple, rather than multiple Hoare
triples. However, in this example, I choose to give multiple
specifications of the same procedure in order to illustrate that it is
indeed possible within specification logic.

The $\newiter_\alpha(c)$ function is declared on line
\ref{decl:newiter-type}.  Its type is $\forall \alpha:\star.\;
\colltype(\alpha) \to \monad{(\itertype(\alpha))}$.  This means that
it is given a type and a collection of that type, and then it returns
iterator over that type, possibly creating auxilliary data structures.

A call $\newiter_\alpha(c)$ is specified on line
\ref{decl:newiter-spec}, and beginning from a precondition state
$\collpred_\alpha(c, xs, P)$, it goes to a postcondition state
$\collpred_\alpha(c, xs, P) * \iterpred_\alpha(a, xs, \setof{(c, xs, P)})$.
This means that given access to a collection $c$, our function will
return an iterator object (bound to $a$), which will enumerate the
elements of $c$ (that is, it will produce the elements
$xs$). Furthermore, the abstract state that it depends on is just the
singleton set $\setof{P}$, since this iterator will read only $c$.
Finally, the fact that $\collpred_\alpha(c, xs, P)$ occurs in both
the pre- and the post-condition means that this function needs 
access to $c$'s state, but does not modify its abstract state. 

The $\filteriter_\alpha(p, i)$ (declared on line \ref{decl:filter-type})
takes a boolean function $p$ and an iterator $i$, and returns a new
iterator which will enumerate only those elements which for which $p$
returns true. This function is specified on line \ref{decl:filter-spec}, 
and it takes a precondition $\iterpred_\alpha(i, xs, S)$ to a postcondition
$\iterpred_\alpha(a, \mathit{filter}\;p\;xs, S)$. 

First, note that we use a mathematical function $\mathit{filter}$ to
explain the filtering behavior in terms of sequences. Second, note
that the original iterator state $\iterpred_\alpha(i, xs, S)$ vanishes
from the postcondition -- it is consumed by the call the
$\filteriter$.  This reflects the fact the filtered iterator takes
ownership of the underlying iterator, in order to prevent third
parties from making calls to $\nextiter(i)$ and possibly changing the
state of the filtered iterator.

This is also why the support set $S$ for an iterator only needs to
track the abstract states of the collections, rather than tracking the
state of both collections and iterators. When we take ownership of the
argument's iterator state, we prevent third parties from being able to
call functions on the argument after creating the new iterator. This
takes advantage of the resource-conscious nature of separation logic:
a specification must have access to its footprint, and so we can hide
state inside a predicate to control which operations are allowed.

The $\mergeiter$ function is declared on line $\ref{decl:zip-type}$, and
has type $\forall \alpha,\beta,\gamma:\star.\; (\alpha \to \beta \to \gamma) \times \itertype(\alpha) \times
\itertype(\beta) \to \monad{(\itertype(\gamma))}$.  Thus,
a call $\mergeiter_{\alpha\;\beta\;\gamma}(f, i_1, i_2)$ takes a function and two 
iterators, and constructs a new iterator which steps over the two inputs in parallel,
returning the result of $f$ applied to each pair of elements of $i_1$ and $i_2$. 

We specify calls $\mergeiter_{\alpha\;\beta\;\gamma}(f, i_1, i_2)$ on line
$\ref{decl:zip-spec}$, and it takes a precondition
$\iterpred_\alpha(i_1, xs, S_1) * \iterpred_\beta(i_2, ys, S_2)$.
This means that we have state associated with two separate iterators,
which we take to the postcondition $\iterpred_{\gamma}(a,
\mathit{map2}\;f\;xs\;ys, S_1 \cup S_2)$. As with the $\filteriter$ function, 
we consume the two input iterators to produce the return value iterator. 
And also as with $\filteriter$, we use a mathematical function $\mathit{map2}$
to specify the action on mathematical sequences. 

One point worth noting is that it is important that the two argument
iterators have separate state from one another. In a functional
program, there is no difficulty with a program
$\mathit{map2}\;f\;xs\;xs$, because we are free to re-traverse a list
multiple times. However, since traversing an iterator is a destructive
operation, a call like $\mergeiter_{\alpha,\alpha,\beta}\;f\;i\;i$
could (if it were allowed) give the wrong answer, for example by
pairing consecutive elements of the iterator.

The final operation in our interface is the $\nextiter$ function,
declared on line \ref{decl:next-type}. The type of this function
is $\forall \alpha:\star.\; \itertype(\alpha) \to
\monad{(\opttype{(\alpha)})}$.  When invoked, it will return an
option, with the $\None$ value if the iterator is exhausted,
and $\Some$ of an element if the iterator still has elements to
produce.

As with $\removecoll$, we specify this procedure with two
specifications, one for the case when the iterator is empty and
another for one it is non-empty. On line \ref{decl:next-spec-1}, we
give the specification for when the iterator is exhausted, and
on line \ref{decl:next-spec-2}, we give the specification for when
the iterator is not exhausted.  

In either case, the precondition for the function contains as one part
the predicate $\mathit{colls}(S)$.  The assertion-level function
$\mathit{colls}(S)$ is a function that iterates over a set of abstract
states, and re-associates them with collection predicates (coming from
the argument $S$) in the precondition, to form a predicate
$\collpred_{\tau_1}(c_1, xs_1, S_1) * \ldots * \collpred_{\tau_n}(c_n,
xs_n, S_n)$.  This expresses the requirement that we need access to
\emph{all} of the collections $i$ depends on, all in the correct
abstract state, in order to use it. 
This function is defined in
Figure~\ref{iterator-interface-aux}.\footnote{Technically, this is an
  abuse of notation, since primitive recursion is not well defined on
  sets.  The proper way to do this would be to introduce a binary
  relation $\mathit{colls(S, R)}$ between state sets and predicates,
  and putting $R$ in the precondition state. However, since the
  separating conjunction is commutative, no confusion is possible and
  I will retain the functional form.}

In line \ref{decl:next-spec-1}, $\mathit{colls}(C, S)$ is joined with 
the specification of the iterator, $\iterpred_\alpha(i, \epsilon, S)$. Note
that the same $S$ is used, so that we are referring only to the collections
the iterator may need to read.  As expected, $\nextiter_\alpha(i)$ returns 
$\None$.  On the other hand, if the iterator still has elements (i.e., is in a state
$iter_\alpha(i, x\cdot xs, S)$), we use the specification on line \ref{decl:next-spec-2}, 
and see it returns the first element as
$\Some\;x$, and sets the state to $iter(i, S, xs)$ in the
postcondition (line 15). 

\subsubsection{Example Client}

Below, we give an example use of this module in annotated program
style. (Here, and in what follows, we suppress explicit type
applications when they are obvious in context.)

\begin{specification}
\nextline
 $\setof{\emp}$  \nextlinelabel{label:iterclient:makec1}
 $\letv{c_1}{\newcoll()}{}$  \nextline
 $\setof{\exists P'_1.\; coll(c_1, \epsilon, P'_1)}$  \nextlinelabel{label:iterclient:dropp1ex}
 $\setof{coll(c_1, \epsilon, P_1)}$  \nextlinelabel{label:iterclient:addc1}
 $\letv{()}{\addcoll(c_1, 4)}{}$  \nextline
 $\setof{\exists P_2.\; coll(c_1, 4\cdot\epsilon, P_2)}$  \nextlinelabel{label:iterclient:dropp2ex}
  $\setof{coll(c_1, 4\cdot\epsilon, P_2)}$  \nextlinelabel{label:iterclient:addc1again}
 $\letv{()}{\addcoll(c_1, 3)}{}$  \nextline
 $\letv{()}{\addcoll(c_1, 2)}{}$  \nextline
 $\setof{coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)}$  \nextlinelabel{label:iterclient:makec2}
 $\letv{c_2}{\newcoll()}{}$  \nextline
 $\letv{()}{\addcoll(c_2, 3)}{}$  \nextlinelabel{label:iterclient:finishc2}
 $\letv{()}{\addcoll(c_2, 5)}{}$  \nextlinelabel{label:iterclient:c2state}
 $\setof{coll(c_2, 5\cdot3\cdot\epsilon, Q_2) * coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)}$  \nextlinelabel{label:iterclient:makei1}
 $\letv{i_1}{\newiter(c_1)}{}$  \nextlinelabel{label:iterclient:i1pred}
 $\{$\=$iter(i_1, 2\cdot3\cdot4\cdot\epsilon, \setof{(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)})$
 \nextline 
 \>$*\; coll(c_2, 5\cdot3\cdot\epsilon, Q_2) * coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)\}$  \nextlinelabel{label:iterclient:makeevens}
 $\letv{i'_1}{\filteriter(even?, i_1)}{}$  \nextlinelabel{label:iterclient:evenspred}
 $\{$\=$iter(i'_1, 2\cdot4\cdot\epsilon, \setof{(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)})$  \nextline
  \>$*\; coll(c_2, 5\cdot3\cdot\epsilon, Q_2) * coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)\}$ \nextlinelabel{label:iterclient:makei2}
 $\letv{i_2}{\newiter(c_2)}{}$  \nextlinelabel{label:iterclient:i2state}
 $\{$\=$iter(i'_1, 2\cdot4\cdot\epsilon, \setof{(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)})$ \nextline
  \>$*\;iter(i_2, 5\cdot3\cdot\epsilon, \setof{(c_2, 5\cdot3\cdot\epsilon, Q_2)})$  \nextline
 \>$*\;coll(c_2, 5\cdot3\cdot\epsilon, Q_2) * coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)\}$  \nextlinelabel{label:iterclient:mapi}
 $\letv{i}{\ctext{map2}(plus, i'_1, i_2)}{}$  \nextline
 $\{$\=$iter(i, 7\cdot7\cdot\epsilon, \setof{(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4), (c_2, 5\cdot3\cdot\epsilon, Q_2)})$  \nextline
 \> $*\; coll(c_2, 5\cdot3\cdot\epsilon, Q_2) * coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)\}$  \nextlinelabel{label:iterclient:sizecoll}
 $\letv{n}{\sizecoll(c_2)}{}$  \nextline
 $\{$\=$n = 2 \;\land
iter(i, 7\cdot7\cdot\epsilon, \setof{(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4), (c_2, 5\cdot3\cdot\epsilon, Q_2)})$ \nextline
\>$*\;coll(c_2, 5\cdot3\cdot\epsilon, Q_2) * coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)\}$  \nextlinelabel{label:iterclient:nexti}

 $\letv{x}{\nextiter(i)}{}$  \nextline
 $\{$\= $n = 2 \land x = \ctext{Some }7 \;\land $ \nextlinelabel{label:iterclient:istate}
 \> $iter(i, 7\cdot\epsilon, \setof{(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4), (c_2, 5\cdot3\cdot\epsilon, Q_2)})$  \nextline
 \> $*\; coll(c_2, 5\cdot3\cdot\epsilon, Q_2) * coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)\}$  \nextlinelabel{label:iterclient:add}
 $\addcoll(c_2, 17)$  \nextline
 $\{$\= $n = 2 \land x = \ctext{Some }7 \;\land $ \nextline
 \> $iter(i, 7\cdot\epsilon, \setof{(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4), (c_2, 5\cdot3\cdot\epsilon, Q_2)})$  \nextlinelabel{label:iterclient:q3state}
 \> $* \; (\exists Q_3.\; coll(c_2, 17\cdot5\cdot3\cdot\epsilon, Q_3)) * coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)\}$  \nextline
 $\{$\= $\exists Q_2, Q_3, P_4. n = 2 \land x = \ctext{Some }7 \;\land $ \nextline
 \> $iter(i, 7\cdot\epsilon, \setof{(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4), (c_2, 5\cdot3\cdot\epsilon, Q_2)})$  \nextline 
 \> $* \; coll(c_2, 17\cdot5\cdot3\cdot\epsilon, Q_3) * coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)\}$  
\end{specification}


In line 1 of this example, we begin in an empty heap. In
line~\ref{label:iterclient:makec1}, we create a new collection $c_1$,
which yields us the state $\exists P'_1.\; coll(c_1, \epsilon, P'_1)$,
with an existentially quantified abstract state.

Because $P'_1$ is existentially quantified, we do not know what value
it actually takes on. However, if we prove the rest of the program
using a freshly-introduced variable $P_1$, then we know that the rest
of the program will work for \emph{any} value of $P_1$, because free
variables are implicitly universally quantified.  So it will work with
whatever value $P'_1$ had. So we drop the quantifier on line~\ref{label:iterclient:dropp1ex}, and
try to prove this program with the universally-quantified
$P_1$.\footnote{A useful analogy is the existential elimination rule
  in the polymorphic lambda calculus: we prove that we can use an
  existential by showing that our program is well-typed no matter what
  the contents of the existential are.}

This permits us to $\addcoll$ the element 4 to $c_1$ on
line~\ref{label:iterclient:addc1}. Its specification puts the
predicate $coll()$ on line 6 again into an existentially quantified
state $P_2$. So we again replace $P_2$ with a fresh variable $P_2$ on
line~\ref{label:iterclient:dropp2ex}, and will elide these existential introductions and unpackings
henceforth.

Starting on line~\ref{label:iterclient:addc1again}, we add two more
elements to $c_1$, and on
lines~\ref{label:iterclient:makec2}-\ref{label:iterclient:finishc2},
we create another collection $c_2$, and add $3$ and $5$ to it, as can
be seen in the state predicate on
line~\ref{label:iterclient:c2state}. On
line~\ref{label:iterclient:makei1}, we create the iterator $i_1$ on
the collection $c_1$. The $iter$ predicate on
line~\ref{label:iterclient:i1pred} names $i_1$ as its value, and lists
$c_1$ in state $P_4$ as its support, and promises to enumerate the
elements 2, 3, and 4.

On line~\ref{label:iterclient:makeevens}, $\filteriter(even?, i_1)$
creates the new iterator $i'_1$. This iterator yields only the even
elements of $i_1$, and so will only yield 2 and 4. On
line~\ref{label:iterclient:evenspred}, $i_1$'s iterator state has been
consumed to make $i'_1$'s state. We can no longer call
$\nextiter(i_1)$, since we do not have the resource invariant needed
to prove anything about that call. Thus, we cannot write a program
that would break $i'_1$'s representation invariant.

On line~\ref{label:iterclient:makei2}, we create a second iterator
$i_2$ enumerating the elements of $c_2$. The state on line~\ref{label:iterclient:i2state} now has
predicates for $i'_1$, $i_2$, $c_1$ and $c_2$. On line~\ref{label:iterclient:mapi},
$\ctext{map2}(plus, i'_1, i_2)$ creates a new iterator $i$, which
produces the pairwise sum of the elements of $i'_1$ and $i_2$, and
consumes the iterator states for $i'_1$ and $i_2$ to yield the state
for the new iterator $i$. Note that the invariant for $i$ does not
make any mention of what it was constructed from, naming only the
collections it needs as support.
%Furthermore, the support of $i$ is the union of the
%supports of $i'_1$ and $i_2$ -- namely, the two collections $c_1$ and
%$c_2$.

On line~\ref{label:iterclient:sizecoll}, the $\sizecoll$ call on $c_2$
illustrates that we can call non-destructive methods while iterators
are active. The call to $\nextiter(i)$ on
line~\ref{label:iterclient:nexti} binds $\ctext{Some }7$ to $x$, and
the the iterator's sequence argument
(line~\ref{label:iterclient:istate}) shrinks by one element. On
line~\ref{label:iterclient:add}, we call $\addcoll(c_2, 17)$ the state
of $c_2$ changes to $\exists Q_3.\; coll(c, 17\cdot 5 \cdot
3\cdot\epsilon, Q_3)$ (line~\ref{label:iterclient:q3state}). So we can
no longer call $\nextiter(i)$, since it needs $c_2$ to be in the state
$Q_2$. (On the following 

\textbf{Discussion.} This example shows a pleasant synergy between
higher-order quantification and separation logic. We can give a
relatively simple specification to the clients of the collection
library, even though the internal invariant is quite subtle (as we
will see in the next section, it will use the magic wand). Higher-order
logic also lets us freely define new data types, and so our
specifications can take advantage of the pure, non-imperative nature
of the mathematical world, as can be seen in the specifications of the
$\filteriter$ and $\ctext{map2}$ functions -- we can use equational
reasoning on purely functional lists in our specifications, even
though our algorithms are imperative.


\subsection{Example Implementation}

In this subsection, we describe one particular implementation of
iterators, based on a simple linked list implementation. The type and
predicate definitions are given in Figure~\ref{iterator-pred-impl},
and the implementation of the procedures is given in
Figure~\ref{iterator-implementation}. 


\subsubsection{Definitions of the Types and Predicates}

We'll begin by giving an intuitive explanation of the predicates,
before giving correctness proofs for the operations.

Starting on line \ref{decl:colltype-impl} of
Figure~\ref{iterator-pred-impl}, we define the type of
collections. Technically, these are recursive type definitions, which
we did not define in our semantics in Chapter 1. Fortunately, there is
no great difficulty in these definitions --- we are giving polynomial
data types, and we can justify these definition via the fact that for
every polynomial functor $F$, the category of $F$-algebras over CPO
has an initial object (which means that the data type and primitive
iteration over it are well-defined).

The type of collections $\colltype(\tau)$ is a mutable linked list,
consisting of a reference to a value of type $\Listcontent(\tau)$. A
list content is either a $\Nil$ value, or a cons cell $\Cons(x, tl)$
consisting of a value of type $\tau$ and a tail list of type
$\colltype(\tau)$. Unlike the typical definition of purely functional
linked lists in ML, the tails of a list are mutable references, rather
than list values.

The type of iterators, given in line \ref{decl:itertype-impl}, is an
inductive datatype, with one clause for each of the possible ways to
construct a new iterator from an old one. This type arises as
follows. When we filter an iterator (or merge two iterators), we
simply store a function together with the iterator (or two iterators)
that are given as inputs. For an iterator into a single collection, we
store an interior pointer into the argument list, giving us the type
of a pointer to a list as the type of the $\One$ constructor.

Then, on line \ref{decl:listpred-impl}, we define the auxilliary
predicate $\listpred(\tau, c, xs)$, which asserts that $c$ is a list
value representing the mathematical sequence $xs$, with element type
$\tau$. This is defined by recursion over the sequence $xs$, with the
empty sequence represented by a pointer to $\Nil$, and a sequence 
$y\cdot ys$ represented by a pointer to a cons cell whose head is $x$
and whose tail is a collection representing $ys$. 

\begin{figure}
\mbox{}
\begin{specification}
\nextlinelabel{decl:colltype-impl}
$\Listcontent(\alpha) = \Nil \bnfalt \Cons\; \alpha \times \reftype{\Listcontent(\alpha)}$ 
\nextline
$\ctext{colltype}(\alpha) = \reftype{(\Listcontent(\alpha))}$ 
\nextlinelabel[0.5em]{decl:itertype-impl}
$\itertype{(\alpha)} =$\=$\;\One\;\reftype{(\colltype{(\alpha)})} \bnfalt 
                          \Filter\;(\alpha \to bool) \times \itertype{(\alpha)}$
\nextline
                       \>$\bnfalt \Merge\; (\alpha \to \alpha \to \alpha) \times \itertype(\alpha) \times \itertype(\alpha)$
\nextlinelabel[0.5em]{decl:listpred-impl}
$\listpred(\tau, c, xs)$ \qquad\= $\equiv \exists v.\; c \pointsto_\tau v * \listcontentpred(\tau, v, xs)$ 
\nextline
$\listcontentpred(\tau, \Nil, xs)$ \qquad\qquad\= $\equiv$ \= $xs = \epsilon$ 
\nextline
$\listcontentpred(\tau, \Cons(y,c), xs)$ \> $\equiv$ \> 
   $\exists ys.\; xs = y\cdot ys \land \listpred(\tau, c, ys)$ 
\nextlinelabel[0.5em]{decl:collpred-impl}
$collpred_\tau$\=$(c, xs, P) \equiv list(\tau, c, xs) \land P \land \mbox{exact}(P)$ 

\nextlinelabel[0.5em]{decl:iterpred-impl}
$\iterpred_\tau(\One\;i, xs, \setof{(\tau, \_, \_, P)}) \qquad\qquad$\=$\equiv \exists c.\; i \pointsto c * (P \wand (P \land (\top * \listpred(\tau, c, xs))))$ 
\nextline
$\iterpred_\tau(\Filter(p, i), xs, S)$ \>$\equiv 
  \exists ys.\; \iterpred_\tau(i, ys, S) \land xs = \mathit{filter}\;p\;ys$ 
\nextline
$\iterpred_{\tau}(\Merge(f, i_1, i_2), xs, S)$ \>$\equiv 
  \exists S_1, ys, S_2, zs.$ 
\nextline
  \> \qquad \= $\iterpred_\tau(i_1, ys, S_1) * \iterpred_\tau(i_2, zs, S_2) \;\land$
\nextline \> \>$S = S_1 \uplus S_2 \land xs = \mathit{map2}\;f\;ys\;zs$ 
\end{specification}
\caption{Type and Predicate Definitions of the Iterator Implementation}
\label{iterator-pred-impl}
\end{figure}

\begin{figure}
\mbox{}
\begin{specification}
\nextlinelabel[0.5em]{decl:newcoll-impl}
$\newcoll_\alpha \equiv \comp{\newref{\alpha}{\Nil}}$ 
\nextlinelabel[0.5em]{decl:sizecoll-impl}
$\sizecoll_\alpha(c) \equiv $\=
         $[$\=$\letv{p}{\comp{!c}}{}$ 
\nextline\> \>$\;
               \Listcase($\=$p,$ 
\nextline\> \>                   \>$\Nil \to \comp{0},$
\nextline\> \>                   \>$\Cons(\_, tl) \to 
                                     \comp{\letv{n}{\sizecoll_\alpha(tl)}{n+1}})]$

\nextlinelabel[0.5em]{decl:addcoll-impl}
$\addcoll_\alpha(c, x) \equiv [$
          \=$\letv{p}{\comp{!c}}{}$ 
\nextline \>$\letv{c'}{\newref{\Listcontent(\alpha)}{p}}{}$ 
\nextline \>$c := \Cons(x, c')]$ 

\nextlinelabel[0.5em]{decl:removecoll-impl}
$\removecoll_\alpha(c) \equiv [$\=
            $\letv{p}{\comp{!c}}{}$ 
\nextline \>$\Run\;\Listcase($\=$p,$ 
\nextline \>                                \>$\Nil \to \comp{\None},$ 
\nextline \>                                \>$\Cons(x, c') \to [$\=$\letv{p'}{\comp{!c'}}{}$ 
\nextline \>                                \> \>$\letv{\_}{\comp{c := p'}}{}$ 
\nextline \>                                \> \>$\Some(x)])]$

\nextlinelabel[0.5em]{decl:newiter-impl}
$\newiter_\alpha(c) \equiv \comp{\letv{i}{\comp{\newref{\colltype(\alpha)}{c}}}{\One(i)}}$

\nextlinelabel[0.5em]{decl:filteriter-impl}
$\filteriter_\alpha(p, i) \equiv \comp{\Filter(p, i)}$

\nextlinelabel[0.5em]{decl:mergeiter-impl}
$\mergeiter_\alpha(f, i, i') \equiv \comp{\Merge(f, i, i')}$ 

\nextlinelabel[0.5em]{decl:next-one-impl}
$\nextiter_\alpha(\One\;i) \equiv [$\=$\letv{c}{\comp{!i}}{}$ 
\nextline \> $\letv{p}{\comp{!c}}{}$ 
\nextline \> $\Run\;\Listcase($\=$p,$ 
\nextline \>                                  \>$\Nil \to \comp{\None},$ 
\nextline \>                                  \>$\Cons(x, c') \to [$\=$\letv{\_}{\comp{i := c'}}{\Some(x)}])]$ 
\nextlinelabel{decl:next-filter-impl}
$\nextiter_\alpha(\Filter(p, i)) \equiv [$\= 
           $\letv{v}{\nextiter_\alpha(i)}$ 
\nextline\>$\Run\;\Listcase($\=$v,$ 
\nextline\>\> $\None \to \comp{\None},$ 
\nextline\>\> $\Some\;x \to \IfThenElse{p\;x}{\comp{v}}{\nextiter_\alpha(\Filter(p,i))})]$ 
\nextlinelabel{decl:next-merge-impl}
$\nextiter_\alpha(\Merge(f, i_1, i_2)) \equiv [$\=
            $\letv{x_1}{\nextiter_\alpha(i_1)}{}$ 
\nextline\> $\letv{x_2}{\nextiter_\alpha(i_2)}{}$ 
\nextline\> $\Optcase($\=$x_1,$ 
\nextline\> \>           $\None \to \None,$ 
\nextline\> \>           $\Some\;v_1 \to \Optcase($\=$x_2,$ 
\nextline\> \> \>                                 $\None \to \None,$
\nextlinelabel{decl:next-impl-end}\> \> \>                                 $\Some\;v_2 \to f\;v_1\;v_2))]$
                                                   
\end{specification}
\caption{Implementation of Collections and Iterators}
\label{iterator-implementation}
\end{figure}

The collection predicate $\collpred_\tau(c, xs, P)$, defined on line
\ref{decl:collpred-impl}, makes use of the list predicate. In addition
to asserting that the value $c$ represents the sequence $xs$, it
asserts two further things. First, it says that this program state is
also described by the abstract predicate $P$, and that this predicate
is an \emph{exact} predicate.

Exact predicates are predicates that hold of exactly one heap: that
is, they are the atomic elements of the lattice of assertions. This
means that they uniquely identify a heap data structure. This property
lets us track modifications to the collection: any change to the actual
heap structure will result in the falsification of $P$.  

The iterator predicate, defined on line \ref{decl:iterpred-impl} is 
given as a recursive definition. The base case is when we have an 
iterator over a single collection, in the $\One(i)$ case.  Here, 
we have the following assertion: 

\begin{displaymath}
  \iterpred_\tau(\One(i), xs, \setof{(\tau, \_, \_, P)}) \equiv 
    \exists c.\; i \pointsto c * P \wand (P \land (\top * \listpred(\tau, c, xs)))
\end{displaymath}

The $i \pointsto c$ clause says that $i$ is a pointer to a linked
list.  The second clause of this invariant is more complex, and its
purpose is to say that $c$ is an interior pointer into a collection.

Note that $P$ will come from the abstract predicate field of some
collection, and so will be an exact predicate. So the conclusion 
of the magic wand, $(P \land (\top * \listpred(\tau, c, xs)))$, says
that the state is $P$, and can also be viewed as containing the 
list starting at $c$ and representing the sequence $xs$. 

Adding the magic wand to the formula , so that we get $(P \land (\top
* \listpred(\tau, c, xs)))$, ends up meaning, ``if you give me the
collection state $P$, then I promise to you that it contains $c$,
which represents $xs$.'' In this way, we express the fact that the
iterator's invariant depends on having controlled access to the state
of the collection. However, we are also able to avoid giving the
iterator state direct ownership of the collection; the magic wand
ensures that we can follow the pointer $c$ when we are given the
collection state of the collection.

The reason we go to this effort is to simplify the specification and
proof of client programs -- we could eliminate the use of the magic
wand in the base case if iterators owned their collections, but this
would complicate verifying programs that use multiple iterators over
the same collection, or which want to call pure methods on the
underlying collection. In those cases, the alternative would require
us to explicitly transfer ownership of the collection in the proofs of
client programs, which is quite cumbersome, and forces clients to
reason using the magic wand. The current approach isolates that
reasoning within the proof of the implementation.

Also, I should note that exactness plays a role in making this use of
the magic wand work correctly. The semantics of the magic wand $p
\wand q$ quantify over all heaps in $p$, but if $p$ is exact, then
there is at most one satisfying heap. This lets us treat the wand as a
``subtraction'' operator, which is ordinarily not legitimate. There is
possibly a connection to Parkinson's ``septraction''
operator~\cite{parkinson-septraction}. 

One the next line, we give the case for $\iterpred_\tau(\Filter(p, i),
xs, S)$.  This is a very simple formula; we simply assert that $i$ is
an iterator yielding some other sequence $ys$, which when filtered with
the predicate $p$ is $xs$. There are no changes to the set of abstract
states. 

On the line after that, we give the case for $\iterpred_\tau(\Merge(f, i_1, i_2), xs, S)$. 
This case says we can divide the abstract state into two parts, one of which is 
used by $i_1$, and the other of which is used by $i_2$,  which 
yield sequences $ys$ and $zs$ respectively, and which can be merged using
$f$ to yield $xs$. 

In both of these cases, we define the behavior of the imperative
linked list in terms of purely functional sequences. This is a very
common strategy in many verification efforts, but here we see that we
can use it in a local way -- in the base case, we are forced to
consider issues of aliasing and ownership, but in the inductive cases
we can largely avoid that effort.


\subsubsection{Correctness Proofs of the Iterator Implementation}

Now that we know the definitions of the types and predicates, we can
give the correctness proofs for the operations defined in
Figure~\ref{iterator-implementation}. 

\begin{lemma}{(Correctness of $\newcoll$)}
  We have that 
  \begin{displaymath}
    \forall \alpha.\; \spec{\emp}
                           {\run{\newcoll_\alpha}}
                           {a:\colltype(\alpha)}{\exists P.\; \collpred_\alpha(a, \epsilon,P)}
  \end{displaymath}
  is valid. 
\end{lemma}

\begin{proof}
All $\newcoll_\alpha$ does is allocate a list. To prove the
specification, we will assume that $\alpha:\star$, and then prove the
program in annotated program style.

\begin{specification}
\nextline $\setof{\emp}$ 
\nextline $\newref{\alpha}{\Nil}$ 
\nextline $\setof{a \pointsto_\alpha \Nil}$ 
\nextline $\setof{\listpred(\alpha, a, \epsilon)}$ 
\nextline $\setof{\listpred(\alpha, a, \epsilon) \land \exists Q.\; Q \land \exact{Q}}$ 
\nextline $\setof{\exists Q.\;\listpred(\alpha, a, \epsilon) \land Q \land \exact{Q}}$ 
\nextline $\setof{\exists Q.\; \collpred_\alpha(a, \epsilon, Q)}$ 
\end{specification}

Line 4 follows from 3, because of the definition of the list predicate. Line 5 
follows from 4, because this is an axiomatic property of all predicates -- if
a predicate holds, there is always at least one heap in which it holds. Line 6 
is a quantifier manipulation, and line 7 follows from the definition of the predicate.
\end{proof}

\begin{lemma}{(Correctness of $\sizecoll$)}
We have that 
\begin{displaymath}
\forall \alpha, c, xs, P.\; \spec{\collpred_\alpha(c, xs, P)}{\sizecoll_\alpha(c)}{a:\N}
                                  {\collpred_\alpha(c, xs, P) \land a = |xs|}
\end{displaymath}
is valid.   
\end{lemma}

\begin{proof}
This function, defined on line \ref{decl:sizecoll-impl} of Figure~\ref{iterator-implementation},
is a recursively defined function. So we will prove it using the fixed point rule. So we 
will assume an identifier $\sizecoll$ satisfying the specification above, and then prove
the correctness of the body, in annotated specification style. 

Now, assume we have $\alpha, c, xs,$ and $P$: 

\begin{specification}
\nextline $\setof{\collpred_\alpha(c, xs, P)}$ 
\nextline $\setof{\listpred(\alpha, c, xs) \land P \land \exact{P}}$ 
\nextline $\setof{\exists p.\; c \pointsto p \land \listcontentpred(\alpha, p, xs) \land P \land \exact{P}}$ 
\nextline $\letv{p}{\comp{!c}}{}$
\nextline $\setof{c \pointsto p \land \listcontentpred(\alpha, p, xs) \land P \land \exact{P}}$ 
\nextline $\Run\;\Listcase($\=$p,$ 
\nextline \> $\Nil \to $
\nextline \> \qquad \=$\setof{c \pointsto p \land p = \Nil \land xs = \epsilon \land P \land \exact{P}}$ 
\nextline \> \> \comp{0}
\nextline \> \> $\setof{c \pointsto \Nil \land p = \Nil \land xs = \epsilon \land a = 0 \land P \land \exact{P}}$ 
\nextline \> \> $\setof{\collpred_\alpha(c, xs, P) \land a = |xs|}$ 
\nextline \> $\Cons(y, c') \to $ 
\nextline \> \qquad \= $\setof{c \pointsto \Cons(y, c') * \listpred(\alpha, c', ys) \land xs = y\cdot ys) \land P \land \exact{P}}$ 
\nextline \> \> $[\letv{n}{\sizecoll_\alpha(c')}{}$
\nextline \> \> \,$\setof{c \pointsto \Cons(y, c') * \listpred(\alpha, c', ys) \land xs = y\cdot ys \land P \land \exact{P} \land n = |ys|}$ 
\nextline \> \> \,$\setof{\collpred_\alpha(c, xs, P) \land xs = y\cdot ys \land n = |ys|}$ 
\nextline \> \> \,$n+1]$
\nextline \> \> $\setof{\collpred_\alpha(c, xs, P) \land xs = y\cdot ys \land a = |ys| + 1}$ 
\nextline \> \> $\setof{\collpred_\alpha(c, xs, P) \land xs = y\cdot ys \land a = |y\cdot ys|}$ 
\nextline \> \> $\setof{\collpred_\alpha(c, xs, P) \land a = |xs|}$ 
\nextline $\setof{\collpred_\alpha(c, xs, P) \land a = |xs|}$ 
\end{specification}
\end{proof}

\begin{lemma}{(Specification of $\addcoll$)}
We have that for 
\begin{displaymath}
  \forall \alpha, x, c, xs, P.\; \spec{\collpred_\alpha(c, xs, P)}{\addcoll_\alpha(c, x)}
                                      {a:\unittype}{\exists Q.\; \collpred_\alpha(c, x\cdot xs, Q)}
\end{displaymath}
\end{lemma}

\begin{proof}
  This function, defined on line \ref{decl:addcoll-impl}, just conses on an element. 
Assume we have $\alpha, x, c, xs, P$, and then proceed with the following proof, in
annotated specification style. 

\begin{specification}
\nextline $\setof{\collpred_\alpha(c, xs, P)}$ 
\nextline $\setof{\listpred(\alpha, c, xs) \land P \land \exact{P}}$ 
\nextline $\setof{\listpred(\alpha, c, xs)}$ 
\nextline $\setof{\exists p.\; c \pointsto p * \listcontentpred(\alpha, p, xs)}$ 
\nextline $\letv{p}{\comp{!c}}{}$ 
\nextline $\setof{c \pointsto p * \listcontentpred(\alpha, p, xs)}$ 
\nextline $\letv{c'}{\comp{\newref{\Listcontent(\alpha)}{p}}}{}$
\nextline $\setof{c \pointsto p * c' \pointsto p * \listcontentpred(\alpha, p, xs)}$ 
\nextline $\setof{c \pointsto p * \listpred(\alpha, c', xs)}$ 
\nextline $c := \Cons(x, c')$ 
\nextline $\setof{c \pointsto \Cons(x, c') * \listpred(\alpha, c', xs)}$ 
\nextline $\setof{\listpred(\alpha, c, x\cdot xs)}$ 
\nextline $\setof{\exists Q.\; \listpred(\alpha, c, x\cdot xs) \land Q \land \exact{Q}}$ 
\nextline $\setof{\exists Q.\; \collpred_\alpha(c, x\cdot xs, Q)}$ 
\end{specification}
\end{proof}

\begin{lemma}
We have that 
\begin{displaymath}
  \forall \alpha, c, P.\; \spec{\collpred_\alpha(c, \epsilon, P)}
                               {\removecoll_\alpha(c)}{a:\opttype{(\alpha)}}
                               {\collpred_\alpha(c, \epsilon, P) \land a = \None}
\end{displaymath}
\end{lemma}

\begin{proof}
  This is one of the two specifications about $\removecoll$, for the case
  when the list is empty. Assume we have $\alpha, c, P$ and give an annotated
  specification as follows: 

\begin{specification}
\nextline $\setof{\collpred_\alpha(c, \epsilon, P)}$ 
\nextline $\setof{\exists p.\; c \pointsto p \land p = \Nil \land P \land \exact{P}}$ 
\nextline $\letv{p}{\comp{!c}}{}$ 
\nextline $\setof{c \pointsto p \land p = \Nil \land P \land \exact{P}}$ 
\nextline $\Run\;\Listcase($\=$p,$ 
\nextline \> $\Nil \to $\=$\comp{\None}$ 
\nextline \> \> $\setof{c \pointsto p \land p = \Nil \land P \land \exact{P} \land a = \None}$ 
\nextline \> \> $\setof{\collpred_\alpha(c, \epsilon, P) \land a = \None}$ 
\nextline \> $\Cons(y, c') \to $ 
\nextline \> \> $\setof{c \pointsto p \land p = \Nil \land p = \Cons(y, c') \land P \land \exact{P}}$ 
\nextline \> \> $\setof{\bot}$ 
\nextline \> \> $[\letv{p'}{\comp{!c'}}{}$ 
\nextline \> \> $\letv{\_}{\comp{c := p'}}{}$ 
\nextline \> \> $\Some(y)])$
\nextline \> \> $\setof{\collpred_\alpha(c, \epsilon, P) \land a = \None}$ 
\nextline $\setof{\collpred_\alpha(c, \epsilon, P) \land a = \None}$ 
\end{specification}
\end{proof}

\begin{lemma}{(Correctness of $\removecoll$, part 2)}
We have that 
\begin{displaymath}
  \forall \alpha, c, x, xs, P.\; \spec{\collpred_\alpha(c, x\cdot xs, P)}
                                      {\removecoll_\alpha(c)}
                                      {a:\opttype{(\alpha)}}
                                      {\exists Q.\; \collpred_\alpha(c, xs, Q) 
                                       \land a = \Some(x)}
\end{displaymath}
\end{lemma}

\begin{proof}
This is the other case of $\removecoll$, for when the iterator is not
yet exhausted. Assume that we have $\alpha, c, x, xs$, and $P$ of the
appropriate type. We can give an annotated-specification style proof
as follows:

\begin{specification}
\nextline $\setof{\collpred_\alpha(c, x\cdot xs, P)}$ 
\nextline $\setof{\exists p.\; c \pointsto p \land \exists c'.\; p = \Cons(x, c') * \listpred(\alpha, c', xs) \land P \land \exact{P}}$ 
\nextline $\letv{p}{\comp{!c}}{}$ 
\nextline $\setof{c \pointsto p \land \exists c'.\; p = \Cons(x, c') * \listpred(\alpha, c', xs) \land P \land \exact{P}}$ 
\nextline $\Run\;\Listcase($\=$p,$ 
\nextline \> $\Nil \to $\= $\setof{p = \Nil \land c \pointsto p \land \exists c'.\; p = \Cons(x, c') * \listpred(\alpha, c', xs) \land P \land \exact{P}}$ 
\nextline \> \> $\setof{\bot}$
\nextline \> \> $\comp{\None}$ 
\nextline \> \> $\setof{\exists Q.\; \collpred_\alpha(c, xs, Q) \land a = \Some(x)}$
\nextline \> $\Cons(y, c') \to $ 
\nextline \> \> $\setof{p = \Cons(y, c') \land c \pointsto p \land \exists c'.\; p = \Cons(x, c') * \listpred(\alpha, c', xs) \land P \land \exact{P}}$ 
\nextline \> \> $\setof{x = y \land c \pointsto p \land p = \Cons(x, c') * \listpred(\alpha, c', xs) \land P \land \exact{P}}$ 
\nextline \> \> $\setof{x = y \land c \pointsto p \land p = \Cons(x, c') * \listpred(\alpha, c', xs)}$
\nextline \> \> $\setof{x = y \land c \pointsto p \land p = \Cons(x, c') * \exists p'.\; c' \pointsto p' * \listcontentpred(\alpha, p', xs)}$
\nextline \> \> $[\letv{p'}{\comp{!c'}}{}$ 
\nextline \> \> $\setof{x = y \land c \pointsto p \land p = \Cons(x, c') * c' \pointsto p' * \listcontentpred(\alpha, p', xs)}$
\nextline \> \> $\letv{\_}{\comp{c := p'}}{}$ 
\nextline \> \> $\setof{x = y \land c \pointsto p' * c' \pointsto p' * \listcontentpred(\alpha, p', xs)}$
\nextline \> \> $\setof{x = y \land c' \pointsto p' * \listpred(\alpha, c, xs)}$
\nextline \> \> $\setof{x = y \land \listpred(\alpha, c, xs)}$
\nextline \> \> $\Some(y)])$
\nextline \> \> $\setof{\listpred(\alpha, c, xs) \land a = \Some(y) \land x = y}$ 
\nextline \> \> $\setof{\listpred(\alpha,c, xs) \land a = \Some(x)}$
\nextline \> \> $\setof{(\exists Q.\; \listpred(\alpha,c, xs) \land Q \land \exact{Q}) \land a = \Some(x)}$
\nextline \> \> $\setof{\exists Q.\; \collpred_\alpha(c, xs, Q) \land a = \Some(x)}$
\nextline $\setof{\exists Q.\; \collpred_\alpha(c, xs, Q) \land a = \Some(x)}$
\end{specification}
\end{proof}

\begin{lemma}{(Correctness of $\newiter$)}
We have that
\begin{displaymath}
\forall \alpha, c, xs, P.\; \spec{\collpred_\alpha(c, xs, P)}
                                 {\newiter_\alpha(c)}
                                 {a:\itertype(\alpha)}
                                 {\collpred_\alpha(c, xs, P) * 
                                  \iterpred_\alpha(a, xs, \setof{P})}
\end{displaymath}
is valid.
\end{lemma}

\begin{proof}
The definition of $\newiter$ is given on line \ref{decl:newiter-impl} of
Figure~\ref{iterator-implementation}. To prove the correctness of this
implementation, we assume we have $\alpha, c, xs$, and $P$, and then
give the following proof: 

\begin{specification}
\nextline $\setof{\collpred_\alpha(c, xs, P)}$ 
\nextline $\setof{\listpred(\alpha, c, xs, P) \land P \land \exact{P}}$ 
\nextline $\letv{i}{\comp{\newref{\colltype(\alpha)}{c}}}{}$ 
\nextline $\setof{\listpred(\alpha, c, xs) \land P \land \exact{P} * i \pointsto c}$ 
\nextline $\setof{\listpred(\alpha, c, xs) \land P \land \exact{P} * i \pointsto c * \emp}$ 
\nextline $\setof{\listpred(\alpha, c, xs) \land P \land \exact{P} * i \pointsto c * (P \wand P)}$
\nextline $\setof{\listpred(\alpha, c, xs) \land P \land \exact{P} * i \pointsto c * P \wand (P \land \listpred(\alpha, c, xs))}$ 
\nextline $\One(i)$ 
\nextline $\setof{\listpred(\alpha, c, xs) \land P \land \exact{P} * i \pointsto c * P \wand (P \land \listpred(\alpha, c, xs)) \land a = \One(i)}$ 
\nextline $\setof{\listpred(\alpha, c, xs) \land P \land \exact{P} * \iterpred_\alpha(a, xs, \setof{P})}$
\nextline $\setof{\collpred_\alpha(c, xs, P) * \iterpred_\alpha(a, xs, \setof{P})}$
\end{specification}

The key steps in this proof are from lines 5 to 7. On line 5, we introduce an extra $\emp$, which
entails $P \wand P$ on line 6. Then, because $P$ is strict exact, we know that for any $Q$, either $P \land Q = \bot$ or $P \land Q = P$. This justifies changing the formula to $P \wand (P \land \listpred(\alpha, c, xs))$ in line 7. If $P \land \listpred(\alpha, c, xs)$ is false, then the whole formula is false, since it occurs as part of the whole assertion. Otherwise, it's an equality, and we can rewrite the conclusion of the magic wand without changing the meaning of the assertion.
\end{proof}

\begin{lemma}{(Correctness of $\filteriter$)}
We have that
\begin{displaymath}
  \forall \alpha, p, i, xs, S.\; \spec{\iterpred_\alpha(i, xs, S)}
                                      {\filteriter_\alpha(i)}
                                      {a:\itertype(\alpha)}
                                      {\iterpred_\alpha(a, \filtermath\;p\;xs, S)}
\end{displaymath}
\end{lemma}

\begin{proof}
The definition of this procedure is given on line \ref{decl:filteriter-impl}. Assume
$\alpha, p, i, xs$, and $S$. 

\begin{specification}
\nextline $\setof{\iterpred_\alpha(i, xs, S)}$ 
\nextline $\Filter(p, i)$ 
\nextline $\setof{\iterpred_\alpha(i, xs, S) \land a = \Filter(p, i)}$ 
\nextline $\setof{\iterpred_\alpha(i, xs, S) \land \filtermath\;p\;xs = \filtermath\;p\;xs \land a = \Filter(p, i)}$ 
\nextline $\setof{\exists i, xs.\; \iterpred_\alpha(i, xs, S) \land \filtermath\;p\;xs = \filtermath\;p\;xs \land a = \Filter(p, i)}$
\nextline $\setof{\iterpred_\alpha(a, \filtermath\;p\;xs, S)}$ 
\end{specification}
\end{proof}

\begin{lemma}{(Correctness of $\mergeiter$)}
We have that 
\begin{displaymath}
  \forall \alpha, f, i, xs, S, i', xs', S'.\; 
  \begin{array}{l}
    \setof{\iterpred_\alpha(i, xs, S) * \iterpred_\alpha(i',xs',S')} \\
    \mergeiter_\alpha(f, i, i') \\
    \setof{a:\itertype(\alpha).\; \iterpred_\alpha(a, \mergemath\;f\;xs\;xs', S \cup S')}\\
  \end{array}
\end{displaymath}
\end{lemma}

\begin{proof}
The definition of $\mergeiter$ is given on line \ref{decl:mergeiter-impl} of
Figure~\ref{iterator-implementation}. Assume $f, i, xs, S, i', xs', S'$ as 
hypotheses, and proceed with the proof as follows: 

\begin{specification}
\nextline $\setof{\iterpred_\alpha(i, xs, S) * \iterpred_\alpha(i',xs',S') \land S \cap S' = \emptyset}$ 
\nextline $\Merge(f, i, i')$
\nextline $\setof{\iterpred_\alpha(i, xs, S) * \iterpred_\alpha(i',xs',S') \land a = \Merge(f, i', i')}$ 
\nextline $\{$\=$\iterpred_\alpha(i, xs, S) * \iterpred_\alpha(i',xs',S') \land S \cup S' = S \uplus S' \land \mergemath\;f\;xs\;xs' = \mergemath\;f\;xs\;xs'$
\nextline \> $\land a = \Merge(f, i', i')\}$ 
\nextline $\{$\=$\exists i, i', S, S'.\; \iterpred_\alpha(i, xs, S) * \iterpred_\alpha(i',xs',S') \land S \cup S' = S \cup S' \land \mergemath\;f\;xs\;xs' = \mergemath\;f\;xs\;xs'$ 
\nextline \> $\land a = \Merge(f, i', i')\}$ 
\nextline $\setof{\iterpred_\alpha(a, \mergemath\;f\;xs\;xs', S \cup S')}$
\end{specification}
\end{proof}

\begin{lemma}{(Correctness of $\nextiter$)}
We have that 
\begin{displaymath}
  \forall \alpha, i, S, xs.\; 
  \begin{array}{l}
    \setof{\iterpred_\alpha(i, xs, S) * \mathit{colls}(S)} \\
    \nextiter_\alpha(i) \\
    \{a:\opttype{(\alpha)}.\; \\
      \qquad [(a = \None \land xs = \epsilon \land \iterpred_\alpha(i, xs, S)) \vee \\
      \qquad (\exists y, ys.\; a = \Some(y) \land xs = y\cdot ys \land \iterpred_\alpha(i, ys, S))]
       \\
      \qquad * \mathit{colls}(S)\} \\
  \end{array}
\end{displaymath}
\end{lemma}

\begin{proof}
This function is defined on lines \ref{decl:next-one-impl} to \ref{decl:next-impl-end} 
of Figure~\ref{iterator-implementation}. To prove this, we will use fixed point induction,
and assume that the specification above holds for an identifier named $\nextiter$, and
use it to prove the correctness for the body of the function. 

Assuming $\alpha, i, S, xs$, this proof will proceed by cases, on
the structure of $i$. Then we can exploit the fact that we can unroll fixed
points and beta-reduce expression to avoid proving the impossible branches (which
in typical Hoare logic proofs are ruled out by getting false as a precondition). 

\begin{itemize}
\item Suppose $i = \One(i')$. Then, we proceed with an annotated proof as 
follows: 
\begin{specification}
\nextline $\setof{\iterpred_\alpha(\One(i'), xs, S) * \mathit{colls}(S)}$ 
\nextline $\setof{\exists P.\; \iterpred_\alpha(\One(i'), xs, \setof{(\alpha, c, ys, P)}) * \mathit{colls}_\alpha(\setof{(\alpha, c, ys, P)})}$ 
\nextline $\setof{\exists P.\; \iterpred_\alpha(\One(i'), xs, \setof{P}) * \collpred_\alpha(c, ys, P)}$
\nextline $\setof{\exists c'.\; i' \pointsto c' * (P \wand (P \land (\top * \listpred(\alpha, c', xs)))) * \listpred(\alpha,c, ys) \land P \land \exact{P}}$ 
\nextline $\setof{\exists c'. i' \pointsto c' * P \land \listpred(\alpha, c, ys) \land (\top * \listpred(\alpha, c', xs)) \land \exact{P}}$ 
\nextline $\letv{c'}{\comp{!i'}}{}$ 
\nextline $\setof{i' \pointsto c' * P \land \listpred(\alpha, c, ys) \land (\top * \listpred(\alpha, c', xs)) \land \exact{P}}$ 
\nextline $\setof{i' \pointsto c' * P \land \listpred(\alpha, c, ys) \land (\top * \exists v.\; c' \pointsto v * \listcontentpred(\alpha, v, xs)) \land \exact{P}}$ 
\nextline $\letv{v}{\comp{!c'}}{}$ 
\nextline $\setof{i' \pointsto c' * P \land \listpred(\alpha, c, ys) \land (\top * c' \pointsto v * \listcontentpred(\alpha, v, xs)) \land \exact{P}}$ 
\nextline $\Run\;$\=$\Listcase(v,$ 
\nextline \> $\Nil $\=$\to$ 
\nextline \> \> Framing $\exact{P}$
\nextline \> \> $\setof{v = \Nil \land i' \pointsto c' * P \land \listpred(\alpha, c, ys) \land (\top * c' \pointsto v * \listcontentpred(\alpha, v, xs))}$ 
\nextline \> \> $\setof{i' \pointsto c' * P \land \listpred(\alpha, c, ys) \land (\top * c' \pointsto \Nil * xs = \epsilon}$ 
\nextline \> \> $\comp{\None}$ 
\nextline \> \> $\setof{a = \None \land i' \pointsto c' * P \land \listpred(\alpha, c, ys) \land (\top * c' \pointsto \Nil * xs = \epsilon}$ 
\nextline \> \> $\setof{a = \None \land ys = \epsilon \land i' \pointsto c' * P \land \listpred(\alpha, c, ys)  \land (\top * \listpred(\alpha, c', xs)) }$ 
\nextline \> \> Restoring frame $\exact{P}$ 
\nextline \> \> $\setof{a = \None \land xs = \epsilon \land i' \pointsto c' * P \land \listpred(\alpha, c, ys) \land (\top * \listpred(\alpha, c', xs)) \land \exact{P}}$ 
\nextline \> \> $\setof{a = \None \land xs = \epsilon \land \iterpred_\alpha(i, xs, \setof{(\alpha, c, ys, P)}) * colls(\setof{(\alpha, c, ys, P)})}$
\nextline \> $\Cons(z, c'') \to$ 
\nextline \> \> Framing $\exact{P}$
\nextline \> \> $\setof{v = \Cons(z, c'') \land i' \pointsto c' * P \land \listpred(\alpha,c,ys) \land (\top * c' \pointsto v * \listcontentpred(\alpha, v, xs))}$  
\nextline \> \> $\setof{i' \pointsto c' * P \land \listpred(\alpha,c,ys) \land (\top * c' \pointsto \Cons(z,c'') * \listcontentpred(\alpha, \Cons(z,c''), xs))}$  
\nextline \> \> $\setof{i' \pointsto c' * P \land \listpred(\alpha,c,ys) \land (\top * c' \pointsto \Cons(z,c'') * \exists zs.\; xs = z\cdot zs \land \listpred(\alpha, c'', xs))}$  
\nextline \> \> $\setof{i' \pointsto c' * P \land \listpred(\alpha,c,ys) \land (\top * c' \pointsto \Cons(z,c'') * xs = z\cdot zs \land \listpred(\alpha, c'', xs))}$  
\nextline \> \> $[\letv{\_}{i' := c''}{}$ 
\nextline \> \> $\setof{i' \pointsto c'' * P \land \listpred(\alpha,c,ys) \land (\top * c' \pointsto \Cons(z,c'') * xs = z\cdot zs \land \listpred(\alpha, c'', xs))}$  
\nextline \> \> $\setof{i' \pointsto c'' * P \land \listpred(\alpha,c,ys) \land (\top * xs = z\cdot zs \land \listpred(\alpha, c'', xs))}$  
\nextline \> \> $\setof{xs = z\cdot zs \land i' \pointsto c'' * P \land \listpred(\alpha,c,ys) \land (\top * \listpred(\alpha, c'', xs))}$  
\nextline \> \> Restoring frame $\exact{P}$ 
\nextline \> \> $\setof{xs = z\cdot zs \land i' \pointsto c'' * P \land \listpred(\alpha,c,ys) \land (\top * \listpred(\alpha, c'', xs)) \land \exact{P}}$  
\nextline \> \> $\setof{xs = z\cdot zs \land i' \pointsto c'' * (P \land \listpred(\alpha,c,ys) \land \exact{P}) * (P \wand (P \land (\top * \listpred(\alpha, c'', xs))))}$  
\nextline \> \> $\setof{xs = z\cdot zs \land \collpred_\alpha(c, zs, P) * \iterpred_\alpha(\One(i'),zs,\setof{(\alpha, c, zs, P)})}$  
\nextline \> \> $\Some(z)]$ 
\nextline \> \> $\setof{a. \exists z, zs.\; xs = z\cdot zs \land a = \Some(z) \land \collpred_\alpha(c, zs, P) * \iterpred_\alpha(\One(i'),zs,\setof{(\alpha, c, zs, P)})}$  
\end{specification}

\item Suppose $i = \Filter(p, i')$. Then, we proceed with an annotated proof as 
follows: 
\begin{specification}
\nextline $\setof{\iterpred_\alpha(\Filter(p, i'), xs, S) * \mathit{colls}(S)}$ 
\nextline $\setof{\exists ys. \iterpred_\alpha(i', ys, S) \land 
                              xs = \mathit{filter}\;p\;ys * 
                              \mathit{colls}(S)}$   
\nextline $\setof{xs = \mathit{filter}\;p\;ys \land
                  \iterpred_\alpha(i', ys, S) *
                  \mathit{colls}(S)}$
\nextline $[\letv{v}{\nextiter_\alpha(i')}{}$ 
\nextline \{\=$[(ys = \epsilon \land v = \None) \vee 
                (\exists z, zs.\; ys = z\cdot zs \land v = \Some(z))] \land
               xs = \mathit{filter}\;p\;ys \;\land$ 
\contline $\iterpred_\alpha(i', ys, S) * \mathit{colls}(S)$\}
\nextline \;$\Run$\=$\;\Listcase(v,$ 
\nextline \> $\None$\=$ \to [\None]$ 
\nextline \> \{$a = \None \land 
               (ys = \epsilon \land v = \None) \land 
                xs = \mathit{filter}\;p\;ys \;\land
                \iterpred_\alpha(i', ys, S) * \mathit{colls}(S)$\}
\nextline \> \{$a = \None \land xs = \epsilon \land
                xs = \mathit{filter}\;p\;ys \;\land
                \iterpred_\alpha(i', ys, S) * \mathit{colls}(S)$\}
\nextline \> \{$a = \None \land xs = \epsilon \land
                \exists ys.\;
                xs = \mathit{filter}\;p\;ys \;\land
                \iterpred_\alpha(i', ys, S) * \mathit{colls}(S)$\}
\nextline \> \{$a = \None \land xs = \epsilon \land
                \iterpred_\alpha(\Filter(p, i'), xs, S) * \mathit{colls}(S)$\}
\nextline \> $\Some(z)$\=$\to$
\nextline \> \;\;\{\=$\exists zs.\; ys = z\cdot zs \land v = \Some(z) \land
                  xs = \mathit{filter}\;p\;(z\cdot zs) \;\land$ 
\\ \> \> \>      $\iterpred_\alpha(i', zs, S) * \mathit{colls}(S)$\}
\nextline \> \;\;\{\=$ys = z\cdot zs \land v = \Some(z) \land
                   xs = \mathit{filter}\;p\;(z\cdot zs) \;\land$ 
\\ \> \> \>      $\iterpred_\alpha(i', zs, S) * \mathit{colls}(S)$\}
\nextline \> \;\;$\ctext{if}($\=$p\;z,$ 
\nextline \> \> \{\=$p\;z = \True \land ys = z\cdot zs \land v =\Some(z) \land
                   xs = \mathit{filter}\;p\;(z\cdot zs) \;\land$ 
\\ \> \> \>      $\iterpred_\alpha(i', zs, S) * \mathit{colls}(S)$\}
\nextline \> \> \{\=$ys = z\cdot zs \land v =\Some(z) \land
                   xs = z\cdot(\mathit{filter}\;p\;zs) \;\land$ 
\\ \> \> \>      $\iterpred_\alpha(i', zs, S) * \mathit{colls}(S)$\}
\nextline \> \> \{\=$v =\Some(z) \land
                   xs = z\cdot(\mathit{filter}\;p\;zs) \;\land
                   \iterpred_\alpha(i', zs, S) * \mathit{colls}(S)$\}
\nextline \> \> \{\=$v =\Some(z) \land
                   xs = z\cdot(\mathit{filter}\;p\;zs) \;\land
                   \mathit{filter}\;p\;zs = \mathit{filter}\;p\;zs \;\land
                   \iterpred_\alpha(i', zs, S) * \mathit{colls}(S)$\}
\nextline \> \> \{\=$v =\Some(z) \land
                   xs = z\cdot(\mathit{filter}\;p\;zs) \;\land
                   \iterpred_\alpha(i', \mathit{filter}\;p\;zs, S) * \mathit{colls}(S)$\}
\nextline \> \> \{\=$\exists z', zs'.\; v =\Some(z') \land
                     xs = z'\cdot zs' \land
                   \iterpred_\alpha(i', zs', S) * \mathit{colls}(S)$\}
\nextline \> \> $[v],$
\nextline \> \> \{\=$\exists z', zs'.\; a = \Some(z') \land
                     xs = z'\cdot zs' \land
                   \iterpred_\alpha(i', zs', S) * \mathit{colls}(S)$\}
\\ \> \> \> (Now the else-branch)
\nextline \> \> \{\=$p\;z = \False \land ys = z\cdot zs \land a = \Some(z) \land
                   xs = \mathit{filter}\;p\;(z\cdot zs) \;\land$ 
\\ \> \> \>      $\iterpred_\alpha(i', zs, S) * \mathit{colls}(S)$\}
\nextline \> \> \{\=$xs = \mathit{filter}\;p\; zs \;\land
                     \iterpred_\alpha(i', zs, S) * \mathit{colls}(S)$\}
\nextline \> \> \{\=$\iterpred_\alpha(\Filter(p, i'), xs, S) * \mathit{colls}(S)$\}
\nextline \> \> $\nextiter_\alpha(\Filter(p, i'))))$ 
\nextline \> \> \{\=$(xs = \epsilon \land a = \None \land \iterpred_\alpha(\Filter(p, i'), xs, S) * \mathit{colls}(S)) \;\vee$ 
\contline \> \>  $(\exists z, zs. xs = z\cdot zs \land a = \Some(Z) \land
                    \iterpred_\alpha(\Filter(p, i'), zs, S) * \mathit{colls}(S))$\}   
\end{specification}

Note that this is not a structural induction on $i$; the
$\Filter(p,i)$ case has a non-structural recursive call. Here, we make
use of fixed-point induction in our proof of $\nextiter$. 

\item Suppose $i = \Merge(f, i_1, i_2)$. Then, we proceed with an
annotated proof as follows:

\begin{specification}
\nextline $\setof{\iterpred_\alpha(\Merge(f, i_1, i_2), xs, S) * \mathit{colls}(S)}$ 
\nextline $\setof{\exists S_1, ys, S_2, zs.\; 
                    xs = \mathit{map2}\;f\;ys\;zs \land 
                    S = S_1 \uplus S_2 \land
                    \iterpred_\alpha(i_1, ys, S_1) * 
                    \iterpred_\alpha(i_2, zs, S_2) * 
                    \mathit{colls}(S)}$ 
\nextline \{$xs = \mathit{map2}\;f\;ys\;zs \land 
                  S = S_1 \uplus S_2 \land$ 
\\ \>\;     $\iterpred_\alpha(i_1, ys, S_1) * 
                  \iterpred_\alpha(i_2, zs, S_2) * 
                  \mathit{colls}(C,S)$\}
\nextline \{$xs = \mathit{map2}\;f\;ys\;zs \land 
                  S = S_1 \uplus S_2 \land$ 
\\ \>\;     $\iterpred_\alpha(i_1, ys, S_1) * 
                  \iterpred_\alpha(i_2, zs, S_2) * 
                  \mathit{colls}(S_1) * \mathit{colls}(S_2)$\}
\nextline $\letv{v_1}{\nextiter_\alpha(i_1)}{}$
\nextline \{$xs = \mathit{map2}\;f\;ys\;zs \land 
                  S = S_1 \uplus S_2 \land$ 
\\ \>\;     $[(ys = \epsilon \land v_1 = \None \land \iterpred_\alpha(i_1, ys, S_1))$ 
\\ \>\;\;    $\vee\; (\exists b, bs.\; ys = b\cdot bs \land v_1 = \Some(b) \land 
                                \iterpred_\alpha(i, bs, S_1))] \;*$ 
\\ \>\;     $\iterpred_\alpha(i_2, zs, S_2) * 
             \mathit{colls}(S_1) * \mathit{colls}(S_2)$\}
\nextline \{$xs = \mathit{map2}\;f\;ys\;zs \land 
                  S = S_1 \uplus S_2 \land$ 
\\ \>\;     $[(ys = \epsilon \land v_1 = \None \land \iterpred_\alpha(i_1, ys, S_1))$ 
\\ \>\;\;    $\vee\; (\exists b, bs.\; ys = b\cdot bs \land v_1 = \Some(b) \land 
                                \iterpred_\alpha(i_1, bs, S_1))] \;*$ 
\\ \>\;     $\iterpred_\alpha(i_2, zs, S_2) * 
             \mathit{colls}(S_1) * \mathit{colls}(S_2)$\}
\nextline $\letv{v_2}{\nextiter_\alpha(i_2)}{}$ 
\nextline \{$xs = \mathit{map2}\;f\;ys\;zs \land 
                  S = S_1 \uplus S_2 \land$ 
\\ \>\;     $[(ys = \epsilon \land v_1 = \None \land \iterpred_\alpha(i_1, ys, S_1))$ 
\\ \>\;\;    $\vee\; (\exists b, bs.\; ys = b\cdot bs \land v_1 = \Some(b) \land 
                                \iterpred_\alpha(i_1, bs, S_1))] \;*$ 
\\ \>\;     $[(zs = \epsilon \land v_2 = \None \land \iterpred_\alpha(i_2, zs, S_2))$ 
\\ \>\;\;    $\vee\; (\exists c, cs.\; ys = c\cdot cs \land v_2 = \Some(c) \land 
                                \iterpred_\alpha(i_2, cs, S_2))] \;*$ 
\\ \>\;     $\mathit{colls}(S_2) * \mathit{colls}(S_1)$\}
\nextline \{$xs = \mathit{map2}\;f\;ys\;zs \land 
                  S = S_1 \uplus S_2 \land$ 
\\ \>\;     $[(ys = \epsilon \land v_1 = \None \land \iterpred_\alpha(i_1, ys, S_1))$ 
\\ \>\;\;    $\vee\; (\exists b, bs.\; ys = b\cdot bs \land v_1 = \Some(b) \land 
                                \iterpred_\alpha(i_1, bs, S_1))] \;*$ 
\\ \>\;     $[(zs = \epsilon \land v_2 = \None \land \iterpred_\alpha(i_2, zs, S_2))$ 
\\ \>\;\;    $\vee\; (\exists c, cs.\; ys = c\cdot cs \land v_2 = \Some(c) \land 
                                \iterpred_\alpha(i_2, cs, S_2))] \;*$ 
\\ \>\;     $\mathit{colls}(S)$\}
\nextline $\Optcase(v_1$ 
\nextline \;\;\; $\None$\=$ \to $ 
\nextline \> \{$xs = \mathit{map2}\;f\;ys\;zs \land 
                  S = S_1 \uplus S_2 \land$ 
\\ \> \>\;     $(ys = \epsilon \land v_1 = \None \land \iterpred_\alpha(i_1, ys, S_1)) * $ 
\\ \> \>\;     $[(zs = \epsilon \land v_2 = \None \land \iterpred_\alpha(i_2, zs, S_2))$ 
\\ \> \>\;\;    $\vee\; (\exists c, cs.\; ys = c\cdot cs \land v_2 = \Some(c) \land 
                                \iterpred_\alpha(i_2, cs, S_2))] \;*$ 
\\ \> \>\;     $\mathit{colls}(S)$\}
\nextline \> \{$xs = \epsilon \land xs = \mathit{map2}\;f\;ys\;zs \land 
                  S = S_1 \uplus S_2 \land$ 
\\ \> \>\;     $(ys = \epsilon \land v_1 = \None \land \iterpred_\alpha(i_1, ys, S_1)) * $ 
\\ \> \>\;     $[(zs = \epsilon \land v_2 = \None \land \iterpred_\alpha(i_2, zs, S_2))$ 
\\ \> \>\;\;    $\vee\; (\exists c, cs.\; ys = c\cdot cs \land v_2 = \Some(c) \land 
                                \iterpred_\alpha(i_2, cs, S_2))] \;*$ 
\\ \> \>\;     $\mathit{colls}(S)$\}
\\ \> \>     The next line follows because it holds in either branch of the disjunction 
\nextline \> \{$xs = \epsilon \land \iterpred_\alpha(\Merge(f, i_1, i_2), xs, S)
               * \; \mathit{colls}(S)$\}
\nextline \> $\None$ 
\nextline \> \{$a = \None \land xs = \epsilon \land \iterpred_\alpha(\Merge(f, i_1, i_2), xs, S)
               * \mathit{colls}(S) $\}
\nextline \;\;\;$\Some(b)$\=$ \to $ 
\nextline \> \{$xs = \mathit{map2}\;f\;ys\;zs \land 
                  S = S_1 \uplus S_2 \land$ 
\\ \> \>\;\;    $(\exists bs.\; ys = b\cdot bs \land v_1 = \Some(b) \land 
                                \iterpred_\alpha(i_1, bs, S_1)) \;*$ 
\\ \> \>\;     $[(zs = \epsilon \land v_2 = \None \land \iterpred_\alpha(i_2, zs, S_2))$ 
\\ \> \>\;\;    $\vee\; (\exists c, cs.\; ys = c\cdot cs \land v_2 = \Some(c) \land 
                                \iterpred_\alpha(i_2, cs, S_2))] \;*$ 
\\ \> \>\;     $\mathit{colls}(S)$\}
\nextline \> $\Optcase(v_2,$ 
\nextline \> \;\;\; $\None$\=$\to$ 
\nextline \> \> \{$xs = \mathit{map2}\;f\;ys\;zs \land 
                  S = S_1 \uplus S_2 \land$ 
\\ \> \> \>\;\;    $(\exists bs.\; ys = b\cdot bs \land v_1 = \Some(b) \land 
                                \iterpred_\alpha(i_1, bs, S_1)) \;*$
\\ \> \> \>\;     $(zs = \epsilon \land v_2 = \None \land \iterpred_\alpha(i_2, zs, S_2))$ 
\\ \> \> \>\;     $\mathit{colls}(S)$\}

\nextline \> \> \{$xs = \epsilon \land xs = \mathit{map2}\;f\;ys\;zs \land 
                  S = S_1 \uplus S_2 \land$ 
\\ \> \> \>\;\;    $(\exists bs.\; ys = b\cdot bs \land v_1 = \Some(b) \land 
                                \iterpred_\alpha(i_1, bs, S_1)) \;*$
\\ \> \> \>\;     $(zs = \epsilon \land v_2 = \None \land \iterpred_\alpha(i_2, zs, S_2))$ 
\\ \> \> \>\;     $\mathit{colls}(S)$\}

\nextline \> \> \{$xs = \epsilon \land \iterpred_\alpha(\Merge(f, i_1, i_2), xs, S) * 
                   \mathit{colls}(S)$\}
\nextline \> \> $\None$,
\nextline \> \> \{$a = \None \land xs = \epsilon \land \iterpred_\alpha(\Merge(f, i_1, i_2), xs, S) * 
                   \mathit{colls}(S)$\}

\nextline \> \;\;\; $\Some(c)$\=$ \to $ 
\nextline \> \> \{$xs = \mathit{map2}\;f\;ys\;zs \land 
                  S = S_1 \uplus S_2 \land$ 
\\ \> \> \>\;\;    $(\exists bs.\; ys = b\cdot bs \land v_1 = \Some(b) \land 
                                \iterpred_\alpha(i_1, bs, S_1)) \;*$
\\ \> \> \>\;\;    $(\exists cs.\; ys = c\cdot cs \land v_2 = \Some(c) \land 
                                \iterpred_\alpha(i_2, cs, S_2)) \;*$

\\ \> \> \>\;     $\mathit{colls}(S)$\}
\nextline \> \> \{$xs = \mathit{map2}\;f\;ys\;zs \land 
                  S = S_1 \uplus S_2 \land$ 
\\ \> \> \>\;    $(ys = b\cdot bs \land v_1 = \Some(b) \land 
                                \iterpred_\alpha(i_1, bs, S_1)) \;*$
\\ \> \> \>\;    $(zs = c\cdot cs \land v_2 = \Some(c) \land 
                                \iterpred_\alpha(i_2, cs, S_2)) \;*$

\\ \> \> \>\;     $\mathit{colls}(S)$\}

\nextline \> \> \{$xs = (f\;b\;c) \cdot \mathit{map2}\;f\;bs\;cs \land 
                  S = S_1 \uplus S_2 \land$ 
\\ \> \> \>\;    $\iterpred_\alpha(i_1, bs, S_1)) * \iterpred_\alpha(i_2, cs, S_2)) \;*$

\\ \> \> \>\;     $\mathit{colls}(S)$\}

\nextline \> \> \{$xs = (f\;b\;c) \cdot \mathit{map2}\;f\;bs\;cs \; \land$
\\ \> \> \>\;    $\iterpred_\alpha(\Merge(f, i_1, i_2), \mathit{map2}\;f\;bs\;cs, S) \;*$
\\ \> \> \>\;     $\mathit{colls}(S)$\}

\nextline \> \> $\Some(f\;b\;c)))$ 
\nextline \> \> \{$a = (f\;b\;c) \land xs = (f\;b\;c) \cdot \mathit{map2}\;f\;bs\;cs \; \land$
\\ \> \> \>\;    $\iterpred_\alpha(\Merge(f, i_1, i_2), \mathit{map2}\;f\;bs\;cs, S) \;*$
\\ \> \> \>\;     $\mathit{colls}(S)$\}

\nextline \> \> \{$\exists v, vs.\; a = v \land xs = v \cdot vs \; \land$
\\ \> \> \>\;    $\iterpred_\alpha(\Merge(f, i_1, i_2), vs, S) \;*$
\\ \> \> \>\;     $\mathit{colls}(S)$\}


\end{specification}

\end{itemize}

\end{proof}


$\nextiter$ (lines 20-31) recursively walks down the structure of
the iterator tree, and combines the results from the leaves upwards.
The base case is the $\ctext{Coll }r$ case (lines 20-23). The iterator
pointer is doubly-dereferenced, and then the contents examined. If the
end of the list has been reached and the contents are $\Nil$,
then $\None$ is returned to indicate there are no more
elements. Otherwise, the pointer $r$ is advanced, and the head
returned as the observed value. The $\ctext{Filter(p,i)}$ case (lines
24-26) will return $\None$ if $i$ is exhausted, and if it is
not, it will pull elements from $i$ until it finds one that satisfies
$p$, calling itself recursively until it succeeds or $i$ is exhausted.
Finally, in the $\ctext{Map2}(f, i_1, i_2)$ case (lines 27-31),
$\nextiter$ will draw a value from both $i_1$ and $i_2$, and will
return $\None$ if either is exhausted, and otherwise it will
return $f$ applied to the pair of values.


\section{The Flyweight and Factory Patterns}

The flyweight pattern is a style of cached object creation. Whenever a
constructor method is called, it first consults a table to see if an
object corresponding to those arguments has been created. If it has,
then the preexisting object is returned.  Otherwise, it allocates a
new object, and updates the table to ensure that future calls with the
same arguments will return this object. Because objects are re-used,
they become pervasively aliased, and must be used in an immutable
style to avoid surprising updates. (Functional programmers call this
style of value creation ``hash-consing''.)

This is an interesting design pattern to verify, for two reasons.
First, the constructor has a memo table to cache the result of
constructor calls, which needs to be hidden from clients. Second, this
pattern makes pervasive use of aliasing, in a programmer-visible
way. In particular, programmers can test two references for identity
in order to establish whether two values are equal or not. This allows
constant-time equality testing, and is a common reason for using this
pattern. Therefore, our specification has to be able to justify this
reasoning.

Below, we specify a program that uses the flyweight pattern to create
and access glyphs (i.e., refs of pairs of characters and fonts) of a
particular font $f$. We have a function $\ctext{newglyph}$ to create
new glyphs, which does the caching described above, using a predicate
variable $I$ to refer to the table invariant; and a function
$\ctext{getdata}$ to get the character and font information from a
glyph.

Furthermore, these functions will be created by a call to another
function, $\ctext{make\_flyweight}$, which receives a font as an
argument and will return appropriate $\ctext{newglyph}$ and
$\ctext{getdata}$ functions.

\begin{tabbing}
Flyweight$($\=$I : (\ctext{char} \rightharpoonup \ctext{glyph}) \to \assert,\;\; $\\
\> $\ctext{newglyph} : \chartp \to \monad{\ctext{glyph}},$ \\
\> $\ctext{getdata} : \ctext{glyph} \to \monad{(\chartp \times \fonttp)},$ \\
\> $f:\fonttp) \equiv$ \\
% 1 \qquad \=$\exists $\=$glyph : \ctext{glyph} \times \chartp \times \fonttp \To \assert.$ 
% \\[0.5em]

1 \qquad\=  $\forall$\=$c, h.\;$\=
         $\angles{I(h)}$ \\
  \>\>\>${\ctext{newglyph}(c)}$ \\
  \>\>\>$\angles{a:\ctext{glyph}.\; I([h|c:a]) \land (c \in \domain{h} \implies a = h(c))}$ \\
  \> \!$\specand$ \\
2 \> $\forall l, h, c.\;$\=
     $\angles{I(h) \land l = h(c)}$ \\
\>\> ${\ctext{getdata}(l)}$ \\
\>\> $\angles{a:\chartp \times \fonttp.\;  I(h) \land a = (c,f)}$
%   \> \!$\specand$ \\
% 4 \> $\{\forall l, l', c, c'.\;$\=$I \land glyph(l,c,f) \land glyph(l',c',f')
%  \implies $ \\
% \>\>  $\;\;\;\left(l = l' \iff (c = c'\land f=f')\right)\}$ \\[0.5em]
% 
% $chars(\emptyset)$ \qquad\qquad\qquad \;\;\= $\equiv$ \= $\top$ \\
% $chars(\setof{(l,(c,f))} \cup S)$ \> $\equiv$ \> $glyph(l,c,f) \land chars(S)$ \\
\end{tabbing}


In the opening , we informally parametrize our specification over the
predicate variable $I$, the function variable $\ctext{newglyph}$, the
function variable $\ctext{getdata}$, and the variable $f$ of $\fonttp$
type. The reason we do this instead of existentially quantifying over
them will become clear shortly, once we see the factory function that
creates flyweight constructors.

On line 1, we specify the effect of a call $\ctext{newglyph}(c)$
procedure. Its precondition is the predicate $I(h)$, which asserts
that the abstract state contains glyphs for all of the characters in
the domain of the partial function $h$. Its postcondition changes to a
state $I([h|c:a])$, which is a partial function extended to include
the character $c$, with the additional condition that if $c$ was
already in $h$'s domain, the same glyph value is returned. 

The intuition for this specification is that $I(h)$ abstractly represents
a memo table (i.e., the function $h$), which characterizes all the 
glyphs allocated so far. 

On line 2, we specify the $\ctext{getdata}$ function. This just says 
that if we call $\ctext{getdata}(c)$ on a character $c$ in the domain
of $h$, then we are returned the data (the alphabetic character 
and font) for this glyph. 

The specification of the flyweight factory looks like this:


\begin{tabbing}
1 \qquad \= $\exists \ctext{make\_flyweight} :
\fonttp \to \bigcirc($\=$(\chartp \to \monad{\ctext{glyph}}) \times$ \\
\> \> $(\ctext{glyph} \to \monad{(\chartp \times \fonttp)})).$\\
2 \> \;\;\= $\forall f.\;$\=$\setof{\emp}$ \\
  \>\> \> $\run{\ctext{make\_flyweight}(f)}$ \\
  \>\> \> $\setof{a.\; \exists I.\; I([]) \land \validprop{\mbox{Flyweight}(I, \fst{a}, \snd{a}, f)}}$ 
\end{tabbing}


Here, we assert the existence of a function $\ctext{make\_flyweight}$,
which takes a font $f$ as an input argument, and returns two functions
to serve as the $\ctext{getchar}$ and $\ctext{getdata}$ functions of
the flyweight. In the postcondition, we assert the existence of some
private state $I$, which contains the table used to cache glyph
creations. 

This pattern commonly arises when encoding aggressively
object-oriented designs in a higher-order style --- we call a
function, which creates a hidden state, and returns other procedures
which are the only way to access that state. This style of
specification resembles the existential encodings of objects into type
theory. The difference is that instead of making the fields of an
object an existentially quantified \emph{value}~\cite{pierce-turner}, we
make use of existentially-quantified \emph{state}.

Below, we define $\ctext{make\_flyweight}$ and its predicates:

\begin{specification}
\nextline $\ctext{m}$\=$\ctext{ake\_flyweight} \equiv$ 
\nextline            \> $\lambda f:$\=$\fonttp.\;$
\nextline \> $[$\=$\letv{t}{\ctext{newtable}()}{}$ 
\nextline \> \> $\ctext{letv\;}$\=$\mathit{newglyph} =$ 
\nextline \> \> \> \!\!$[\lambda c.[$\=$\ctext{letv}\; x = \ctext{lookup}(t, c)\;\ctext{in}$
\nextline \> \>\>\> $\ctext{run}\;\ctext{case}(x,$\=
$\None \to [$\=$\ctext{letv}\; r = [\newref{\ctext{glyph}}{c,f}] \;\ctext{in}$
\nextline \> \>\>\>\>\> $\ctext{letv } \_ = \ctext{update}(t, c, r) \;\ctext{ in }\;r],$ 
\nextline \> \>\>\>\> $\Some(r) \to [r])] \;\ctext{in}$ 
\nextline \> \> $\ctext{letv\;}\mathit{getdata} = [\lambda r.\; [!r]] \;\ctext{in}$ 
\nextline \> \> $(\mathit{newglyph}, \mathit{getdata})]$
\\

\nextline $I(h) \equiv table(t,h) * \mathit{refs}(mapping, \mbox{dom}(mapping))$ \\
\nextline $\mathit{refs}(h) = \forall^{*} c \in \domain{h}.\; h(c) \pointsto (c, f)$ 
\end{specification}


In this implementation we have assumed the existence of a hash table
implementation with operations $\ctext{newtable}$, $\ctext{lookup}$,
and $\ctext{update}$, whose specifications we give at the end of the
chapter. The $\ctext{make\_flyweight}$ function definition takes a
font argument $f$, and then in its body it creates a new table $t$. It
then constructs two functions as closures which capture this state
(and the argument $f$) and operate on it. In lines 4-7, we define
$newglyph$, which takes a character and checks to see (line 5) if it
is already in the table. If it is not (lines 5-6), it allocates a new
glyph reference, stores it in the table, and returns the
reference. Otherwise (line 7), it returns the existing reference from
the table.  On lines 8, we define $getdata$, which dereferences its
pointer argument and returns the result. This implementation does no
writes, fulfilling the promise made in the specification. The
definition of the invariant state $I$ describes the state of the table
$t$ (as the partial function $h$), which are hidden from clients.

Observe how the post-condition to $\ctext{make\_flyweight}$ nests the
existential state $I$ with the validity assertion to specialize the
flyweight spec to the \emph{dynamically} created table. Each created
flyweight factory receives its own private state, and we can reuse
specifications and proofs with no possibility that the wrong
$\ctext{getdata}$ will be called on the wrong reference, even though
they have compatible types.

\subsection{Verification of the Implementation}

To prove the correctness of the implementation, we will work
inside-out, first giving specifications and correctness proofs for the
``methods'' of the factory, and then using these to prove the correctness
of the $\mathsf{make\_flyweight}$ function. 

\begin{lemma}{(Correctness of \textit{newglyph})}
Define the function $\mathit{newglyph}$ as follows:
\begin{tabbing}
  $\mathit{newglyph} \equiv \lambda c.\; [$\=
     $\letv{x}{\ctext{lookup}(t,c)}{}$ \\
\>   $\ctext{run}\;\ctext{case}(x, $\=$\None \to [$\=$\letv{r}{[\newref{\ctext{glyph}}{(c,f)}]}{}$ \\
\>                                  \>            \>$\letv{\_}{\ctext{update}(t, c, r)}{}$ \\
\>                                  \>            \>$r]$\\
\>                                  \>$\Some(r) \to [r])]$ 
\end{tabbing}
\noindent This function satisfies the following specification:
\begin{tabbing}
\;\;\=
$\setof{\mathit{table}(t, \sigma) * \forall^{*}c \in \domain{\sigma}.\;\sigma(c) \pointsto (c,f)}$ \\
\> $\mathit{newglyph}(c,f)$ \\
\> $\{a:\reftype{\ctext{glyph}}.\;
   \exists \sigma' \supseteq \sigma. \mathit{table}(t, \sigma') \land \sigma'(a) = (c,f) * \forall^{*}c \in \domain{\sigma}.\;\sigma(c) \pointsto (c,f) \}$ \\
\end{tabbing}
\end{lemma}

\begin{proof}
Note that in this definition, the $t$ and $f$ variables occur free. We are going to 
prove our specification valid with respect to these variables, so that we can substitute
them with whatever actual terms in context that we need to use. 

\begin{tabbedproof}
\oo Assume we are in the precondition state $\mathit{table}(t, \sigma) * \forall^{*}c \in \domain{\sigma}.\;\sigma(c) \pointsto (c,f)$ \\
\ooo $[\letv{x}{\ctext{lookup}(t,c)}{}$ \\
\ooo We now have $\mathit{table}(t, \sigma) * \forall^{*}c \in \domain{\sigma}.\;\sigma(c) \pointsto (c,f)$  \\
\oox $\land\; ((c \not \in \domain{\sigma} \land x = \None) \vee (c \in \domain{\sigma} \land x = \Some(\sigma(c))))$ \\
\ooo Suppose $x = \None$:\\
\oooo Then it follows that $\mathit{table}(t, \sigma) * \forall^{*}c \in \domain{\sigma}.\;\sigma(c) \pointsto (c,f) \land c \not \in \domain{\sigma} \land x = \None$\\
\oooo By equality reasoning, we can simplify the case statement. \\
\oooo $\letv{r}{[\newref{\ctext{glyph}}{(c,f)}]}{}$ \\
\oooo So $r \pointsto (c,f) * \forall^{*}c \in \domain{\sigma}.\;\sigma(c) \pointsto (c,f) * \mathit{table}(t, \sigma) \land c \not \in \domain{\sigma} \land x = \None$\\
\oooo $\letv{\_}{\ctext{update}(t, c, r)}{}$ \\
\oooo So $r \pointsto (c,f) * \forall^{*}c \in \domain{\sigma}.\;\sigma(c) \pointsto (c,f) * \mathit{table}(t, [\sigma|c:r]) \land c \not \in \domain{\sigma}$\\
\oooo So $\forall^{*}c \in \domain{\sigma'}.\;\sigma(c) \pointsto (c,f) * \mathit{table}(t, \sigma') \land \sigma' = [\sigma|c:r]$\\
\oooo $r]$ \\
\oooo We can choose the witness to the existential to be $\sigma'$, yielding \\
\ooox $\exists \sigma' \supseteq \sigma. \mathit{table}(t, \sigma') \land \sigma'(a) = (c,f) * \forall^{*}c \in \domain{\sigma}.\;\sigma(c) \pointsto (c,f)$ \\
\ooo Suppose $\exists y.\; x = \Some(y)$:  \\
\oooo Then it follows that $\mathit{table}(t, \sigma) * \forall^{*}c \in \domain{\sigma}.\;\sigma(c) \pointsto (c,f) \land x = \Some(\sigma(c))$\\
\oooo By equality reasoning, we can simplify the case statement. \\
\oooo Furthermore, we have $\mathit{table}(t, \sigma) * \forall^{*}c \in \domain{\sigma}.\;\sigma(c) \pointsto (c,f) \land r = \sigma(c)$\\
\oooo $r]$ \\
\oooo We can choose the witness to the existential to be $\sigma$, yielding \\
\ooox $\exists \sigma' \supseteq \sigma. \mathit{table}(t, \sigma') \land \sigma'(a) = (c,f) * \forall^{*}c \in \domain{\sigma}.\;\sigma(c) \pointsto (c,f)$ 
\end{tabbedproof}
\end{proof}

\begin{lemma}{(Correctness of $\mathit{getdata}$)}
Define the $\mathit{getdata}$ function as follows:
\begin{tabbing}
\qquad $\mathit{getdata} \equiv \semfun{r}{[!r]}$  
\end{tabbing}

\noindent This function satisfies the following specification:
\begin{displaymath}
  \spec{\mathit{table}(t, \sigma) * \mathit{refs}(\sigma) \land l \in \domain{\sigma}}
       {\mathit{getdata}(l)}
       {a:\chartp \times \fonttp}
       {\mathit{table}(t, \sigma) * \mathit{refs}(\sigma)}
\end{displaymath}
\end{lemma}

\begin{proof}
The correctness proof for this function is very simple, amounting to a single application
of the frame rule, together with the dereference rule.
\begin{tabbedproof}
\oo Assume our state is $\mathit{table}(t, \sigma) * \mathit{refs}(\sigma) \land l = h(c)$ \\
\oo This is $\mathit{table}(t, \sigma) * (\mathit{refs}(\sigma - [c:l]) * l \pointsto (c,f)$ \\
\oo $[!l]$ \\
\oo This is $\mathit{table}(t, \sigma) * (\mathit{refs}(\sigma - [c:l]) * l \pointsto (c,f) \land a = (c,f)$ \\
\oo This is $\mathit{table}(t, \sigma) * \mathit{refs}(\sigma) \land a = (c, f)$ 
\end{tabbedproof}
\end{proof}

\begin{lemma}{(Correctness of $\mathsf{make\_flyweight}$)}
  The $\mathsf{make\_flyweight}$ meets the specification above. 
\end{lemma}

Now, we can prove the correctness of the $\mathsf{make\_flyweight}$ function. To do
this, we will assume a font argument $f$, and then prove the correctness of the body
of the function.

\begin{proof}
\begin{tabbedproof}
\oo We begin with an assumed $\emp$ precondition. \\
\oo $[\letv{t}{\mathsf{newtable}()}{}$ \\
\oo Now our state is $\mathit{table}(t, [])$ \\
\oo $\letv{\mathit{newglyph}}{\ldots}{}$ (Take the ``\ldots'' to be the definition above) \\
\oo $\letv{\mathit{getdata}}{\ldots}{}$ (Take the ``\ldots'' to be the definition above)\\
\oo Now our state is $\mathit{table}(t, []) \land \mathit{newglyph} = \ldots \land \mathit{getdata} = \ldots$ \\
\oo Choose $I(h) = table(t,h) * \mathit{refs}(mapping, \mbox{dom}(mapping))$ with the $t$ in this scope\\
\oo Then we can instantiate the specs of $\mathit{newglyph}$ and $\mathit{getdata}$ with $I$ and $f$ in this scope\\
\oo Then since valid specs allow us to introduce validity assertions, we know  \\
\ox $I([]) \land \validprop{\mbox{Flyweight}(I, \mathit{newglyph}, \mathit{getdata}, f)}$  \\
\oo We can existentially quantify over $I$, to get \\
\ox $\exists I.\; I([]) \land \validprop{\mbox{Flyweight}(I, \mathit{newglyph}, \mathit{getdata}, f)}$  \\
\oo $(\mathit{newglyph}, \mathit{getdata})]$\\
\oo Now we know $\exists I.\;I([]) \land \validprop{\mbox{Flyweight}(I, \fst{a}, \snd{a}, f)}$\\
\end{tabbedproof}
\end{proof}

\section{Subject-Observer}

The subject-observer pattern is one of the most characteristic
patterns of object-oriented programming, and is extensively used in
GUI toolkits. This pattern features a mutable data structure called
the \emph{subject}, and a collection of data structures called
\emph{observers} whose invariants depend on the state of the
subject. Each observer registers a callback function with the subject
to ensure it remains in sync with the subject. Then, whenever the
subject changes state, it iterates over its list of callback
functions, notifying each observer of its changed state. While
conceptually simple, this is a lovely problem for verification, since
every observer can have a different invariant from all of the others,
and the implementation relies on maintaining lists of callback
functions in the heap.  

In our example, we will model this pattern with one type of subjects,
and three functions. A subject is simply a pair, consisting of a
pointer to a number, the subject state; and a list of observer
actions, which are imperative procedures to be called with the new
value of the subject whenever it changes. There is a function
$\ctext{newsub}$ to create new subjects; a function
$\ctext{register}$, which attaches observer actions to the subject;
and finally a function $\ctext{broadcast}$, which updates a subject
and notifies all of its observers of the change. 


We give a specification for the subject-observer pattern below:
%

\begin{tabbing}
1 \qquad \= $\exists sub : A_s \times \N \times \seqsort{((\N \To \assert) \times (\N \to \monad{1}))}.$ \\
2 \> $\exists \ctext{newsub} : \N \to \monad{A_s},$ \\ 
3 \> $\exists \ctext{register} : A_s \times (\N \to \monad{1}) \to \monad{1},$ \\
4 \> $\exists \ctext{broadcast} : A_s \times \N \to \monad{1}.$ \\
\\[0.5em]
5 \>$\forall n.\; \mspec{\emp}{{\ctext{newsub}(n)}}{a:A_s}{sub(a, n, \epsilon)}$ \\
\> $\specand$ \\
6 \> $\forall f, O, s, n, os. $\=$(\forall i, k. \mspec{O(i)}{{f(k)}}{a:1}{O(k)})$ \\
7\> \>$\specimp$\=$\angles{sub(s, n, os)}$ \\
8\> \>          \>${\ctext{register}(s, f)}$ \\
9 \> \>          \>$\angles{a:1.\; sub(s, n, (O,f)\cdot os)}$ \\
\> $\specand$ \\
10 \> $\forall s,i,os,k.\; $\=$\angles{sub(s, i, os) * obs(os)}$ \\
  \>                       \>${\ctext{broadcast}}(s,k)$ \\
  \>                       \>$\angles{a:1.\; sub(s, k, os) * obs\_at(os, k)}$ 
\\[0.5em]
$obs(\epsilon) \;\qquad\qquad $\=$\equiv \emp$ \\
$obs((O,f)\cdot os) $\>$\equiv (\exists i.\; O(i)) * obs(os)$ 
\\[0.5em]
$obs\_at(\epsilon, k) \;\qquad\qquad $\=$\equiv \emp$ \\
$obs\_at((O,f)\cdot os, k) $\>$\equiv O(k) * obs\_at(os, k)$ 
\\
\end{tabbing}

%
On line 1 we assert the existence of a three-place predicate $sub(s,
n, os)$. The first argument is the subject $s$'s whose state this
predicate represents. The second argument $n$ is the data the
observers depend on, and the field $os$ is a sequence of callbacks
paired with their invariants. That is, $os$ is a sequence of pairs,
consisting of the observer functions which act on a state, along with
the predicate describing what that state should be.

On lines 2-4, we assert the existence of $\ctext{newsub}$,
$\ctext{register}$ and $\ctext{broadcast}$, which create a new
subject, register a callback, and broadcast a change, respectively.

$\ctext{register}$ is a higher order function, which takes a subject
and an observer action its two arguments. The observer action is a
function of type $\N \to \monad{1}$, which can be read as saying it
takes the new value of the subject and performs a side-effect. Because
$\ctext{register}$ depends on code, its specification must say how
this observer action should behave. $\ctext{register}$'s specification
on lines 6-9 accomplishes this via an implication over Hoare
triples. It says that \emph{if} the function $f$ is a good observer
callback, \emph{then} it can be safely registered with the
subject. Here, a ``good callback'' $f$ is one that takes an argument
$k$ and sends an observer state to $O(k)$. If this condition is
satisfied, then $\ctext{register}(s, f)$ will add the pair $(O,f)$ to
the sequence of observers in the $sub$ predicate.

$\ctext{broadcast}$ updates a subject and all its interested
observers.  The precondition state of $\ctext{broadcast}(s,k)$
requires the subject state $sub(s,n,os)$, and all of the observer
states $obs(os)$. The definition $obs(os)$ takes the list of observers
and yields the separated conjunction of the observer states. So when
$\ctext{broadcast}$ is invoked, it can modify the subject and any of
its observers. Then, after the call, the postcondition puts the $sub$
predicate and all of the observers in the same state $k$. The
$obs\_at(os,k)$ function generates the separated conjunction of all
the $O$ predicates, all in the same state $k$.

The implementation follows:

\begin{tabbing}
1 \qquad \= $A_s \equiv \reftype{\N} \times \reftype{\listtype{(\N \to \monad{1})}}$
\\[0.5em]
2 \> $sub(s, n, os) \equiv$\=$ \fst{s} \pointsto n * 
              list(\snd{s}, map\; \snd{} os) \land Good(os)$ 
\\[0.5em]
3 \> $Good(\epsilon) \!\qquad\qquad \equiv \top$ \\
4 \> $Good((O,f)\cdot os) \equiv\; $\=
   $\validprop{(\forall i,k.\; \spec{O(i)}{\run{f(k)}}{a:1}{O(k)})}$ \\
  \> \>      $\land Good(os)$ 
\\[0.5em]
5 \> $\ctext{register}(s, f) \equiv$ \=
         $[$\= $\letv{cell}{\comp{!(\snd{s})}}{}$ \\
6 \> \> \> $\letv{r}{\comp{\newref{\reftype{\listtype{(\N \to \monad{1})}}}{cell}}}{}$ \\
7 \> \> \> $\snd{s} := \Cons(f, r)]$
\\[0.5em]

8  \> $\ctext{broad}$\=$\ctext{cast}(s, k) \equiv$ \\
9  \>  \> $[$\=$\letv{dummy}{[\fst{s} := k]}{\ctext{loop}(k, \snd{s})}]$ \\


10 \> $\ctext{loop}$\=$(k, list) \equiv $\\
   \>         \>$[$\=$\letv{cell}{[!list]}{}$ \\
11 \>\>\> $\run{}\ctext{case}(cell,$\= 
            $\Nil \to [()],$ \\
12 \>\>\>\> $\Cons(f, tl) \to [$\=$\letv{dummy}{f(k)}{}$ \\
13  \>\>\>\> \> $\run{\ctext{loop}(k,tl)}])$ \\[0.5em]
14 \> $\ctext{new}\ctext{sub}(n) \equiv$ \=
          $[$\=$\letv{data}{\newref{\N}{n}}{}$ \\
15 \> \> \> $\letv{callbacks}{\newref{\listtype{(\N \to \monad{1})}}{\Nil}}{}$ \\
16 \> \> \> $(data, callbacks)]$
\end{tabbing}


In line 1, we state concrete type of the subject $A_s$ is a pair of a
pointer to a reference, and a pointer to a list of callback
functions. On line 2, we define the three-place subject predicate,
$sub(s,n,os)$. The first two subclauses of the predicate's body
describe the physical layout of the subject, and assert that the first
component of $s$ should point to $n$, and that the second component of
$s$ should be a linked list containing the function pointers in
$os$. (The $list$ predicate is described in Section 3, when we give
the definition of the iterator predicates.)

Then we require that $os$ be ``Good''. $Good$-ness is defined on lines
3 and 4, and says a sequence of predicates and functions is good when
every $(O,f)$ pair in the sequence satisfies the same validity
requirement the specification of $\ctext{register}$ demanded -- that
is, that each observer function $f$ update $O$ properly.  Note that we
interleave assertions and specifications to constrain the behavior of
code stored in the heap.

Next, we give the implementations of $\ctext{register}$ and
$\ctext{broadcast}$. $\ctext{register}$, on lines 5-7, adds its
argument to the list of callbacks. Though the code is trivial, its
correctness depends on the fact the $Good$ predicate holds for the
extended sequence --- we use the fact that the argument $f$ updates 
$O$ properly to establish that the extended list remains $Good$. 

$\ctext{broadcast}$, on lines 8-9, updates the subject's data field
(the first component), and then calls $\ctext{loop}$ (on lines 10-13)
to invoke all the callbacks. $\ctext{loop}(k, \snd{s})$ just recurs
over the list and calls each callback with argument $k$. The
correctness of this function also relies on the $Good$ predicate --
each time we call one of the functions in the observer list, we use
the hypothesis of its behavior given in $Good(os)$ to be able to make
progress in the proof.


Below, we give a simple piece of client code using this interface.


\begin{tabbing}
1 \qquad \= 
$\setof{\emp}$ \\
2 \> 
$\letv{s}{\ctext{newsub}(0)}{}$ \\
3 \> $\setof{sub(s, 0, \epsilon)}$ \\
4 \> $\letv{d}{\newref{\N}{(0)}}{}$ \\
5 \> $\letv{b}{\newref{\ctext{bool}}{(\ctext{true})}}{}$ \\
6 \> $\setof{sub(s, 0, \epsilon) * d \pointsto 0 * b \pointsto \ctext{true}}$\\
7 \> $\letv{()}{\ctext{register}(s, f)}{}$\\
8 \> $\setof{sub(s, 0, (double, f)\cdot\epsilon) * double(0) * b \pointsto \ctext{true}}$ \\
9 \> $\letv{()}{\ctext{register}(s, g)}{}$\\
10 \> $\setof{sub(s, 0, (even, g)\cdot(double, f)\cdot\epsilon) * double(0) * even(0)}$ \\
11 \> $\ctext{broadcast}(s, 5)$ \\
12 \> $\setof{sub(s, 5, (even, g)\cdot(double, f)\cdot\epsilon) * double(5) * even(5)}$ \\
13 \> $\setof{sub(s, 5, (even, g)\cdot(double, f)\cdot\epsilon) * d \pointsto 10 * b \pointsto \ctext{false}}$ 
\\[0.5em]
14 \> $f \qquad \qquad $\=$\equiv \lambda n:\N.\; [d := 2 \times n]$ \\
15 \> $double(n)$ \> $\equiv d \pointsto (2 \times n)$ \\
16 \> $g$ \> $\equiv \lambda x:\N.\; [b := even?(x)]$ \\
17 \> $even(n)$ \> $\equiv b \pointsto even?(n)$ \\
\end{tabbing}

% \vspace{-1em}
We start in the empty heap, and create a new subject $s$ on line 2.
On line 4, we create a new reference to $0$, and on line 5, we create
a reference to $\ctext{true}$. So on line 6, the state consists of a
subject state, and two references.  On line 7, we call
$\ctext{register}$ on the function $f$ (defined on line 14), which
sets $d$ to twice its argument. To the observer list in sub, we add
$f$ and the predicate $double$ (defined on line 15), which asserts
that indeed, $d$ points to two times the predicate argument. On line
8, we call $\ctext{register}$ once more, this time with the function
$g$ (defined on line 16) as its argument, which stores a boolean
indicating whether its argument was even into the pointer $b$. Again,
the state of $sub$ changes, and we equip $g$ with the $even$ predicate
(defined on line 17) indicating that $b$ points to a boolean
indicating whether the predicate argument was even or not. Since $d
\pointsto 0$ and $b \pointsto \ctext{true}$ are the same as
$double(0)$ and $even(0)$, so we can write them in this form on line
10.  We can now invoke $\ctext{broadcast}(s, 5)$ on line 11, and
correspondingly the states of all three components of the state shift
in line 12.  In line 13, we expand $double$ and $even$ to see $d$
points to 10 (twice 5), and $b$ points to $\ctext{false}$ (since 5
is odd).

\textbf{Discussion.} One nice feature of the proof of the
subject-observer implementation is that the proofs are totally
oblivious to the concrete implementations of the notification
callbacks, or to any details of the observer invariants. Just as
existential quantification hides the details of a module
implementation from the clients, the universal quantification in the
specification of $\ctext{register}$ and $\ctext{broadcast}$ hides all
details of the client callbacks from the proof of the implementation
-- since they are free variables, we are unable to make any
assumptions about the code or predicates beyond the ones explicitly
laid out in the spec. Another benefit of the passage to higher-order
logic is the smooth treatment of observers with differing invariants;
higher-order quantification lets us store and pass formulas around,
making it easy to allow each callback to have a totally different
invariant. 

\subsection{Correctness Proofs for Subject-Observer}

\subsubsection{Proof of the $\ctext{register}$ Function}
\begin{proof}
\begin{tabbedproof}
\oo Assume we have $f, O, s, cs, n, os$ and the specification $\forall i,k.\;\mspec{O(i)}{{f(k)}}{a:1}{O(k)}$ \\
\ooo Assume we are in the prestate $\setof{\mathit{sub}((s,cs), n, os)}$ \\
\ooo This state is equivalent to $s \pointsto n * \mathit{list}(cs, \mathit{map}\;\snd{}\;os) \land \mathit{Good}(os)$ \\
\ooo By the definition of $\mathit{list}$, we know \\
\ooox $\mathit{list}(cs, \mathit{map}\;\snd{}\;os) = \exists cell.\;cs \pointsto cell * \mathit{listcell}(cell, \mathit{map}\;\snd{}\;os)$ \\
\ooo $[\letv{mathit{cell}}{!r}{}$ \\
\ooo We can drop the existential now, giving a state of \\
\ooox $s \pointsto n * cs \pointsto cell * \mathit{listcell}(cell, \mathit{map}\;\snd{}\;os) \land \mathit{Good}(os)$ \\
\ooo $\letv{r}{[\newref{\reftype{\listtype{(\N \to \monad{\unittype})}}}{\mathit{cell}}]}{}$ \\
\ooo We add $r \pointsto \mathit{cell}$ to the state, and fold the definition of list, to get \\
\ooox $s \pointsto n * cs \pointsto - * \mathit{list}(r, \mathit{map}\;\snd{}\;os) \land \mathit{Good}(os)$ \\ 
\ooo $cs := \Cons(f, r)]$ \\
\ooo Therefore $s \pointsto n * cs \pointsto \Cons(f,r) * \mathit{list}(r, \mathit{map}\;\snd{}\;os) \land \mathit{Good}(os)$ \\ 
\ooo Therefore $s \pointsto n * \mathit{list}(cs, \mathit{map}\;\snd{}\;((O,f) \cdot os)) \land \mathit{Good}(os)$ \\ 
\ooo Since we assume $\forall i,k.\;\spec{O(i)}{\run{f(k)}}{a:1}{O(k)}$, we can conjoin it to the state \\
\ooox to get $s \pointsto n * \mathit{list}(cs, \mathit{map}\;\snd{}\;((O,f) \cdot os)) \land \mathit{Good}((O,f) \cdot os)$ \\ 
\ooo This is $\mathit{sub}((s,cs), (O,f)\cdot os)$
\end{tabbedproof}
\end{proof}

\subsubsection{Proof of the $\ctext{newsub}$ Function}
\begin{proof}
\begin{tabbedproof}
\oo Assume we have a variable $n$ and the initial state $\emp$\\
\ooo Now consider the body of the $\ctext{newsub}$ function: \\
\ooo $[\letv{\mathit{data}}{\newref{\N}{n}}{}$ \\
\ooo The state is $\mathit{data} \pointsto n$ \\
\ooo $\letv{\mathit{callbacks}}{\newref{\listtype{\N\to\monad{1}}}}{\Nil}{}$ \\
\ooo The state is $\mathit{data} \pointsto n * \mathit{callbacks} \pointsto \Nil$ \\
\ooo This is equivalent to $\mathit{data} \pointsto n * \mathit{list}(\mathit{callbacks}, \epsilon)$ \\
\ooo This is equivalent to $\mathit{data} \pointsto n * \mathit{list}(\mathit{callbacks}, \epsilon) \land \mathit{Good}(\epsilon)$ \\
\ooo This is equivalent to $\mathit{sub}(\mathit{(data,callbacks)}, n, \epsilon)$ \\
\ooo $(data, callbacks)]$ \\
\ooo Therefore $\mathit{sub}(a, n, \epsilon)$ with return value $a$ 
\end{tabbedproof}
\end{proof}

\subsubsection{Proof of the $\ctext{broadcast}$ Function}

To prove this function, we first need to prove an auxilliary lemma about the $\ctext{loop}$ function:

\begin{lemma}{($\ctext{loop}$ Invariant)}
For all $k$, $fs$  and $os$ we have
\begin{displaymath}
\spec{\mathit{Good}(os) \land \mathit{list}(fs, \mathit{map}\;\snd{}os) * \mathit{obs}(os)}{\ctext{loop}(k, fs)}{a:1}{\mathit{list}(fs, \mathit{map}\;\snd{}\;os) * \mathit{obs\_at}(os, k)}
\end{displaymath}
\end{lemma}
\begin{proof}
\begin{tabbedproof}
\oo Assume we have a suitable $k$ and $fs$, and then proceed by induction on $os$.  \\
\oo Pull $\mathit{Good}(os)$ into the context and frame it away. \\
\oo Suppose $os = \epsilon$:  \\
\ooo Then our precondition is $\mathit{list}(fs, \epsilon) * \mathit{obs}(\epsilon)$ \\
\ooo This is equivalent to $fs \pointsto \Nil$ \\
\ooo $[\letv{\mathit{cell}}{[!fs]}{}$  \\
\ooo So we know $fs \pointsto \Nil \land \mathit{cell} = \Nil$ \\
\ooo Since $\mathit{cell} = \Nil$ and equational reasoning, we know the remainder of the program is $\unit$\\
\ooo $\unit]$ \\
\ooo So the state is still $\mathit{list}(fs, \epsilon)$ \\
\ooo This is equivalent to $\mathit{list}(fs, \epsilon) * \emp$ \\
\ooo This is equivalent to $\mathit{list}(fs, \epsilon) * \mathit{obs\_at}(\epsilon, k)$ \\
\oo Suppose $os = (O,f) \cdot os'$:  \\
\ooo Then our precondition is $\mathit{list}(fs, f \cdot \mathit{map}\;\snd{}\;os') * \mathit{obs}((O,f)\cdot os')$ \\
\ooo This is equivalent to $\mathit{list}(fs, f \cdot \mathit{map}\;\snd{}\; os') * \mathit{obs}((O,f)\cdot os')$ \\
\ooo This is equivalent to $\exists fs'.\; fs \pointsto \Cons(f, fs') * \mathit{list}(fs',  \mathit{map}\;\snd{}\; os') * \exists j.\;O(j) * \mathit{obs}( os')$ \\
\ooo By dropping existentials, $fs \pointsto \Cons(f, fs') * \mathit{list}(fs',  \mathit{map}\;\snd{}\;os') * O(j) * \mathit{obs}( os')$ \\
\ooo $[\letv{\mathit{cell}}{[!fs]}{}$ \\
\ooo We know $fs \pointsto \mathit{cell}  * \mathit{list}(fs',  \mathit{map}\;\snd{}\;os') * O(j) * \mathit{obs}( os') \land \mathit{cell} = \Cons(f, fs')$ \\
\ooo Since $\mathit{cell} = \Cons(f, fs')$, we can use equational reasoning to eliminate the case \\
\ooo $\letv{\mathit{dummy}}{f(k)}{}$ \\
\ooo We know $fs \pointsto \mathit{cell}  * \mathit{list}(fs',  \mathit{map}\;\snd{}\;os') * O(k) * \mathit{obs}( os') \land \mathit{cell} = \Cons(f, fs')$ \\
\ooo This call is justified because we assumed $\mathit{Good}(os)$ \\
\ooo $\mathsf{loop}(k, fs')]$\\
\ooo By induction, $fs \pointsto \mathit{cell}  * \mathit{list}(fs',  \mathit{map}\;\snd{}\;os') * O(k) * \mathit{obs\_at}( os') \land \mathit{cell} = \Cons(f, fs')$ \\
\ooo This is equivalent to $\mathit{list}(fs, \mathit{map}\;\snd{}\;os) * \mathit{obs\_at}(os, k)$ \\
\oo Therefore $\mathit{Good}(os) * \mathit{list}(fs, \mathit{map}\;\snd{}\;os) * \mathit{obs\_at}(os, k)$ \\
\oo Since $\mathit{Good}$ is pure, we have $\mathit{list}(fs, \mathit{map}\;\snd{}\;os) * \mathit{obs\_at}(os, k)$
\end{tabbedproof}
\end{proof}

There is an interesting feature of this proof which distinguishes it
from the proofs we gave for the iterator implementation. In this
proof, we reason about branches by doing a case analysis in the
program logic, and then using the specification-level equality to
justify \emph{simplifying} the program we are proving. That is, when
we consider an empty input list, we can use the equational theory of
the lambda calculus to simplify the program we are proving to
eliminate that case altogether.

This contrasts with the usual situation in Hoare logic, 




Now, the proof of the $\mathsf{broadcast}$ function is quite easy: 

\begin{tabbedproof}
\oo Assume we have $s, i, os, k$ with a precondition of $\mathit{sub}(s, i, os) * \mathit{obs}(os)$ \\
\oo This is equivalent to $\mathit{Good}(os) \land \fst{s}\pointsto i * \mathit{list}(\snd{s}, \mathit{map}\;\snd{}\;os) * \mathit{obs}(os)$ \\
\oo $[\letv{dummy}{[\fst{s} := k]}{}$ \\
\oo This yields $\mathit{Good}(os) \land \fst{s}\pointsto k * \mathit{list}(\snd{s}, \mathit{map}\;\snd{}\;os) * \mathit{obs}(os)$ \\
\oo $\mathsf{loop}(k, \snd{s})]$ \\
\oo This yields $\mathit{Good}(os) \land \fst{s}\pointsto k * \mathit{list}(\snd{s}, \mathit{map}\;\snd{}\;os) * \mathit{obs\_at}(os, k)$ \\
\oo This is equivalent to $\mathit{sub}(s, k, os) * \mathit{obs\_at}(os,k)$ 
\end{tabbedproof}

% \input{htt-experiments}

\section{Related Work}
% \vspace{-0.5em}

In his dissertation~\cite{parkinson-thesis}, Parkinson gave as an
example a simple iterator protocol, lacking the integration with
composites we have exhibited.  Subsequently, we formalized a similar
account of iterators~\cite{iterator}, again lacking the integration
with composites. Jacobs, Meijer, Piessens and
Schulte~\cite{iterators-revisited} extend Boogie with new rules for
the coroutine constructs C\# uses to define iterators. Their solution
typifies the difficulties ownership-based approaches face with
iterators, which arise from the fact that iterators must have access
to the private state of a collection but may have differing
lifetimes. This work builds on Barnett and Naumann's generalization of
ownership to friendship~\cite{friends}, which allows object invariants
to have some dependency on non-owned objects.

The subject-observer pattern has been the focus of a great deal of effort,
given its prominence in important applications. Simultaneously with our own
initial formulation, Parkinson gave an example of verifying the
subject-observer protocol~\cite{parkinson-iwaco-07}. Recently, Parkinson and
Distefano~\cite{jstar-parkinson-distefano} have implemented a tool to verify
these programs, and have demonstrated several examples including a verification
of a subject-observer pattern specified along these lines. The tool includes
automatic generation of loop invariants. 

The style of invariant in our work and Parkinson's is very similar,
and subject to similar limitations. Since each subject needs to know
what its observers are, verifying programs with chains of
subject-observers is extremely cumbersome. This is especially
problematic given that GUI programs --- which are one of the primary
uses of the subject-observer pattern --- rely upon chains of subjects
and observers. 

The work of Barnett and Naumann is also capable of reasoning about the
subject-observer pattern, but only if all of the possible observers
are known at verification.  Leino and Schulte~\cite{boogie-sub-obs}
made use of Liskov and Wing's concept of history invariants or
monotonic predicates~\cite{liskov-wing} to give a more modular
solution. The idea in this work is to require the changes that a
subject makes to ``increase the truth'' of the observer's predicate
along some partial order. This is less flexible than the approach we
took, though perhaps a little easier to use when it is
applicable. Unfortunately, this work is not merely heavyweight, but
entirely inapplicable for event-driven programs such as GUIs, since
there is no natural partial order on user actions.

More recently, Shaner, Naumann and Leavens~\cite{ShanerLN07} gave a
``gray-box'' treatment of the subject-observer pattern.  Instead of
tracking the specifications of the observers in the predicate, they
give a model program that should approximates the behavior of any
actual notification method. This works as long as the runtime
dependencies are known statically enough to include them in the model
programs --- again, a limitation which is problematic in the case of
GUIs.

Lest the reader get too depressed, in the following two chapters, I
will give a solution to the problem of verifying invariants across
dynamic chains of dependencies.

Pierik, Clarke and de Boer~\cite{creational-invariants} formalize another
extension to the Boogie framework which they name \emph{creation
  guards}, specifically to handle flyweights. They consider flyweights
an instance of a case where object invariants can be invalidated by
the allocation of new objects, and add guards to their specifications
to control allocation to the permitted cases. 



