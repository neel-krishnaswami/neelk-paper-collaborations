\chapter{Proving the Correctness of Design Patterns}

\section{Introduction}

The widespread use of object-oriented languages creates an opportunity
for designers of formal verification systems, above and beyond a
potential ``target market''. Object-oriented languages have been used
for almost forty years, and in that time practitioners have developed
a large body of informal techniques for structuring object-oriented
programs called \emph{design patterns}\cite{GoF}.  Design patterns
were developed to both take best advantage of the flexibility
object-oriented languages permit, and to control the potential
complexities arising from the unstructured use of these features.

This pair of characteristics make design patterns an excellent set of
benchmarks for a program logic.  First, design patterns use higher
order programs to manipulate aliased, mutable state. This is a
difficult combination for program verification systems to handle, and
attempting to verify these programs will readily reveal weaknesses or
lacunae in the program logic. Second, the fact that patterns are
intended to structure and modularize programs means that we can use
them to evaluate whether the proofs in a program logic respect the
conceptual structure of the program -- we can check to see if we need
to propagate conceptually irrelevant information out of program
modules in order to meet our proof obligations. Third, we have the
confidence that these programs, though small, actually reflect
realistic patterns of usage.

In this chapter, we give good specifications for and verify the
following programs:

\begin{itemize}
\item We prove a collection and iterator implementation, which builds
  the Java aliasing rules for iterators into its specification, and
  which allows the construction of new iterators from old ones via the
  composite and decorator patterns.  

\item We prove a general version of the flyweight pattern (also known as
  hash-consing in the functional programming community), which is a
  strategy for aggressively creating aliased objects to save memory
  and permit fast equality tests. This also illustrates the use of the
  factory pattern.

\item We prove a general version of the subject-observer pattern in a
  way that supports a strong form of information hiding between the
  subject and the observers.
\end{itemize}

% Finally, we give machine-verified proofs of the correctness of the
% iterator and flyweight patterns in the Ynot extension of Coq, and
% compare them with the paper proofs. We also see that proper treatment
% of the subject-observer pattern seems to call for the use of an
% impredicative type theory.

\section{Iterators, Composites and Decorators}

The iterator pattern is a design pattern for uniformly enumerating the
elements of a collection. The idea is that in addition to a
collection, we have an auxiliary data structure called the iterator,
which has an operation $\nextiter$. Each time $\nextiter$ is
called, it produces one more element of the collection, with some
signal when all of the elements have been produced. The iterators are
mutable data structures whose invariants depend on the collection,
itself another mutable data structure. Therefore, most object oriented
libraries state that while an iterator is active, a client is only
permitted to call methods on a collection that do not change the
collection state (for example, querying the size of a collection). If
destructive methods are invoked (for example, adding or removing an
element), it is no longer valid to query the iterator again.

We also support operations to create new iterators from old ones, and
to aggregate them into composite iterators. For example, given an
iterator and a predicate, we can construct a new iterator that only
returns those elements for which the predicate returns true. This sort
of decorator takes an iterator object, and \emph{decorates} it to
yield an iterator with different behavior. Likewise, we can take two
iterators and a function, and combine them into a new,
\emph{composite} iterator that returns the result of a parallel
iteration over them.  These sorts of synthesized iterators are found
in the \texttt{itertools} library in the Python programming language,
the Google Java collections library, or the C5 library~\cite{C5} for
C\#.

Aliasing enters into the picture, above and beyond the restrictions on
the underlying collections, because iterators are stateful
objects. For example, if we create a filtering iterator, and advance
the underlying iterator, then what the filtering iterator will return
may change. Even more strikingly, we cannot pass the same iterator
twice to a parallel iteration constructor -- the iterators must be
disjoint in order to correctly generate the two sequences of elements
to combine.

Below, we give a specification of an iterator pattern. We'll begin 
by describing the interface informally, in English, and then move on 
to giving formal specifications and explaining them. 

The interface consists of two types, one for collections, and one for
iterators. The operations the collection type supports are 1) creating
new mutable collections, 2) adding new elements to an existing
collection, and 3) querying a collection for its size. Adding new
elements to a collection is a destructive operation which modifies the
existing collection, whereas getting a collections size does not
modify the collection.

The interface that the iterator type supports are 1) creating a new
iterator on a collection, 2) destructively getting the next element
from an iterator (returning an error value if the iterator is
exhausted), and 3) operations to produce new iterators from old. The
iterator transformations we support are 1) a filter operation, which
takes an iterator along with a boolean predicate, and returns an
iterator which enumerates the elements satisfying the predicate, and
2) a parallel map operation, which takes two iterators and a
two-argument function, and returns an iterator which returns the
result of enumerating the elements of the two iterators in parallel,
and applying the function to each pair of elements. 

The aliasing protocol that our iterator protocol will satisfy is
essentially the same as the one the Java standard libraries specify in
their documentation.

\begin{itemize}
\item Any number of iterators can be created from a given collection.
  Each of these iterators has the collection as its underlying state.

\item An iterator constructed from other iterators has underlying 
  state consisting of its arguments, as well as their backing states. 

\item An iterator is valid as long as none of its underlying state has
  been destructively modified from the time of the iterator's creation. An
  iterators underlying state consists of any collections it depends
  on, as well as any iterators it was constructed from.

\item It is legal to call functions on an iterator only when it 
  is in a valid state. Performing a destructive operation on any 
  part of an iterator's underlying state invalidates it. For example,
  adding an element to a collection in an iterator's underlying state
  will invalidate it, as will trying to get elements from any other 
  iterators in its underlying state. 
\end{itemize}

\subsection{The Iterator Specification}

Now, we will describe the specification, given in
Figure~\ref{iterator-interface}, in detail. This whole specification
follows the usual pattern of introducing abstract types, predicates,
and operations with existential quantification, and then specifying
the behavior of the operations with a conjunction of Hoare triples.

In lines \ref{decl:colltype} and \ref{decl:itertype}, we introduce two
abstract type constructors, $\colltype$ and $\itertype$. These are
both type constructors of kind $\star \to \star$, and take an argument
which describes their element type. So $\colltype(\N)$ represents a
collection of natural numbers, and $\itertype(\bool)$ represents an
iterator which will produce a sequence of boolean values.

In lines \ref{decl:collpred} and \ref{decl:iterpred}, we give two
abstract predicates $\collpred$ and $\iterpred$, to represent the
state associated with a collection and iterator, respectively. The
sort of the collection predicate is $\Pi \alpha:\star.\;
\colltype(\alpha) \To \seqsort{\alpha} \To \assert \To \assert$, which
we will write using an expression like $\collpred_\alpha(c, xs, P)$.

The first argument is a type argument (e.g., $\alpha$), indexing the
whole predicate by the element type of the collection. The second
argument is an argument of collection type (in our example, $c$, of
type $\colltype(\alpha)$) which indicates the value with which our
state is associated. The third argument (here, $xs$) is the purely
mathematical sequence (i.e., an element of the free monoid over
$\alpha$) the collection $c$ represents.  The fourth, and final,
argument is a proposition-valued \emph{abstract state} of the
collection, which we use to track whether or not the collection has
been modified or not.

The appearance of this argument might be a little bit surprising:
naively, we might suppose the mathematical sequence that collection
represents constitutes a sufficient description of the collection, and
so we might expect our predicates to take on the form
$\collpred_\alpha(c, xs)$, with no state argument. However, this is
\emph{not} sufficient, and the reason we need an abstract state is
that the iterator contract forbids modifying the collection at all,
while iterators are active upon it, and it turns out that the
mathematical sequence a collection represents is not enough to decide
whether a collection has been modified or not.

For example, suppose we have a collection $c$, which represents the
mathematical sequence $\left<2, 3, 4, 5\right>$, which -might have the
predicate $\collpred_\N(c, \left<2, 3, 4, 5\right>, P)$.  Now, suppose
we first add the element $1$ to the front of the sequence (so that the
collection $c$ now represents $\left<1, 2, 3, 4, 5\right>$), and then
immediately remove the first element. Then, the collection $c$ will
still represent the sequence $\left<2, 3, 4, 5\right>$, but it will
have suffered an intervening modification.

This kind of change can be catastrophic for iterator implementations.
As a concrete example, suppose that we had represented our collection
with a balanced binary tree, and represent the iterator as a pointer
into the middle of that tree. Adding and removing elements from the
collection can cause a tree rebalancing, which can potentially leave
the iterator pointer with a stale or dangling pointer.

As a result, our specification has to have some way of establishing
whether a collection has been modified or not, and this is what the
abstract state field on the predicate is intended to track. Operations
on a collection which do not change its underlying state will leave
the abstract state unchanged from pre- to post-condition, whereas
destructive operations (such as adding or removing elements) will
change the abstract state.

On line \ref{decl:iterpred}, we assert the existence of the iterator predicate
$\iterpred$.  It is also a four-place predicate, and has sort $\Pi
\alpha:\star.\; \colltype(\alpha) \To \seqsort{\alpha} \To
\powerset{\assert} \To \assert$, which we will write using an
expression like $\iterpred_\alpha(i, xs, S)$.

The first argument (in our example, $\alpha$) is a type argument
describing the type of elements the iterator will produce. The second
argument (here, $i$) is the concrete iterator value to which the
predicate is associated. Then, we have a sequence argument (here,
$xs$) which describes the elements yet to be produced from this
iterator -- if $xs = \left<5, 7, 9\right>$, then the next three
elements the iterator will yield are 5, 7, and 9, in that order.
Subsequently the iterator will be empty and unable to produce any
more elements.

Finally, we have a state argument for iterators, as well. In contrast
to the case for collections, this argument is a set of propositions,
representing an entire set of abstract states! The reason for this is
that our interface will also support operations which allow building
new iterators from old, and so an iterator might read many different
collections to produce a single new element. As a result, we have to
track the abstract state of each collection the iterator depends on,
so that we can verify that we do not ever need to read a modified
collection.

\begin{figure}
\mbox{}
\begin{specification}
\nextlinelabel{decl:colltype}
$\exists \colltype : \star \to \star$  
\nextlinelabel{decl:itertype}
$\exists \itertype : \star \to \star$  
\nextlinelabel[0.5em]{decl:collpred}
$\exists \collpred : 
        \Pi \alpha:\star.\; \colltype(\alpha) \To \seqsort{\alpha} \To \assert \To \assert.$ 
\nextlinelabel{decl:iterpred}
$\exists iter : 
        \Pi \alpha:\star.\; \itertype(\alpha) \To \seqsort{\alpha} \To \powerset{\assert} \To \assert.$ 
\nextlinelabel[0.5em]{decl:newcoll-type}
$\exists \newcoll : 
         \forall \alpha:\star.\; \monad{(\colltype(\alpha))}.$
\nextlinelabel{decl:size-type}
 $\exists \sizecoll : 
         \forall \alpha:\star.\; \colltype(\alpha) \to \monad{\N}.$ 
\nextlinelabel{decl:add-type}
 $\exists \addcoll : 
         \forall \alpha:\star.\; \colltype(\alpha) \times \alpha \to \monad{\unittype}.$
\nextlinelabel{decl:remove-type}
 $\exists \removecoll :
         \forall \alpha:\star.\; \colltype(\alpha) \to \monad{(\opttype{(\alpha)})}$. 
\nextlinelabel[0.5em]{decl:newiter-type}
 $\exists \newiter : 
         \forall \alpha:\star.\; \colltype(\alpha) \to \monad{(\itertype(\alpha))}.$ 
\nextlinelabel{decl:filter-type}
 $\exists \filteriter : 
         \forall \alpha:\star.\; (\alpha \to \ctext{bool}) \times \itertype(\alpha) 
                                 \to \monad{(\itertype(\alpha))}.$ 
\nextlinelabel{decl:zip-type}
$\exists$\=$ \mergeiter : 
         \forall \alpha:\star.\; (\alpha \to \alpha \to \alpha) \times
                                 \itertype(\alpha) \times \itertype(\beta) 
                                   \to \monad{(\itertype(\alpha))}.$
\nextlinelabel{decl:next-type}
$\exists$\=$\nextiter : 
         \forall \alpha:\star.\; \itertype(\alpha) \to \monad{(\opttype{(\alpha)})}.$  
\nextlinelabel[0.5em]{decl:newcoll-spec}

\> $\forall \alpha.\; \spec{\emp}{\run{\newcoll[\alpha]}}
                                 {a:\colltype(\alpha)}{\exists P.\; \collpred_\alpha(a, \epsilon,P)}$ $\specand$ 
\nextlinelabel[0.5em]{decl:size-spec}

\> $\forall \alpha, c, P, xs.\;$\=
         $\setof{\collpred_\alpha(c, xs, P)}$
\nextline
\> \>  $\run{\sizecoll[\alpha](c)}$ 
\nextline
\> \>  $\setof{a:\N.\; \collpred_\alpha(c, xs, P) \land a = |xs|}$  $\specand$ 
\nextlinelabel[0.5em]{decl:add-spec}

 \> $\forall \alpha, c, P, x, xs.\;$\=
               $\setof{\collpred_\alpha(c, xs, P)}$ 
\nextline
\>\>         $\run{\sizecoll[\alpha](c, x)}$
\nextline
\>\>         $\setof{a:1.\; \exists Q.\; \collpred_\alpha(c, x\cdot xs, Q)}$ $\specand$ 

\nextlinelabel[0.5em]{decl:remove-spec-1}

\> $\forall \alpha, c, P.\; \spec{\collpred_\alpha(c, \epsilon, P)}
                                        {\run{\removecoll[\alpha](c)}}
                                        {a:\opttype{(\alpha)}}
                                        {\collpred_\alpha(c, \epsilon, P) \land a = \None}$
$\specand$ 
\nextlinelabel{decl:remove-spec-2}
\> $\forall \alpha, c, x, xs, P.$\=
          $\setof{\collpred_\alpha(c, x\cdot xs, P)}$ 
\nextline
\>\> $\run{\removecoll[\alpha](c)}$ 
\nextline
\>\> $\setof{a:\opttype{(\alpha)}.\;
             \exists Q.\;\collpred_\alpha(c, xs, Q) \land a = \Some(x)}$ 
\nextlinelabel[0.5em]{decl:newiter-spec}

\> $\specand \forall \alpha, c, P, xs.\;$\=
            $\setof{\collpred_\alpha(c, xs, P)}$ 
\nextline
\>\>$\run{\newiter[\alpha](c)}$
\nextline
\>\>$\setof{a:A_i.\; \collpred(c, xs, P) * \iterpred(a, xs, \setof{P})}$ $\specand$ 
\nextlinelabel[0.5em]{decl:filter-spec}

 \> $\forall \alpha, p, i, S, xs.\;$\=
         $\setof{\iterpred_\alpha(i, xs, S)}$ 
\nextline
\>\>   $\run{\filteriter[\alpha](p, i)}$
\nextline
\>\>   $\setof{a:A_i.\; \iterpred_\alpha(a, \mathit{filter}\; p\;xs, S)}$ $\specand$ 
\nextlinelabel[0.5em]{decl:zip-spec}

 \> $\forall \alpha, f, i, S,$\=$ xs, i', S', xs'.\;$ 
\nextline
 \> \> 
     $\setof{\iterpred_\alpha(i, S, xs) * \iterpred_\beta(i', S', xs')}$ 
\nextline
 \> \> $\run{\mergeiter[\alpha](f, i, i')}$ 
\nextline
 \> \> $\setof{a:A_i.\; \iterpred_{\alpha}(a, S \cup S', \mathit{map2}\;f\;xs\;xs')}$ $\specand$ 
\nextlinelabel[0.5em]{decl:next-spec-1}

 \> $\forall i, C, S.\;$\=
      $\setof{\mathit{colls}(C, S) * \iterpred_\alpha(i, S, \epsilon)}$
\nextline  
\>\>$\run{\nextiter(i)}$ 
\nextline
\>\>$\setof{a:\opttype{(\alpha)}.\; \mathit{colls}(S) * \collpred_\alpha(i, S, \epsilon) \land a = \None}$ $\specand$ 
\nextlinelabel[0.5em]{decl:next-spec-2}

 \> $\forall i,C, $\=$ S, x, xs.\;$ \= 
      $\setof{\mathit{colls}(C, S) * \iterpred_\alpha(i, S, x \cdot xs)}$
\nextline
\>\>\>$\run{\nextiter(i)}$
\nextline
\>\>\>$\setof{a:\opttype{(\alpha)}.\; 
              \mathit{colls}(C, S) * \iterpred_\alpha(i, S, xs) \land a = (\Some\;x)}$ 

\end{specification}
\caption{Interface to the Iterator Library}
\label{iterator-interface}
\end{figure}

\begin{figure}
\mbox{}
\begin{specification}
\nextline
$\mathit{colls}(\emptyset, \emptyset) \qquad\qquad\qquad\qquad\;\;$ \=$\equiv\;$\= $\emp$ 
\nextline
$\mathit{colls}(\setof{(c,xs)} \cup C, \setof{P} \cup S)$ \> $\equiv$ \> $\collpred(c, xs, P) * \mathit{colls}(C, S)$ 

\nextlinelabel[0.5em]{decl:mathfilter-impl}
$\mathit{filter}\;p\;\epsilon \qquad\quad$\=$\equiv \epsilon$
\nextline
$\mathit{filter}\;p\;(x\cdot xs)$ \> $\equiv \ctext{if}\;p\;x = \ctext{true}\;\ctext{then}\;
                                         x\cdot(\mathit{filter}\;p\;xs)\;
                                      \ctext{else}\; \mathit{filter}\;p\;xs$ 

\nextlinelabel[0.5em]{decl:map2-impl}
$\mathit{map2}\;f\;\epsilon\;ys \qquad\qquad\qquad $\=$= \epsilon$ 
\nextline
$\mathit{map2}\;f\;xs\;\epsilon $\>$= \epsilon$ 
\nextline
$\mathit{map2}\;f\;(x\cdot xs)\;(y\cdot ys)$ \>$= (f\;x\;y)\cdot(\mathit{map2}\;f\;xs\;ys)$
    
\end{specification}
\caption{Auxilliary Functions Used in the Iterator Specification}
\label{iterator-interface-aux}
\end{figure}

The operation $\newcoll$ is declared on line \ref{decl:newcoll-type},
and specified on line \ref{decl:newcoll-spec}. The specification
asserts that the call $\newcoll[\alpha]$ may happen from any
precondition state, and that it creates and adds a new, empty
collection to the program state. The postcondition assertion $\exists
P.\; coll_\alpha(a, \epsilon, P)$ says that the return value $a$ is a
collection representing the empty sequence $\epsilon$, and that the
collection begins its life in some arbitrary abstract state $P$.

The $\sizecoll[\alpha](c)$ function, which is declared on line
\ref{decl:size-type} and specified on line \ref{decl:size-spec}, takes
a type argument $\alpha$ and a collection $c$, and returns the number
of elements in $c$. To call this function, we must have access to the
collection $\collpred_\alpha(c, xs, P)$ in our precondition, and it is
returned to us unchanged in the postcondition, with the return value
$a$ equal to the length of $xs$, the sequence $c$ represents. In
particular, note that the abstract state $P$ of the $coll(c, xs, P)$
predicate remains unchanged in the pre- and post-conditions,
indicating that this function does not change the abstract state.

The function call $\addcoll[\alpha](c, x)$, which adds an element $x$
to a collection $c$, is declared on line \ref{decl:add-type} and is
specified on line \ref{decl:add-spec}. We start with a precondition
$\collpred_\alpha(c, xs, P)$ and move to a postcondition state
$\exists Q.\; \collpred_\alpha(c, x\cdot xs, Q)$. Because we
destructively modify the collection $c$ when we add $x$ to it, we also
specify that the abstract state in the postcondition is existentially
quantified. This ensures that clients cannot assume that the abstract
state remains the same after a call to $\addcoll$ has been made. In
this way, the abstract state behaves a bit like a time stamp, changing
to some new state whenever a modification is made to the collection. 

Similarly, the function call $\removecoll[\alpha](c)$ (declared on
line \ref{decl:remove-decl}) removes an element from the collection
$c$. We give this procedure two specifications, on lines
\ref{decl:remove-spec-1} and \ref{decl:remove-spec-2}, corresponding
to when the collection is empty, or not. 

In the first specification (on line \ref{decl:remove-spec-1}), we
begin with the precondition $\collpred_\alpha(c, \epsilon, P)$, and
end in the postcondition $\collpred_\alpha(c, \epsilon, P) \land a =
\None$. The fact that the abstract state remains $P$ means the
collection is unchanged, and the return value $a$ equals $\None$, 
an element of option type, indicating that there was no element to remove.

In the second specification (on line \ref{decl:remove-spec-2}), we
begin with the precondition $\collpred_\alpha(c, x\cdot xs, P)$, from
which we can see that the collection is nonempty. Then, the postcondition
is $\exists Q.\; \collpred_\alpha(c, xs, Q) \land a = \Some(x)$. 
The value $\Some(x)$ is returned as the return value of the function,
and the state of the collection changes to reflect that the element $x$
has been removed --- including a change to the abstract state of the 
collection. 

As an aside, in practice it is usually more convenient to specify a
procedure with a single Hoare triple, rather than multiple Hoare
triples. However, in this example, I choose to give multiple
specifications of the same procedure in order to illustrate that it is
indeed possible within specification logic.

The $\newiter[\alpha](c)$ function is declared on line
\ref{decl:newiter-type}.  Its type is $\forall \alpha:\star.\;
\colltype(\alpha) \to \monad{(\itertype(\alpha))}$.  This means that
it is given a type and a collection of that type, and then it returns
iterator over that type, possibly creating auxilliary data structures.

A call $\newiter[\alpha](c)$ is specified on line
\ref{decl:newiter-spec}, and beginning from a precondition state
$\collpred_\alpha(c, xs, P)$, it goes to a postcondition state
$\collpred_\alpha(c, xs, P) * \iterpred_\alpha(a, xs, \setof{P})$.
This means that given access to a collection $c$, our function will
return an iterator object (bound to $a$), which will enumerate the
elements of $c$ (that is, it will produce the elements
$xs$). Furthermore, the abstract state that it depends on is just the
singleton set $\setof{P}$, since this iterator will read only $c$.
Finally, the fact that $\collpred_\alpha(c, xs, P)$ occurs in both
the pre- and the post-condition means that this function needs 
access to $c$'s state, but does not modify its abstract state. 

The $\filteriter[\alpha](p, i)$ (declared on line \ref{decl:filter-type})
takes a boolean function $p$ and an iterator $i$, and returns a new
iterator which will enumerate only those elements which for which $p$
returns true. This function is specified on line \ref{decl:filter-spec}, 
and it takes a precondition $\iterpred_\alpha(i, xs, S)$ to a postcondition
$\iterpred_\alpha(a, \mathit{filter}\;p\;xs, S)$. 

First, note that we use a mathematical function $\mathit{filter}$ to
explain the filtering behavior in terms of sequences. Second, note
that the original iterator state $\iterpred_\alpha(i, xs, S)$ vanishes
from the postcondition -- it is consumed by the call the
$\filteriter$.  This reflects the fact the filtered iterator takes
ownership of the underlying iterator, in order to prevent third
parties from making calls to $\nextiter(i)$ and possibly changing the
state of the filtered iterator.

This is also why the support set $S$ for an iterator only needs to
track the abstract states of the collections, rather than tracking the
state of both collections and iterators. When we take ownership of the
argument's iterator state, we prevent third parties from being able to
call functions on the argument after creating the new iterator. This
takes advantage of the resource-conscious nature of separation logic:
a specification must have access to its footprint, and so we can hide
state inside a predicate to control which operations are allowed.

The $\mergeiter$ function is declared on line $\ref{decl:zip-type}$, and
has type $\forall \alpha:\star.\; (\alpha \to \alpha \to \alpha) \times \itertype(\alpha) \times
\itertype(\beta) \to \monad{(\itertype(\alpha \times \beta))}$.  Thus,
a call $\mergeiter[\alpha](f, i_1, i_2)$ takes a function and two 
iterators, and constructs a new iterator which steps over the two inputs in parallel,
returning the result of $f$ applied to each pair of elements of $i_1$ and $i_2$. 

We specify calls $\mergeiter[\alpha](f, i_1, i_2)$ on line
$\ref{decl:zip-spec}$, and it takes a precondition
$\iterpred_\alpha(i_1, xs, S_1) * \iterpred_\alpha(i_2, ys, S_2)$.
This means that we have state associated with two separate iterators,
which we take to the postcondition $\iterpred_{\alpha\times\beta}(a,
\mathit{map2}\;f\;xs\;ys, S_1 \cup S_2)$. As with the $\filteriter$ function, 
we consume the two input iterators to produce the return value iterator. 
And also as with $\filteriter$, we use a mathematical function $\mathit{map2}$
to specify the action on mathematical sequences. 

One point worth noting is that it is important that the two argument
iterators are separate from one another. In a functional program,
there is no difficulty with a program $\mathit{map2}\;f\;xs\;xs$, because
we are free to re-traverse a list multiple times. However, since
traversing an iterator is a destructive operation, a call like
$\mergeiter[\tau]\;f\;i\;i$ could (if it were allowed) give the wrong answer,
for example by pairing consecutive elements of the iterator.

The final operation in our interface is the $\nextiter$ function,
declared on line \ref{decl:next-type}. The type of this function
is $\forall \alpha:\star.\; \itertype(\alpha) \to
\monad{(\opttype{(\alpha)})}$.  When invoked, it will return an
option, with the $\None$ value if the iterator is exhausted,
and $\Some$ of an element if the iterator still has elements to
produce.

As with $\removecoll$, we specify this procedure with two
specifications, one for the case when the iterator is empty and
another for one it is non-empty. On line \ref{decl:next-spec-1}, we
give the specification for when the iterator is exhausted, and
on line \ref{decl:next-spec-2}, we give the specification for when
the iterator is not exhausted.  

In either case, the precondition for the function contains as one part
the predicate $\mathit{colls}(C, S)$.  The assertion-level function
$\mathit{colls}(C,S)$ is a function that iterates over a set of
abstract states, and re-associates them with collection predicates
(coming from the argument $C$) in the precondition, to form a
predicate $\collpred_{\tau_1}(c_1, xs_1, S_1) * \ldots *
\collpred_{\tau_n}(c_n, xs_n, S_n)$.  This expresses the requirement
that we need \emph{all} of the collections $i$ depends on, all in the
correct abstract state.

This function is defined in Figure~\ref{iterator-interface-aux}.\footnote{Technically, 
this is an abuse of notation, since primitive recursion is not well defined on
sets, and so this ``function'' is nondeterministic.  The proper way
to do this would be to introduce a three-place relation
$\mathit{colls(C, S, R)}$ and put $R$ in the precondition
state. However, as no confusion is possible I will retain the
functional form.}

In line \ref{decl:next-spec-1}, $\mathit{colls}(C, S)$ is joined with 
the specification of the iterator, $\iterpred_\alpha(i, \epsilon, S)$. Note
that the same $S$ is used, so that we are referring only to the collections
the iterator may need to read.  As expected, $\nextiter[\alpha](i)$ returns 
$\None$.  On the other hand, if the iterator still has elements (i.e., is in a state
$iter_\alpha(i, x\cdot xs, S)$), we use the specification on line \ref{decl:next-spec-2}, 
and see it returns the first element as
$\Some\;x$, and sets the state to $iter(i, S, xs)$ in the
postcondition (line 15). 

\subsection{Example Implementation}

In this subsection, we describe one particular implementation of
iterators, based on a simple linked list implementation. The type and
predicate definitions are given in Figure~\ref{iterator-pred-impl},
and the implementation of the procedures is given in
Figure~\ref{iterator-implementation}. 


\subsubsection{Definitions of the Types and Predicates}

We'll begin by giving an intuitive explanation of the predicates,
before giving correctness proofs for the operations.

Starting on line \ref{decl:colltype-impl} of
Figure~\ref{iterator-pred-impl}, we define the type of
collections. Technically, these are recursive type definitions, which
we did not define in our semantics in Chapter 1. Fortunately, there is
no great difficulty in these definitions --- we are giving polynomial
data types, and we can justify these definition via the fact that for
every polynomial functor $F$, the category of $F$-algebras over CPO
has an initial object (which means that the data type and primitive
iteration over it are well-defined).

The type of collections $\colltype(\tau)$ is a mutable linked list,
consisting of a reference to a value of type $\Listcontent(\tau)$. A
list content is either a $\Nil$ value, or a cons cell $\Cons(x, tl)$
consisting of a value of type $\tau$ and a tail list of type
$\colltype(\tau)$. Unlike the typical definition of purely functional
linked lists in ML, the tails of a list are mutable references, rather
than list values.

The type of iterators, given in line \ref{decl:itertype-impl}, is very
simple -- an iterator is just a pointer to a list. This type arises as
follows. Since the tails of a linked list are themselves lists, we can
represent an interior pointer to a list with a pointer to list, giving
us the type of a pointer to a list as the type of iterators. 

Then, on line \ref{decl:listpred-impl}, we define the auxilliary
predicate $\listpred(\tau, c, xs)$, which asserts that $c$ is a list
value representing the mathematical sequence $xs$, with element type
$\tau$. This is defined by recursion over the sequence $xs$, with the
empty sequence represented by a pointer to $\Nil$, and a sequence 
$y\cdot ys$ represented by a pointer to a cons cell whose head is $x$
and whose tail is a collection representing $ys$. 

\begin{figure}
\mbox{}
\begin{specification}
\nextlinelabel{decl:colltype-impl}
$\Listcontent(\alpha) = \Nil \bnfalt \Cons\; \alpha \times \reftype{\Listcontent(\alpha)}$ 
\nextline
$\ctext{colltype}(\alpha) = \reftype{(\ctext{listcontent}(\alpha))}$ 
\nextlinelabel[0.5em]{decl:itertype-impl}
$\itertype{(\alpha)} =$\=$\;\One\;\reftype{(\colltype{(\alpha)})} \bnfalt 
                          \Filter\;(\alpha \to bool) \times \itertype{(\alpha)}$
\nextline
                       \>$\bnfalt \Merge\; (\alpha \to \alpha \to \alpha) \times \itertype(\alpha) \times \itertype(\alpha)$
\nextlinelabel[0.5em]{decl:listpred-impl}
$\listpred(\tau, c, xs)$ \qquad\= $\equiv \exists v.\; c \pointsto_\tau v * \listcontentpred(\tau, v, xs)$ 
\nextline
$\listcontentpred(\tau, \Nil, xs)$ \qquad\qquad\= $\equiv$ \= $xs = \epsilon$ 
\nextline
$\listcontentpred(\tau, \Cons(y,c), xs)$ \> $\equiv$ \> 
   $\exists ys.\; xs = y\cdot ys \land \listpred(\tau, c, ys)$ 
\nextlinelabel[0.5em]{decl:collpred-impl}
$collpred_\tau$\=$(c, xs, P) \equiv list(\tau, c, xs) \land P \land \mbox{exact}(P)$ 

\nextlinelabel[0.5em]{decl:iterpred-impl}
$\iterpred_\tau(\One\;i, xs, \setof{P}) \qquad\qquad$\=$\equiv \exists c.\; i \pointsto c * (P \wand (P \land (\top * \listpred(\tau, c, xs))))$ 
\nextline
$\iterpred_\tau(\Filter(p, i), xs, S)$ \>$\equiv 
  \exists ys.\; \iterpred_\tau(i, ys, S) \land xs = \mathit{filter}\;p\;ys$ 
\nextline
$\iterpred_{\tau}(\Merge(f, i_1, i_2), xs, S)$ \>$\equiv 
  \exists S_1, ys, S_2, zs.$ 
\nextline
  \> \qquad \= $\iterpred_\tau(i_1, ys, S_1) * \iterpred_\tau(i_2, zs, S_2) \;\land$
\nextline \> \>$S = S_1 \cup S_2 \land xs = \mathit{map2}\;f\;ys\;zs$ 
\end{specification}
\caption{Type and Predicate Definitions of the Iterator Implementation}
\label{iterator-pred-impl}
\end{figure}

\begin{figure}
\mbox{}
\begin{specification}
\nextlinelabel[0.5em]{decl:newcoll-impl}
$\newcoll[\alpha] \equiv \comp{\newref{\alpha}{\Nil}}$ 
\nextlinelabel[0.5em]{decl:sizecoll-impl}
$\sizecoll[\alpha](c) \equiv $\=
         $[$\=$\letv{p}{\comp{!c}}{}$ 
\nextline\> \>$\Run\;
               \Listcase($\=$p,$ 
\nextline\> \>                   \>$\Nil \to \comp{0},$
\nextline\> \>                   \>$\Cons(\_, tl) \to 
                                     \comp{\letv{n}{\sizecoll[\alpha](tl)}{n+1}})]$

\nextlinelabel[0.5em]{decl:addcoll-impl}
$\addcoll[\alpha](c, x) \equiv [$
          \=$\letv{p}{\comp{!c}}{}$ 
\nextline \>$\letv{c'}{\newref{\ctext{listcontent}(\alpha)}{p}}{}$ 
\nextline \>$c := \Cons(x, c')]$ 

\nextlinelabel[0.5em]{decl:removecoll-impl}
$\removecoll[\alpha](c) \equiv [$\=
            $\letv{p}{\comp{!c}}{}$ 
\nextline \>$\Run\;\Listcase($\=$p,$ 
\nextline \>                                \>$\Nil \to \comp{\None},$ 
\nextline \>                                \>$\Cons(x, c') \to [$\=$\letv{p'}{\comp{!c'}}{}$ 
\nextline \>                                \> \>$\letv{\_}{\comp{c := p'}}{}$ 
\nextline \>                                \> \>$\Some(x)])]$

\nextlinelabel[0.5em]{decl:newiter-impl}
$\newiter[\alpha](c) \equiv \comp{\letv{i}{\comp{\newref{\colltype(\alpha)}{c}}}{\One(i)}}$

\nextlinelabel[0.5em]{decl:filteriter-impl}
$\filteriter[\alpha](p, i) \equiv \comp{\Filter(p, i)}$

\nextlinelabel[0.5em]{decl:mergeiter-impl}
$\mergeiter[\alpha](f, i, i') \equiv \comp{\Merge(f, i, i')}$ 

\nextlinelabel[0.5em]{decl:next-one-impl}
$\nextiter[\alpha](\One\;i) \equiv [$\=$\letv{c}{\comp{!i}}{}$ 
\nextline \> $\letv{p}{\comp{!c}}{}$ 
\nextline \> $\Run\;\Listcase($\=$p,$ 
\nextline \>                                  \>$\Nil \to \comp{\None},$ 
\nextline \>                                  \>$\Cons(x, c') \to [$\=$\letv{\_}{\comp{i := c'}}{\Some(x)}])]$ 
\nextlinelabel{decl:next-filter-impl}
$\nextiter[\alpha](\Filter(p, i)) \equiv [$\= 
           $\letv{v}{\nextiter[\alpha](i)}$ 
\nextline\>$\Run\;\Listcase($\=$v,$ 
\nextline\>\> $\None \to \comp{\None},$ 
\nextline\>\> $\Some\;x \to \IfThenElse{p\;x}{\comp{v}}{\nextiter[\alpha](\Filter(p,i))})]$ 
\nextlinelabel{decl:next-merge-impl}
$\nextiter[\alpha](\Merge(f, i_1, i_2)) \equiv [$\=
            $\letv{x_1}{\nextiter[\alpha](i_1)}{}$ 
\nextline\> $\letv{x_2}{\nextiter[\alpha](i_2)}{}$ 
\nextline\> $\Optcase($\=$x_1,$ 
\nextline\> \>           $\None \to \None,$ 
\nextline\> \>           $\Some\;v_1 \to \Optcase($\=$x_2,$ 
\nextline\> \> \>                                 $\None \to \None,$
\nextlinelabel{decl:next-impl-end}\> \> \>                                 $\Some\;v_2 \to f\;v_1\;v_2))]$
                                                   
\end{specification}
\caption{Implementation of Collections and Iterators}
\label{iterator-implementation}
\end{figure}

The collection predicate $\collpred_\tau(c, xs, P)$, defined on line
\ref{decl:collpred-impl}, makes use of the list predicate. In addition
to asserting that the value $c$ represents the sequence $xs$, it
asserts two further things. First, it says that this program state is
also described by the abstract predicate $P$, and that this predicate
is an \emph{exact} predicate.

Exact predicates are predicates that hold of exactly one heap: that
is, they are the atomic elements of the lattice of assertions. This
means that they uniquely identify a heap data structure. This property
lets us track modifications to the collection: any change to the actual
heap structure will result in the falsification of $P$.  

The iterator predicate, defined on line \ref{decl:iterpred-impl} is 
given as a recursive definition. The base case is when we have an 
iterator over a single collection, in the $\One(i)$ case.  Here, 
we have the following assertion: 

\begin{displaymath}
  \iterpred_\tau(\One(i), xs, P) \equiv 
    \exists c.\; i \pointsto c * P \wand (P \land (\top * \listpred(\tau, c, xs)))
\end{displaymath}

The $i \pointsto c$ clause says that $i$ is a pointer to a linked
list.  The second clause of this invariant is more complex, and its
purpose is to say that $c$ is an interior pointer into a collection.

Note that $P$ will come from the abstract predicate field of some
collection, and so will be an exact predicate. So the conclusion 
of the magic wand, $(P \land (\top * \listpred(\tau, c, xs)))$, says
that the state is $P$, and can also be viewed as containing the 
list starting at $c$ and representing the sequence $xs$. 

Adding the magic wand to the formula , so that we get $(P \land (\top
* \listpred(\tau, c, xs)))$, ends up meaning, ``if you give me the
collection state $P$, then I promise to you that it contains $c$,
which represents $xs$.'' In this way, we express the fact that the
iterator's invariant depends on having controlled access to the state
of the collection. However, we are also able to avoid giving the
iterator state direct ownership of the collection; the magic wand
ensures that we can follow the pointer $c$ when we are given the
collection state of the collection.

The reason we go to this effort is to simplify the specification and
proof of client programs -- we could eliminate the use of the magic
wand in the base case if iterators owned their collections, but this
would complicate verifying programs that use multiple iterators over
the same collection, or which want to call pure methods on the
underlying collection. In those cases, the alternative would require
us to explicitly transfer ownership of the collection in the proofs of
client programs, which is quite cumbersome, and forces clients to
reason using the magic wand. The current approach isolates that
reasoning within the proof of the implementation.

Also, I should note that exactness plays a role in making this use of
the magic wand work correctly. The semantics of the magic wand $p
\wand q$ quantify over all heaps in $p$, but if $p$ is exact, then
there is at most one satisfying heap. This lets us treat the wand as a
``subtraction'' operator, which is ordinarily not legitimate. There is
possibly a connection to Parkinson's ``septraction''
operator~\cite{parkinson-septraction}. 

One the next line, we give the case for $\iterpred_\tau(\Filter(p, i),
xs, S)$.  This is a very simple formula; we simply assert that $i$ is
an iterator yielding some other sequence $ys$, which when filtered with
the predicate $p$ is $xs$. There are no changes to the set of abstract
states. 

On the line after that, we give the case for $\iterpred_\tau(\Merge(f, i_1, i_2), xs, S)$. 
This case says we can divide the abstract state into two parts, one of which is 
used by $i_1$, and the other of which is used by $i_2$,  which 
yield sequences $ys$ and $zs$ respectively, and which can be merged using
$f$ to yield $xs$. 

In both of these cases, we define the behavior of the imperative
linked list in terms of purely functional sequences. This is a very
common strategy in many verification efforts, but here we see that we
can use it in a local way -- in the base case, we are forced to
consider issues of aliasing and ownership, but in the inductive cases
we can largely avoid that effort.

\subsubsection{Correctness Proofs of the Iterator Implementation}

Now that we know the definitions of the types and predicates, we can
give the correctness proofs for the operations defined in
Figure~\ref{iterator-implementation}. 

\begin{lemma}{(Correctness of $\newcoll$)}
  We have that 
  \begin{displaymath}
    \forall \alpha.\; \spec{\emp}
                           {\run{\newcoll[\alpha]}}
                           {a:\colltype(\alpha)}{\exists P.\; \collpred_\alpha(a, \epsilon,P)}
  \end{displaymath}
  is valid. 
\end{lemma}

\begin{proof}
All $\newcoll[\alpha]$ does is allocate a list. To prove the
specification, we will assume that $\alpha:\star$, and then prove the
program in annotated program style.

\begin{specification}
\nextline $\setof{\emp}$ 
\nextline $\newref{\alpha}{\Nil}$ 
\nextline $\setof{a \pointsto_\alpha \Nil}$ 
\nextline $\setof{\listpred(\alpha, a, \epsilon)}$ 
\nextline $\setof{\listpred(\alpha, a, \epsilon) \land \exists Q.\; Q \land \exact{Q}}$ 
\nextline $\setof{\exists Q.\;\listpred(\alpha, a, \epsilon) \land Q \land \exact{Q}}$ 
\nextline $\setof{\exists Q.\; \collpred_\alpha(a, \epsilon, Q)}$ 
\end{specification}

Line 4 follows from 3, because of the definition of the list predicate. Line 5 
follows from 4, because this is an axiomatic property of all predicates -- if
a predicate holds, there is always at least one heap in which it holds. Line 6 
is a quantifier manipulation, and line 7 follows from the definition of the predicate.
\end{proof}

\begin{lemma}{(Correctness of $\sizecoll$)}
We have that 
\begin{displaymath}
\forall \alpha, c, xs, P.\; \spec{\collpred_\alpha(c, xs, P)}{\sizecoll[\alpha](c)}{a:\N}
                                  {\collpred_\alpha(c, xs, P) \land a = |xs|}
is valid.   
\end{displaymath}
\end{lemma}

\begin{proof}
This function, defined on line \ref{decl:sizecoll-impl} of Figure~\ref{iterator-implementation},
is a recursively defined function. So we will prove it using the fixed point rule. So we 
will assume an identifier $\sizecoll$ satisfying the specification above, and then prove
the correctness of the body, in annotated specification style. 

Now, assume we have $\alpha, c, xs,$ and $P$: 

\begin{specification}
\nextline $\setof{\collpred_\alpha(c, xs, P)}$ 
\nextline $\setof{\listpred(\alpha, c, xs) \land P \land \exact{P}}$ 
\nextline $\setof{\exists p.\; c \pointsto p \land \listcontentpred(\alpha, p, xs) \land P \land \exact{P}}$ 
\nextline $\letv{p}{\comp{!c}}{}$
\nextline $\setof{c \pointsto p \land \listcontentpred(\alpha, p, xs) \land P \land \exact{P}}$ 
\nextline $\Run\;\Listcase($\=$p,$ 
\nextline \> $\Nil \to $
\nextline \> \qquad \=$\setof{c \pointsto p \land p = \Nil \land xs = \epsilon \land P \land \exact{P}}$ 
\nextline \> \> \comp{0}
\nextline \> \> $\setof{c \pointsto \Nil \land p = \Nil \land xs = \epsilon \land a = 0 \land P \land \exact{P}}$ 
\nextline \> \> $\setof{\collpred_\alpha(c, xs, P) \land a = |xs|}$ 
\nextline \> $\Cons(y, c') \to $ 
\nextline \> \qquad \= $\setof{c \pointsto \Cons(y, c') * \listpred(\alpha, c', ys) \land xs = y\cdot ys) \land P \land \exact{P}}$ 
\nextline \> \> $[\letv{n}{\sizecoll[\alpha](c')}{}$
\nextline \> \> \,$\setof{c \pointsto \Cons(y, c') * \listpred(\alpha, c', ys) \land xs = y\cdot ys \land P \land \exact{P} \land n = |ys|}$ 
\nextline \> \> \,$\setof{\collpred_\alpha(c, xs, P) \land xs = y\cdot ys \land n = |ys|}$ 
\nextline \> \> \,$n+1]$
\nextline \> \> $\setof{\collpred_\alpha(c, xs, P) \land xs = y\cdot ys \land a = |ys| + 1}$ 
\nextline \> \> $\setof{\collpred_\alpha(c, xs, P) \land xs = y\cdot ys \land a = |y\cdot ys|}$ 
\nextline \> \> $\setof{\collpred_\alpha(c, xs, P) \land a = |xs|}$ 
\nextline $\setof{\collpred_\alpha(c, xs, P) \land a = |xs|}$ 
\end{specification}
\end{proof}

\begin{lemma}{(Specification of $\addcoll$)}
We have that for 
\begin{displaymath}
  \forall \alpha, x, c, xs, P.\; \spec{\collpred_\alpha(c, xs, P)}{\addcoll[\alpha](c, x)}
                                      {a:\unittype}{\exists Q.\; \collpred_\alpha(c, x\cdot xs, Q)}
\end{displaymath}
\end{lemma}

\begin{proof}
  This function, defined on line \ref{decl:addcoll-impl}, just conses on an element. 
Assume we have $\alpha, x, c, xs, P$, and then proceed with the following proof, in
annotated specification style. 

\begin{specification}
\nextline $\setof{\collpred_\alpha(c, xs, P)}$ 
\nextline $\setof{\listpred(\alpha, c, xs) \land P \land \exact{P}}$ 
\nextline $\setof{\listpred(\alpha, c, xs)}$ 
\nextline $\setof{\exists p.\; c \pointsto p * \listcontentpred(\alpha, p, xs)}$ 
\nextline $\letv{p}{\comp{!c}}{}$ 
\nextline $\setof{c \pointsto p * \listcontentpred(\alpha, p, xs)}$ 
\nextline $\letv{c'}{\comp{\newref{\Listcontent(\alpha)}{p}}}{}$
\nextline $\setof{c \pointsto p * c' \pointsto p * \listcontentpred(\alpha, p, xs)}$ 
\nextline $\setof{c \pointsto p * \listpred(\alpha, c', xs)}$ 
\nextline $c := \Cons(x, c')$ 
\nextline $\setof{c \pointsto \Cons(x, c') * \listpred(\alpha, c', xs)}$ 
\nextline $\setof{\listpred(\alpha, c, x\cdot xs)}$ 
\nextline $\setof{\exists Q.\; \listpred(\alpha, c, x\cdot xs) \land Q \land \exact{Q}}$ 
\nextline $\setof{\exists Q.\; \collpred_\alpha(c, x\cdot xs, Q)}$ 
\end{specification}
\end{proof}

\begin{lemma}
We have that 
\begin{displaymath}
  \forall \alpha, c, P.\; \spec{\collpred_\alpha(c, \epsilon, P)}
                               {\removecoll[\alpha](c)}{a:\opttype{(\alpha)}}
                               {\collpred_\alpha(c, \epsilon, P) \land a = \None}
\end{displaymath}
\end{lemma}

\begin{proof}
  This is one of the two specifications about $\removecoll$, for the case
  when the list is empty. Assume we have $\alpha, c, P$ and give an annotated
  specification as follows: 

\begin{specification}
\nextline $\setof{\collpred_\alpha(c, \epsilon, P)}$ 
\nextline $\setof{\exists p.\; c \pointsto p \land p = \Nil \land P \land \exact{P}}$ 
\nextline $\letv{p}{\comp{!c}}{}$ 
\nextline $\setof{c \pointsto p \land p = \Nil \land P \land \exact{P}}$ 
\nextline $\Run\;\Listcase($\=$p,$ 
\nextline \> $\Nil \to $\=$\comp{\None}$ 
\nextline \> \> $\setof{c \pointsto p \land p = \Nil \land P \land \exact{P} \land a = \None}$ 
\nextline \> \> $\setof{\collpred_\alpha(c, \epsilon, P) \land a = \None}$ 
\nextline \> $\Cons(y, c') \to $ 
\nextline \> \> $\setof{c \pointsto p \land p = \Nil \land p = \Cons(y, c') \land P \land \exact{P}}$ 
\nextline \> \> $\setof{\bot}$ 
\nextline \> \> $[\letv{p'}{\comp{!c'}}{}$ 
\nextline \> \> $\letv{\_}{\comp{c := p'}}{}$ 
\nextline \> \> $\Some(y)])$
\nextline \> \> $\setof{\collpred_\alpha(c, \epsilon, P) \land a = \None}$ 
\nextline $\setof{\collpred_\alpha(c, \epsilon, P) \land a = \None}$ 
\end{specification}
\end{proof}

\begin{lemma}{(Correctness of $\removecoll$, part 2)}
We have that 
\begin{displaymath}
  \forall \alpha, c, x, xs, P.\; \spec{\collpred_\alpha(c, x\cdot xs, P)}
                                      {\removecoll[\alpha](c)}
                                      {a:\opttype{(\alpha)}}
                                      {\exists Q.\; \collpred_\alpha(c, xs, Q) 
                                       \land a = \Some(x)}
\end{displaymath}
\end{lemma}

\begin{proof}
This is the other case of $\removecoll$, for when the iterator is not
yet exhausted. Assume that we have $\alpha, c, x, xs$, and $P$ of the
appropriate type. We can give an annotated-specification style proof
as follows:

\begin{specification}
\nextline $\setof{\collpred_\alpha(c, x\cdot xs, P)}$ 
\nextline $\setof{\exists p.\; c \pointsto p \land \exists c'.\; p = \Cons(x, c') * \listpred(\alpha, c', xs) \land P \land \exact{P}}$ 
\nextline $\letv{p}{\comp{!c}}{}$ 
\nextline $\setof{c \pointsto p \land \exists c'.\; p = \Cons(x, c') * \listpred(\alpha, c', xs) \land P \land \exact{P}}$ 
\nextline $\Run\;\Listcase($\=$p,$ 
\nextline \> $\Nil \to $\= $\setof{p = \Nil \land c \pointsto p \land \exists c'.\; p = \Cons(x, c') * \listpred(\alpha, c', xs) \land P \land \exact{P}}$ 
\nextline \> \> $\setof{\bot}$
\nextline \> \> $\comp{\None}$ 
\nextline \> \> $\setof{\exists Q.\; \collpred_\alpha(c, xs, Q) \land a = \Some(x)}$
\nextline \> $\Cons(y, c') \to $ 
\nextline \> \> $\setof{p = \Cons(y, c') \land c \pointsto p \land \exists c'.\; p = \Cons(x, c') * \listpred(\alpha, c', xs) \land P \land \exact{P}}$ 
\nextline \> \> $\setof{x = y \land c \pointsto p \land p = \Cons(x, c') * \listpred(\alpha, c', xs) \land P \land \exact{P}}$ 
\nextline \> \> $\setof{x = y \land c \pointsto p \land p = \Cons(x, c') * \listpred(\alpha, c', xs)}$
\nextline \> \> $\setof{x = y \land c \pointsto p \land p = \Cons(x, c') * \exists p'.\; c' \pointsto p' * \listcontentpred(\alpha, p', xs)}$
\nextline \> \> $[\letv{p'}{\comp{!c'}}{}$ 
\nextline \> \> $\setof{x = y \land c \pointsto p \land p = \Cons(x, c') * c' \pointsto p' * \listcontentpred(\alpha, p', xs)}$
\nextline \> \> $\letv{\_}{\comp{c := p'}}{}$ 
\nextline \> \> $\setof{x = y \land c \pointsto p' * c' \pointsto p' * \listcontentpred(\alpha, p', xs)}$
\nextline \> \> $\setof{x = y \land c' \pointsto p' * \listpred(\alpha, c, xs)}$
\nextline \> \> $\setof{x = y \land \listpred(\alpha, c, xs)}$
\nextline \> \> $\Some(y)])$
\nextline \> \> $\setof{\listpred(\alpha, c, xs) \land a = \Some(y) \land x = y}$ 
\nextline \> \> $\setof{\listpred(\alpha,c, xs) \land a = \Some(x)}$
\nextline \> \> $\setof{(\exists Q.\; \listpred(\alpha,c, xs) \land Q \land \exact{Q}) \land a = \Some(x)}$
\nextline \> \> $\setof{\exists Q.\; \collpred_\alpha(c, xs, Q) \land a = \Some(x)}$
\nextline $\setof{\exists Q.\; \collpred_\alpha(c, xs, Q) \land a = \Some(x)}$
\end{specification}
\end{proof}

\begin{lemma}{(Correctness of $\newiter$)}
We have that
\begin{displaymath}
\forall \alpha, c, xs, P.\; \spec{\collpred_\alpha(c, xs, P)}
                                 {\newiter[\alpha](c)}
                                 {a:\itertype(\alpha)}
                                 {\collpred_\alpha(c, xs, P) * 
                                  \iterpred_\alpha(a, xs, \setof{P})}
\end{displaymath}
is valid.
\end{lemma}

\begin{proof}
The definition of $\newiter$ is given on line \ref{decl:newiter-impl} of
Figure~\ref{iterator-implementation}. To prove the correctness of this
implementation, we assume we have $\alpha, c, xs$, and $P$, and then
give the following proof: 

\begin{specification}
\nextline $\setof{\collpred_\alpha(c, xs, P)}$ 
\nextline $\setof{\listpred(\alpha, c, xs, P) \land P \land \exact{P}}$ 
\nextline $\letv{i}{\comp{\newref{\colltype(\alpha)}{c}}}{}$ 
\nextline $\setof{\listpred(\alpha, c, xs) \land P \land \exact{P} * i \pointsto c}$ 
\nextline $\setof{\listpred(\alpha, c, xs) \land P \land \exact{P} * i \pointsto c * \emp}$ 
\nextline $\setof{\listpred(\alpha, c, xs) \land P \land \exact{P} * i \pointsto c * (P \wand P)}$
\nextline $\setof{\listpred(\alpha, c, xs) \land P \land \exact{P} * i \pointsto c * P \wand (P \land \listpred(\alpha, c, xs))}$ 
\nextline $\One(i)$ 
\nextline $\setof{\listpred(\alpha, c, xs) \land P \land \exact{P} * i \pointsto c * P \wand (P \land \listpred(\alpha, c, xs)) \land a = \One(i)}$ 
\nextline $\setof{\listpred(\alpha, c, xs) \land P \land \exact{P} * \iterpred_\alpha(a, xs, \setof{P})}$
\nextline $\setof{\collpred_\alpha(c, xs, P) * \iterpred_\alpha(a, xs, \setof{P})}$
\end{specification}

The key steps in this proof are from lines 5 to 7. On line 5, we introduce an extra $\emp$, which
entails $P \wand P$ on line 6. Then, because $P$ is strict exact, we know that for any $Q$, either $P \land Q = \bot$ or $P \land Q = P$. This justifies changing the formula to $P \wand (P \land \listpred(\alpha, c, xs))$ in line 7. If $P \land \listpred(\alpha, c, xs)$ is false, then the whole formula is false, since it occurs as part of the whole assertion. Otherwise, it's an equality, and we can rewrite the conclusion of the magic wand without changing the meaning of the assertion.
\end{proof}

\begin{lemma}{(Correctness of $\filteriter$)}
We have that
\begin{displaymath}
  \forall \alpha, p, i, xs, S.\; \spec{\iterpred_\alpha(i, xs, S)}
                                      {\filteriter[\alpha](i)}
                                      {a:\itertype(\alpha)}
                                      {\iterpred_\alpha(a, \filtermath\;p\;xs, S)}
\end{displaymath}
\end{lemma}

\begin{proof}
The definition of this procedure is given on line \ref{decl:filteriter-impl}. Assume
$\alpha, p, i, xs$, and $S$. 

\begin{specification}
\nextline $\setof{\iterpred_\alpha(i, xs, S)}$ 
\nextline $\Filter(p, i)$ 
\nextline $\setof{\iterpred_\alpha(i, xs, S) \land a = \Filter(p, i)}$ 
\nextline $\setof{\iterpred_\alpha(i, xs, S) \land \filtermath\;p\;xs = \filtermath\;p\;xs \land a = \Filter(p, i)}$ 
\nextline $\setof{\exists i, xs.\; \iterpred_\alpha(i, xs, S) \land \filtermath\;p\;xs = \filtermath\;p\;xs \land a = \Filter(p, i)}$
\nextline $\setof{\iterpred_\alpha(a, \filtermath\;p\;xs, S)}$ 
\end{specification}
\end{proof}

\begin{lemma}{(Correctness of $\mergeiter$)}
We have that 
\begin{displaymath}
  \forall \alpha, f, i, xs, S, i', xs', S'.\; 
  \begin{array}{l}
    \setof{\iterpred_\alpha(i, xs, S) * \iterpred_\alpha(i',xs',S')} \\
    \mergeiter[\alpha](f, i, i') \\
    \setof{a:\itertype(\alpha).\; \iterpred_\alpha(a, \mergemath\;f\;xs\;xs', S \cup S')}\\
  \end{array}
\end{displaymath}
\end{lemma}

\begin{proof}
The definition of $\mergeiter$ is given on line \ref{decl:mergeiter-impl} of
Figure~\ref{iterator-implementation}. Assume $f, i, xs, S, i', xs', S'$ as 
hypotheses, and proceed with the proof as follows: 

\begin{specification}
\nextline $\setof{\iterpred_\alpha(i, xs, S) * \iterpred_\alpha(i',xs',S')}$ 
\nextline $\Merge(f, i, i')$
\nextline $\setof{\iterpred_\alpha(i, xs, S) * \iterpred_\alpha(i',xs',S') \land a = \Merge(f, i', i')}$ 
\nextline $\{$\=$\iterpred_\alpha(i, xs, S) * \iterpred_\alpha(i',xs',S') \land S \cup S' = S \cup S' \land \mergemath\;f\;xs\;xs' = \mergemath\;f\;xs\;xs'$
\nextline \> $\land a = \Merge(f, i', i')\}$ 
\nextline $\{$\=$\exists i, i', S, S'.\; \iterpred_\alpha(i, xs, S) * \iterpred_\alpha(i',xs',S') \land S \cup S' = S \cup S' \land \mergemath\;f\;xs\;xs' = \mergemath\;f\;xs\;xs'$ 
\nextline \> $\land a = \Merge(f, i', i')\}$ 
\nextline $\setof{\iterpred_\alpha(a, \mergemath\;f\;xs\;xs', S \cup S')}$
\end{specification}
\end{proof}

\begin{lemma}{(Correctness of $\nextiter$)}
We have that 
\begin{displaymath}
  \forall \alpha, i, C, S, xs.\; 
  \begin{array}{l}
    \setof{\iterpred_\alpha(i, xs, S) * \mathit{colls}(C,S)} \\
    \nextiter[\alpha](i) \\
    \{a:\opttype{(\alpha)}.\; \\
      \qquad [(a = \None \land xs = \epsilon \land \iterpred_\alpha(i, xs, S)) \vee \\
      \qquad (\exists y, ys.\; a = \Some(y) \land xs = y\cdot ys \land \iterpred_\alpha(i, ys, S))]
       \\
      \qquad * \mathit{colls}(C,S)\} \\
  \end{array}
\end{displaymath}
\end{lemma}

\begin{proof}
This function is defined on lines \ref{decl:next-one-impl} to \ref{decl:next-impl-end} 
of Figure~\ref{iterator-implementation}. To prove this, we will use fixed point induction,
and assume that the specification above holds for an identifier named $\nextiter$, and
use it to prove the correctness for the body of the function. 

Assuming $\alpha, i, C, S, xs$, this proof will proceed by cases, on
the structure of $i$. Then we can exploit the fact that we can unroll fixed
points and beta-reduce expression to avoid proving the impossible branches (which
in typical Hoare logic proofs are ruled out by getting false as a precondition). 

\begin{itemize}
\item Suppose $i = \One(i')$. Then, we proceed with an annotated proof as 
follows: 
\begin{specification}
\nextline $\setof{\iterpred_\alpha(\One(i'), xs, S) * \mathit{colls}(C,S)}$ 
\nextline $\setof{\exists P.\; \iterpred_\alpha(\One(i'), xs, \setof{P}) * \mathit{colls}_\alpha(C,\setof{P})}$ 
\nextline $\setof{\exists P.\; \iterpred_\alpha(\One(i'), xs, \setof{P}) * \collpred_\alpha(c, ys, P)}$
\nextline $\setof{\exists c'.\; i' \pointsto c' * (P \wand (P \land (\top * \listpred(\alpha, c', xs)))) * \listpred(\alpha,c, ys) \land P \land \exact{P}}$ 
\nextline $\setof{\exists c'. i' \pointsto c' * P \land \listpred(\alpha, c, ys) \land (\top * \listpred(\alpha, c', xs)) \land \exact{P}}$ 
\nextline $\letv{c'}{\comp{!i'}}{}$ 
\nextline $\setof{i' \pointsto c' * P \land \listpred(\alpha, c, ys) \land (\top * \listpred(\alpha, c', xs)) \land \exact{P}}$ 
\nextline $\setof{i' \pointsto c' * P \land \listpred(\alpha, c, ys) \land (\top * \exists v.\; c' \pointsto v * \listcontentpred(\alpha, v, xs)) \land \exact{P}}$ 
\nextline $\letv{v}{\comp{!c'}}{}$ 
\nextline $\setof{i' \pointsto c' * P \land \listpred(\alpha, c, ys) \land (\top * c' \pointsto v * \listcontentpred(\alpha, v, xs)) \land \exact{P}}$ 
\nextline $\Run\;$\=$\Listcase(v,$ 
\nextline \> $\Nil $\=$\to$ 
\nextline \> \> Framing $\exact{P}$
\nextline \> \> $\setof{v = \Nil \land i' \pointsto c' * P \land \listpred(\alpha, c, ys) \land (\top * c' \pointsto v * \listcontentpred(\alpha, v, xs))}$ 
\nextline \> \> $\setof{i' \pointsto c' * P \land \listpred(\alpha, c, ys) \land (\top * c' \pointsto \Nil * xs = \epsilon}$ 
\nextline \> \> $\comp{\None}$ 
\nextline \> \> $\setof{a = \None \land i' \pointsto c' * P \land \listpred(\alpha, c, ys) \land (\top * c' \pointsto \Nil * xs = \epsilon}$ 
\nextline \> \> $\setof{a = \None \land ys = \epsilon \land i' \pointsto c' * P \land \listpred(\alpha, c, ys)  \land (\top * \listpred(\alpha, c', xs)) }$ 
\nextline \> \> Restoring frame $\exact{P}$ 
\nextline \> \> $\setof{a = \None \land xs = \epsilon \land i' \pointsto c' * P \land \listpred(\alpha, c, ys) \land (\top * \listpred(\alpha, c', xs)) \land \exact{P}}$ 
\nextline \> \> $\setof{a = \None \land xs = \epsilon \land \iterpred_\alpha(i, xs, \setof{P}) * colls(C, \setof{P})}$
\nextline \> $\Cons(z, c'') \to$ 
\nextline \> \> Framing $\exact{P}$
\nextline \> \> $\setof{v = \Cons(z, c'') \land i' \pointsto c' * P \land \listpred(\alpha,c,ys) \land (\top * c' \pointsto v * \listcontentpred(\alpha, v, xs))}$  
\nextline \> \> $\setof{i' \pointsto c' * P \land \listpred(\alpha,c,ys) \land (\top * c' \pointsto \Cons(z,c'') * \listcontentpred(\alpha, \Cons(z,c''), xs))}$  
\nextline \> \> $\setof{i' \pointsto c' * P \land \listpred(\alpha,c,ys) \land (\top * c' \pointsto \Cons(z,c'') * \exists zs.\; xs = z\cdot zs \land \listpred(\alpha, c'', xs))}$  
\nextline \> \> $\setof{i' \pointsto c' * P \land \listpred(\alpha,c,ys) \land (\top * c' \pointsto \Cons(z,c'') * xs = z\cdot zs \land \listpred(\alpha, c'', xs))}$  
\nextline \> \> $[\letv{\_}{i' := c''}{}$ 
\nextline \> \> $\setof{i' \pointsto c'' * P \land \listpred(\alpha,c,ys) \land (\top * c' \pointsto \Cons(z,c'') * xs = z\cdot zs \land \listpred(\alpha, c'', xs))}$  
\nextline \> \> $\setof{i' \pointsto c'' * P \land \listpred(\alpha,c,ys) \land (\top * xs = z\cdot zs \land \listpred(\alpha, c'', xs))}$  
\nextline \> \> $\setof{xs = z\cdot zs \land i' \pointsto c'' * P \land \listpred(\alpha,c,ys) \land (\top * \listpred(\alpha, c'', xs))}$  
\nextline \> \> Restoring frame $\exact{P}$ 
\nextline \> \> $\setof{xs = z\cdot zs \land i' \pointsto c'' * P \land \listpred(\alpha,c,ys) \land (\top * \listpred(\alpha, c'', xs)) \land \exact{P}}$  
\nextline \> \> $\setof{xs = z\cdot zs \land i' \pointsto c'' * (P \land \listpred(\alpha,c,ys) \land \exact{P}) * (P \wand (P \land (\top * \listpred(\alpha, c'', xs))))}$  
\nextline \> \> $\setof{xs = z\cdot zs \land \collpred_\alpha(c, zs, P) * \iterpred_\alpha(\One(i'),zs,\setof{P})}$  
\nextline \> \> $\Some(z)]$ 
\nextline \> \> $\setof{a. \exists z, zs.\; xs = z\cdot zs \land a = \Some(z) \land \collpred_\alpha(c, zs, P) * \iterpred_\alpha(\One(i'),zs,\setof{P})}$  
\end{specification}

\item Suppose $i = \Filter(p, i')$. Then, we proceed with an annotated proof as 
follows: 
\begin{specification}
\nextline $\setof{\iterpred_\alpha(\Filter(p, i'), xs, S) * \mathit{colls}(C,S)}$ 
\nextline $\setof{\exists ys. \iterpred_\alpha(i', ys, S) \land 
                              xs = \mathit{filter}\;p\;ys * 
                              \mathit{colls}(C,S)}$   
\nextline $\setof{xs = \mathit{filter}\;p\;ys \land
                  \iterpred_\alpha(i', ys, S) *
                  \mathit{colls}(C,S)}$
\nextline $[\letv{v}{\nextiter[\alpha](i')}{}$ 
\nextline \{\=$[(ys = \epsilon \land v = \None) \vee 
                (\exists z, zs.\; ys = z\cdot zs \land v = \Some(z))] \land
               xs = \mathit{filter}\;p\;ys \;\land$ 
\contline $\iterpred_\alpha(i', ys, S) * \mathit{colls}(C,S)$\}
\nextline \;$\Run\;\Listcase($\=$v,$ 
\nextline \> $\None$\=$ \to [\None]$ 
\nextline \> \{$a = \None \land 
               (ys = \epsilon \land v = \None) \land 
                xs = \mathit{filter}\;p\;ys \;\land
                \iterpred_\alpha(i', ys, S) * \mathit{colls}(C,S)$\}
\nextline \> \{$a = \None \land xs = \epsilon \land
                xs = \mathit{filter}\;p\;ys \;\land
                \iterpred_\alpha(i', ys, S) * \mathit{colls}(C,S)$\}
\nextline \> \{$a = \None \land xs = \epsilon \land
                \exists ys.\;
                xs = \mathit{filter}\;p\;ys \;\land
                \iterpred_\alpha(i', ys, S) * \mathit{colls}(C,S)$\}
\nextline \> \{$a = \None \land xs = \epsilon \land
                \iterpred_\alpha(\Filter(p, i'), xs, S) * \mathit{colls}(C,S)$\}
\nextline \> $\Some(z)$\=$\to$
\nextline \> \;\;\{\=$\exists zs.\; ys = z\cdot zs \land v = \Some(z) \land
                  xs = \mathit{filter}\;p\;(z\cdot zs) \;\land$ 
\\ \> \> \>      $\iterpred_\alpha(i', zs, S) * \mathit{colls}(C,S)$\}
\nextline \> \;\;\{\=$ys = z\cdot zs \land v = \Some(z) \land
                   xs = \mathit{filter}\;p\;(z\cdot zs) \;\land$ 
\\ \> \> \>      $\iterpred_\alpha(i', zs, S) * \mathit{colls}(C,S)$\}
\nextline \> \;\;$\ctext{if}($\=$p\;z,$ 
\nextline \> \> \{\=$p\;z = \True \land ys = z\cdot zs \land v =\Some(z) \land
                   xs = \mathit{filter}\;p\;(z\cdot zs) \;\land$ 
\\ \> \> \>      $\iterpred_\alpha(i', zs, S) * \mathit{colls}(C,S)$\}
\nextline \> \> \{\=$ys = z\cdot zs \land v =\Some(z) \land
                   xs = z\cdot(\mathit{filter}\;p\;zs) \;\land$ 
\\ \> \> \>      $\iterpred_\alpha(i', zs, S) * \mathit{colls}(C,S)$\}
\nextline \> \> \{\=$v =\Some(z) \land
                   xs = z\cdot(\mathit{filter}\;p\;zs) \;\land
                   \iterpred_\alpha(i', zs, S) * \mathit{colls}(C,S)$\}
\nextline \> \> \{\=$v =\Some(z) \land
                   xs = z\cdot(\mathit{filter}\;p\;zs) \;\land
                   \mathit{filter}\;p\;zs = \mathit{filter}\;p\;zs \;\land
                   \iterpred_\alpha(i', zs, S) * \mathit{colls}(C,S)$\}
\nextline \> \> \{\=$v =\Some(z) \land
                   xs = z\cdot(\mathit{filter}\;p\;zs) \;\land
                   \iterpred_\alpha(i', \mathit{filter}\;p\;zs, S) * \mathit{colls}(C,S)$\}
\nextline \> \> \{\=$\exists z', zs'.\; v =\Some(z') \land
                     xs = z'\cdot zs' \land
                   \iterpred_\alpha(i', zs', S) * \mathit{colls}(C,S)$\}
\nextline \> \> $[v],$
\nextline \> \> \{\=$\exists z', zs'.\; a = \Some(z') \land
                     xs = z'\cdot zs' \land
                   \iterpred_\alpha(i', zs', S) * \mathit{colls}(C,S)$\}
\\ \> \> \> (Now the else-branch)
\nextline \> \> \{\=$p\;z = \False \land ys = z\cdot zs \land a = \Some(z) \land
                   xs = \mathit{filter}\;p\;(z\cdot zs) \;\land$ 
\\ \> \> \>      $\iterpred_\alpha(i', zs, S) * \mathit{colls}(C,S)$\}
\nextline \> \> \{\=$xs = \mathit{filter}\;p\; zs \;\land
                     \iterpred_\alpha(i', zs, S) * \mathit{colls}(C,S)$\}
\nextline \> \> \{\=$\iterpred_\alpha(\Filter(p, i'), xs, S) * \mathit{colls}(C,S)$\}
\nextline \> \> $\nextiter[\alpha](\Filter(p, i'))$ 
\nextline \> \> \{\=$(xs = \epsilon \land a = \None \land \iterpred_\alpha(\Filter(p, i'), xs, S) * \mathit{colls}(C,S)) \;\vee$ 
\contline \> \>  $(\exists z, zs. xs = z\cdot zs \land a = \Some(Z) \land
                    \iterpred_\alpha(\Filter(p, i'), zs, S) * \mathit{colls}(C,S))$\}   
\end{specification}

Note that this is not a structural induction on $i$; the
$\Filter(p,i)$ case has a non-structural recursive call. Here, we make
use of fixed-point induction in our proof of $\nextiter$. 



\end{itemize}

\end{proof}


$\nextiter$ (lines 20-31) recursively walks down the structure of
the iterator tree, and combines the results from the leaves upwards.
The base case is the $\ctext{Coll }r$ case (lines 20-23). The iterator
pointer is doubly-dereferenced, and then the contents examined. If the
end of the list has been reached and the contents are $\Nil$,
then $\None$ is returned to indicate there are no more
elements. Otherwise, the pointer $r$ is advanced, and the head
returned as the observed value. The $\ctext{Filter(p,i)}$ case (lines
24-26) will return $\None$ if $i$ is exhausted, and if it is
not, it will pull elements from $i$ until it finds one that satisfies
$p$, calling itself recursively until it succeeds or $i$ is exhausted.
Finally, in the $\ctext{Map2}(f, i_1, i_2)$ case (lines 27-31),
$\nextiter$ will draw a value from both $i_1$ and $i_2$, and will
return $\None$ if either is exhausted, and otherwise it will
return $f$ applied to the pair of values.

Below, we give an example use of this module in annotated program
style: 

\begin{tabbing}
1 \qquad \= $\setof{\emp}$ \\
2 \> $\letv{c_1}{\newcoll()}{}$ \\
3 \> $\setof{\exists P'_1.\; coll(c_1, \epsilon, P'_1)}$ \\
4 \> $\setof{coll(c_1, \epsilon, P_1)}$ \\
5 \> $\letv{()}{\sizecoll(c_1, 4)}{}$ \\
6 \> $\setof{\exists P'_2.\; coll(c_1, 4\cdot\epsilon, P'_2)}$ \\
7 \> $\setof{coll(c_1, 4\cdot\epsilon, P_2)}$ \\
8 \> $\letv{()}{\sizecoll(c_1, 3)}{}$ \\
9 \> $\letv{()}{\sizecoll(c_1, 2)}{}$ \\
10 \> $\setof{coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)}$ \\
11 \> $\letv{c_2}{\newcoll()}{}$ \\
12 \> $\letv{()}{\sizecoll(c_2, 3)}{}$ \\
13 \> $\letv{()}{\sizecoll(c_2, 5)}{}$ \\
14\> $\setof{coll(c_2, 5\cdot3\cdot\epsilon, Q_2) * coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)}$ \\
15\> $\letv{i_1}{\newiter(c_1)}{}$ \\
16\> $\{$\=$iter(i_1, \setof{(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)}, 2\cdot3\cdot4\cdot\epsilon)$
\\ 
  \>\>$*\; coll(c_2, 5\cdot3\cdot\epsilon, Q_2) * coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)\}$ \\
17 \> $\letv{i'_1}{\filteriter(even?, i_1)}{}$ \\
18 \> $\{$\=$iter(i'_1, \setof{(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)}, 2\cdot4\cdot\epsilon)$ \\
   \>\>$*\; coll(c_2, 5\cdot3\cdot\epsilon, Q_2) * coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)\}$\\
19 \> $\letv{i_2}{\newiter(c_2)}{}$ \\
20 \> $\{$\=$iter(i'_1, \setof{(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)}, 2\cdot4\cdot\epsilon)$\\
  \>\>$*\;iter(i_2, \setof{(c_2, 5\cdot3\cdot\epsilon, Q_2)}, 5\cdot3\cdot\epsilon)$ \\
  \>\>$*\;coll(c_2, 5\cdot3\cdot\epsilon, Q_2) * coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)\}$ \\
21 \> $\letv{i}{\ctext{map2}(plus, i'_1, i_2)}{}$ \\
22 \> $\{$\=$iter(i, \setof{(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4), (c_2, 5\cdot3\cdot\epsilon, Q_2)}, 7\cdot7\cdot\epsilon)$ \\
\> \> $*\; coll(c_2, 5\cdot3\cdot\epsilon, Q_2) * coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)\}$ \\
23 \> $\letv{n}{\sizecoll(c_2)}{}$ \\
24 \> $\{$\=$n = 2 \;\land
iter(i, \setof{(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4), (c_2, 5\cdot3\cdot\epsilon, Q_2)}, 7\cdot7\cdot\epsilon)$\\
\>\>$*\;coll(c_2, 5\cdot3\cdot\epsilon, Q_2) * coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)\}$ \\

25 \> $\letv{x}{\nextiter(i)}{}$ \\
26 \> $\{$\= $n = 2 \land x = \ctext{Some }7 \;\land $\\
\> \> $iter(i, \setof{(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4), (c_2, 5\cdot3\cdot\epsilon, Q_2)}, 7\cdot\epsilon)$ \\
\> \> $*\; coll(c_2, 5\cdot3\cdot\epsilon, Q_2) * coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)\}$ \\
27 \> $\sizecoll(c_2, 17)$ \\
28 \> $\{$\= $n = 2 \land x = \ctext{Some }7 \;\land $\\
\> \> $iter(i, \setof{(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4), (c_2, 5\cdot3\cdot\epsilon, Q_2)}, 7\cdot\epsilon)$ \\ 
\> \> $* \; (\exists Q_3.\; coll(c_2, 17\cdot5\cdot3\cdot\epsilon, Q_3)) * coll(c_1, 2\cdot3\cdot4\cdot\epsilon, P_4)\}$ \\
\end{tabbing}


In line 1 of this example, we begin in an empty heap. In line 2, we
create a new collection $c_1$, which yields us the state $\exists
P'_1.\; coll(c_1, \epsilon, P'_1)$, with an existentially quantified
abstract state.

Because $P'_1$ is existentially quantified, we do not know what value
it actually takes on. However, if we prove the rest of the program
using a freshly-introduced variable $P_1$, then we know that the rest
of the program will work for \emph{any} value of $P_1$, because free
variables are implicitly universally quantified.  So it will work with
whatever value $P'_1$ had. So we drop the quantifier on line 4, and
try to prove this program with the universally-quantified
$P_1$.\footnote{A useful analogy is the existential elimination rule
  in the polymorphic lambda calculus: we prove that we can use an
  existential by showing that our program is well-typed no matter what
  the contents of the existential are.}

This permits us to $\sizecoll$ the element 4 to $c_1$ on line 5. Its
specification puts the predicate $coll()$ on line 6 again into an
existentially quantified state $P'_2$. So we again replace $P'_2$ with
a fresh variable $P_2$ on line 7, and will elide these existential
introductions and unpackings henceforth.

In lines 8-9, we add two more elements to $c_1$, and on lines 11-13,
we create another collection $c_2$, and add $3$ and $5$ to it, as can
be seen in the state predicate on line 14. On line 15, we create the
iterator $i_1$ on the collection $c_1$. The $iter$ predicate on line
16 names $i_1$ as its value, and lists $c_1$ in state $P_4$ as its
support, and promises to enumerate the elements 2, 3, and 4.

On line 17, $\filteriter(even?, i_1)$ creates the new iterator
$i'_1$. This iterator yields only the even elements of $i_1$, and so
will only yield 2 and 4. On line 18, $i_1$'s iterator state has been
consumed to make $i'_1$'s state. We can no longer call
$\nextiter(i_1)$, since we do not have the resource invariant
needed to prove anything about that call. Thus, we cannot write a
program that would break $i'_1$'s representation invariant.

On line 19, we create a third iterator $i_2$ enumerating the elements
of $c_2$. The state on line 20 now has predicates for $i'_1$, $i_2$,
$c_1$ and $c_2$. On line 21, $\ctext{map2}(plus, i'_1, i_2)$ creates a
new iterator $i$, which produces the pairwise sum of the elements of
$i'_1$ and $i_2$, and consumes the iterator states for $i'_1$ and
$i_2$ to yield the state for the new iterator $i$. Note that the 
invariant for $i$ does not make any mention of what it was constructed
from, naming only the collections it needs as support. 
%Furthermore, the support of $i$ is the union of the
%supports of $i'_1$ and $i_2$ -- namely, the two collections $c_1$ and
%$c_2$.

On line 23, the $\sizecoll$ call on $c_2$ illustrates that we can
call non-destructive methods while iterators are active. The call to
$\nextiter(i)$ on line 24 binds $\ctext{Some }7$ to $x$, and the
the iterator's sequence argument (line 27) shrinks by one element. On
line 28, we call $\sizecoll(c_2, 17)$ the state of $c_2$ changes to
$\exists Q_3.\; coll(c, 17\cdot 5 \cdot 3\cdot\epsilon, Q_3)$ (line
27). So we can no longer call $\nextiter(i)$, since it needs $c_2$
to be in the state $Q_2$.

\textbf{Discussion.} This example shows a pleasant synergy between
higher-order quantification and separation logic. We can give a
relatively simple specification to the clients of the collection
library, even though the internal invariant is quite subtle (as the
use of the magic wand suggests). Higher-order logic also lets us
freely define new data types, and so our specifications can take
advantage of the pure, non-imperative nature of the mathematical
world, as can be seen in the specifications of the $\filteriter$
and $\ctext{map2}$ functions -- we can use equational reasoning on
purely functional lists in our specifications, even though our
algorithms are imperative.

\section{The Flyweight and Factory Patterns}

The flyweight pattern is a style of cached object creation. Whenever a
constructor method is called, it first consults a table to see if an
object corresponding to those arguments has been created. If it has,
then the preexisting object is returned.  Otherwise, it allocates a
new object, and updates the table to ensure that future calls with the
same arguments will return this object. Because objects are re-used,
they become pervasively aliased, and must be used in an immutable
style to avoid surprising updates. (Functional programmers call this
style of value creation ``hash-consing''.)

This is an interesting design pattern to verify, for two reasons.
First, the constructor has a memo table to cache the result of
constructor calls, which needs to be hidden from clients. Second, this
pattern makes pervasive use of aliasing, in a programmer-visible
way. In particular, programmers can test two references for identity
in order to establish whether two values are equal or not. This allows
constant-time equality testing, and is a common reason for using this
pattern. Therefore, our specification has to be able to justify this
reasoning.

Below, we specify a program that uses the flyweight pattern to create
and access glyphs (i.e., refs of pairs of characters and fonts) of a
particular font $f$. We have a function $\ctext{newglyph}$ to create
new glyphs, which does the caching described above, using a predicate
variable $I$ to refer to the table invariant; and a function
$\ctext{getdata}$ to get the character and font information from a
glyph.

Furthermore, these functions will be created by a call to another
function, $\ctext{make\_flyweight}$, which receives a font as an
argument and will return appropriate $\ctext{newglyph}$ and
$\ctext{getdata}$ functions. 

\begin{tabbing}
Flyweight$($\=$I : \assert,\;\; $\\
\> $\ctext{newglyph} : \chartp \to \monad{\ctext{glyph}},$ \\
\> $\ctext{getdata} : \ctext{glyph} \to \monad{(\chartp \times \fonttp)},$ \\
\> $f:\fonttp) \equiv$ \\
1 \qquad \=$\exists $\=$glyph : \ctext{glyph} \times \chartp \times \fonttp \To \assert.$ 
\\[0.5em]

2  \> \> $\forall c, S.\;$\=
         $\{I \land chars(S)\}$ \\
   \>\>\>$\run{\ctext{newglyph}(c)}$ \\
   \>\>\>$\setof{a:\ctext{glyph}.\; 
                 I \land chars(\setof{(a, (c,f))} \cup S)}$ \\
  \> \!$\specand$ \\
3 \> $\forall l, c, f, P.\;$\=
     $\setof{glyph(l, c, f) \land P}$ \\
\>\> $\run{\ctext{getdata}(l)}$ \\
\>\> $\setof{a:\chartp \times \fonttp.\; glyph(l, c, f) \land P \land a = (c,f)}$
\\
  \> \!$\specand$ \\
4 \> $\{\forall l, l', c, c'.\;$\=$I \land glyph(l,c,f) \land glyph(l',c',f')
 \implies $ \\
\>\>  $\;\;\;\left(l = l' \iff (c = c'\land f=f')\right)\}$ \\[0.5em]

$chars(\emptyset)$ \qquad\qquad\qquad \;\;\= $\equiv$ \= $\top$ \\
$chars(\setof{(l,(c,f))} \cup S)$ \> $\equiv$ \> $glyph(l,c,f) \land chars(S)$ \\
\end{tabbing}


In the opening , we informally parametrize our specification over the
predicate variable $I$, the function variable $\ctext{newglyph}$, the
function variable $\ctext{getdata}$, and the variable $f$ of $\fonttp$
type. The reason we do this instead of existentially quantifying over
them will become clear shortly, once we see the factory function that
creates flyweight constructors.

On line 1, we assert the existence of a three-place predicate
$glyph(l, c, f)$, which is read as saying the glyph value $l$ is a
glyph of character $c$ and font $f$. 

On line 2, we specify the $\ctext{newglyph}$ procedure. Its
precondition says the pre-state must be the private flyweight state
$I$, and that this state overlaps with the character state for the
glyph/data pairs in $S$. The definition of $chars$ takes a set of
glyph/data pairs and produces the conjunction of $glyph(l,c,f)$ for
all the $(l,(c,f)) \in S$. Running $\ctext{newglyph}(c)$ will yield a
postcondition state in which $(a, (c,f))$ is added to the set $S$ --
that is, the postcondition state is $I \land chars(S) \land
glyph(a,c,f)$.

The intuition for this specification is that $I$ represents the
private state of the memo table. We use an ordinary conjunction
instead of the separating conjunction in the definition of $chars$ to
say that the different glyphs may alias with each other, and with the
private state $I$. In other words, even though the $\ctext{newglyph}$
function returns a new glyph value, the ownership of the state
associated with that value is not transferred -- it remains with the
constructor. All the client can know is that some glyph state exists
for the value it created, and that the glyph state is owned by the
constructor.

On line 3, we specify the $\ctext{getdata}$ function. If the predicate
$glyph(r, c, f)$ is in the precondition, then $\ctext{getdata}(r)$
will return $(c, f)$.  To enforce the flyweight invariant that the
glyph objects are read-only, we conjoin the pre- and post-conditions
with an arbitrary predicate variable $P$. Since $P$ must be preserved
for any instantiation, we know that $\ctext{getdata}$ cannot make any
changes to the underlying data. (It might be possible to internalize
this style of argument into the logic via some sort of relational
parametricity proof. However, we have not yet done so.)

Finally, on lines 4, we give an axiom about the interaction of $I$ and
$glyph(l,c,f)$, which says that if we know that $I \land glyph(l,c,f)
\land glyph(l',c', f')$ holds, then $l = l'$ holds if and only $c =
c'$ and $f = f'$. This axiom gives clients the ability to take
advantage of the fact that we are caching object creation and conclude
that two calls to $\ctext{newglyph}$ with the same arguments will
yield the same result. 

Requiring an axiom of separation hold of the abstract predicates is
how we state reasoning principles about aliasing as part of the
interface of a module. When we verify the implementation, we will need
to give concrete definitions of $I$ and $glyph$, and show that the
formula is actually a tautology of separation logic.

The specification of the flyweight factory looks like this:


\begin{tabbing}
1 \qquad \= $\exists \ctext{make\_flyweight} :
\fonttp \to \bigcirc($\=$(\chartp \to \monad{\ctext{glyph}}) \times$ \\
\> \> $(\ctext{glyph} \to \monad{(\chartp \times \fonttp)})).$\\
2 \> \;\;\= $\forall f.\;$\=$\setof{\emp}$ \\
  \>\> \> $\run{\ctext{make\_flyweight}(f)}$ \\
  \>\> \> $\setof{a.\; \exists I:\assert.\; I \land \validprop{\mbox{Flyweight}(I, \fst{a}, \snd{a}, f)}}$ \\
\end{tabbing}


Here, we assert the existence of a function $\ctext{make\_flyweight}$,
which takes a font $f$ as an input argument, and returns two functions
to serve as the $\ctext{getchar}$ and $\ctext{getdata}$ functions of
the flyweight. In the postcondition, we assert the existence of some
private state $I$, which contains the table used to cache glyph
creations. 

This pattern commonly arises when encoding aggressively
object-oriented designs in a higher-order style --- we call a
function, which creates a hidden state, and returns other procedures
which are the only way to access that state. This style of
specification resembles the existential encodings of objects into type
theory. The difference is that instead of making the fields of an
object an existentially quantified \emph{value}~\cite{pierce-turner}, we
make use of existentially-quantified \emph{state}.

Below, we define $\ctext{make\_flyweight}$ and its predicates:


\begin{tabbing}
1 \qquad \= $\ctext{m}$\=$\ctext{ake\_flyweight} \equiv$ \\
   \> \>     $\lambda f:$\=$\fonttp.\;$\\
2  \> \> $[$\=$\letv{t}{\ctext{newtable}()}{}$ \\
3  \> \> \> $\ctext{letv }$\=$newglyph =$ \\
4  \> \> \> \> \!\!$[\lambda c.[$\=$\ctext{letv }x = \ctext{lookup}(t, c)\ctext{ in}$\\
5  \> \> \>\>\> $\ctext{run }\ctext{case}(x,$\=
$\None \to [$\=$\ctext{letv } r = [\newref{\ctext{glyph}}(c,f)] \ctext{ in}$\\
6  \> \> \>\>\>\>\> $\ctext{letv } \_ = \ctext{update}(t, c, r) \ctext{ in }r],$ \\
7 \> \> \>\>\>\> $\ctext{Some }r \to [r])] \ctext{ in}$ \\
8 \> \> \> $\ctext{letv }getdata = [\lambda r.\; [!r]] \ctext{ in}$ \\
9 \> \> \> $(newglyph, getdata)]$
\\[0.5em]

$glyph(r, c, f) \equiv r \pointsto (c,f) * \top$ \\[0.5em]

$I \equiv table(t,mapping) * refs(mapping, \mbox{dom}(mapping))$ \\[0.5em]

$refs(mapping, \emptyset) \qquad\qquad  $ \= $\equiv$\;\;\=$\emp$ \\
$refs(mapping, \setof{c} \cup D)$ \> $\equiv$\> $mapping(c) \pointsto (c,f) * refs(f, D)$ \\
 
\end{tabbing}


In this implementation we have assumed the existence of a hash table
implementation with operations $\ctext{newtable}$, $\ctext{lookup}$,
and $\ctext{update}$, whose specifications we omit for space
reasons. The $\ctext{make\_flyweight}$ function definition takes a
font argument $f$, and then in its body it creates a new table $t$. It
then constructs two functions as closures which capture this state
(and the argument $f$) and operate on it. In lines 4-7, we define
$newglyph$, which takes a character and checks to see (line 5) if it is
already in the table. If it is not (lines 5-6), it allocates a new
glyph reference, stores it in the table, and returns the
reference. Otherwise (line 7), it returns the existing reference from
the table.  On lines 8, we define $getdata$, which dereferences
its pointer argument and returns the result. This implementation does
no writes, fulfilling the promise made in the specification. The
definition of the invariant state $I$ describes the state of the table
$t$ (and $mapping$), which are hidden from clients.

Observe how the post-condition to $\ctext{make\_flyweight}$ nests the
existential state $I$ with the validity assertion to specialize the
flyweight spec to the \emph{dynamically} created table. Each created
flyweight factory receives its own private state, and we can reuse
specifications and proofs with no possibility that the wrong
$\ctext{getdata}$ will be called on the wrong reference, even though
they have compatible types.

% It is also worth noting that our factory here does not correspond
% precisely to the most general form of the factory pattern. While we
% take an argument (the font $f$) and return a flyweight interface
% specialized to $f$, we do not change the implementations of $newchar$
% and $getdata$ based beyond closing over the newly allocated memo table
% and $f$.


% While this is technically possible, our language is simply-typed --
% which means that we cannot readily change data representations, and
% hence there is little reason to change the function
% implementations. If we added polymorphism to our language, we would
% be able to use existential quantification to more closely match
% object-oriented style.
% \vspace{-1em}
\section{Subject-Observer}

The subject-observer pattern is one of the most characteristic
patterns of object-oriented programming, and is extensively used in
GUI toolkits. This pattern features a mutable data structure called
the \emph{subject}, and a collection of data structures called
\emph{observers} whose invariants depend on the state of the
subject. Each observer registers a callback function with the subject
to ensure it remains in sync with the subject. Then, whenever the
subject changes state, it iterates over its list of callback
functions, notifying each observer of its changed state. While
conceptually simple, this is a lovely problem for verification, since
every observer can have a different invariant from all of the others,
and the implementation relies on maintaining lists of callback
functions in the heap.  

In our example, we will model this pattern with one type of subjects,
and three functions. A subject is simply a pair, consisting of a
pointer to a number, the subject state; and a list of observer
actions, which are imperative procedures to be called with the new
value of the subject whenever it changes. There is a function
$\ctext{newsub}$ to create new subjects; a function
$\ctext{register}$, which attaches observer actions to the subject;
and finally a function $\ctext{broadcast}$, which updates a subject
and notifies all of its observers of the change. 


We give a specification for the subject-observer pattern below:
%

\begin{tabbing}
1 \qquad \= $\exists sub : A_s \times \N \times \seqsort{((\N \To \assert) \times (\N \to \monad{1}))}.$ \\
2 \> $\exists \ctext{newsub} : \N \to \monad{A_s},$ \\ 
3 \> $\exists \ctext{register} : A_s \times (\N \to \monad{1}) \to \monad{1},$ \\
4 \> $\exists \ctext{broadcast} : A_s \times \N \to \monad{1}.$ \\
\\[0.5em]
5 \>$\forall n.\; \spec{\emp}{\run{\ctext{newsub}(n)}}{a:A_s}{sub(a, n, \epsilon)}$ \\
\> $\specand$ \\
6 \> $\forall f, O, s, n, os. $\=$(\forall i, k. \spec{O(i)}{\run{f(k)}}{a:1}{O(k)})$ \\
7\> \>$\specimp$\=$\setof{sub(s, n, os)}$ \\
8\> \>          \>$\run{\ctext{register}(s, f)}$ \\
9 \> \>          \>$\setof{a:1.\; sub(s, n, (O,f)\cdot os)}$ \\
\> $\specand$ \\
10 \> $\forall s,i,os,k.\; $\=$\setof{sub(s, i, os) * obs(os)}$ \\
  \>                       \>$\run{\ctext{broadcast}}(s,k)$ \\
  \>                       \>$\setof{a:1.\; sub(s, k, os) * obs\_at(os, k)}$ 
\\[0.5em]
$obs(\epsilon) \;\qquad\qquad $\=$\equiv \emp$ \\
$obs((O,f)\cdot os) $\>$\equiv (\exists i.\; O(i)) * obs(os)$ 
\\[0.5em]
$obs\_at(\epsilon, k) \;\qquad\qquad $\=$\equiv \emp$ \\
$obs\_at((O,f)\cdot os, k) $\>$\equiv O(k) * obs\_at(os, k)$ 
\\
\end{tabbing}

%
On line 1 we assert the existence of a three-place predicate $sub(s,
n, os)$. The first argument is the subject $s$'s whose state this
predicate represents. The second argument $n$ is the data the
observers depend on, and the field $os$ is a sequence of callbacks
paired their invariants. That is, $os$ is a sequence of pairs,
consisting of the observer functions which act on a state, along with
the predicate describing what that state should be.

On lines 2-4, we assert the existence of $\ctext{newsub}$,
$\ctext{register}$ and $\ctext{broadcast}$, which create a new
subject, register a callback, and broadcast a change, respectively.

$\ctext{register}$ is a higher order function, which takes a subject
and an observer action its two arguments. The observer action is a
function of type $\N \to \monad{1}$, which can be read as saying it
takes the new value of the subject and performs a side-effect. Because
$\ctext{register}$ depends on code, its specification must say how
this observer action should behave. $\ctext{register}$'s specification
on lines 6-9 accomplishes this via an implication over Hoare
triples. It says that \emph{if} the function $f$ is a good observer
callback, \emph{then} it can be safely registered with the
subject. Here, a ``good callback'' $f$ is one that takes an argument
$k$ and sends an observer state to $O(k)$. If this condition is
satisfied, then $\ctext{register}(s, f)$ will add the pair $(O,f)$ to
the sequence of observers in the $sub$ predicate.

$\ctext{broadcast}$ updates a subject and all its interested
observers.  The precondition state of $\ctext{broadcast}(s,k)$
requires the subject state $sub(s,n,os)$, and all of the observer
states $obs(os)$. The definition $obs(os)$ takes the list of observers
and yields the separated conjunction of the observer states. So when
$\ctext{broadcast}$ is invoked, it can modify the subject and any of
its observers. Then, after the call, the postcondition puts the $sub$
predicate and all of the observers in the same state $k$. The
$obs\_at(os,k)$ function generates the separated conjunction of all
the $O$ predicates, all in the same state $k$.

The implementation follows:

\begin{tabbing}
1 \qquad \= $A_s \equiv \reftype{\N} \times \reftype{\listtype{(\N \to \monad{1})}}$
\\[0.5em]
2 \> $sub(s, n, os) \equiv$\=$ \fst{s} \pointsto n * 
              list(\snd{s}, map\; \snd{} os) \land Good(os)$ 
\\[0.5em]
3 \> $Good(\epsilon) \!\qquad\qquad \equiv \top$ \\
4 \> $Good((O,f)\cdot os) \equiv\; $\=
   $\validprop{(\forall i,k.\; \spec{O(i)}{\run{f(k)}}{a:1}{O(k)})}$ \\
  \> \>      $\land Good(os)$ 
\\[0.5em]
5 \> $\ctext{register}(s, f) \equiv$ \=
         $[$\= $\letv{cell}{\comp{!(\snd{s})}}{}$ \\
6 \> \> \> $\letv{r}{\comp{\newref{\reftype{\listtype{(\N \to \monad{1})}}}{cell}}}{}$ \\
7 \> \> \> $\snd{s} := \Cons(f, r)]$
\\[0.5em]

8  \> $\ctext{broad}$\=$\ctext{cast}(s, k) \equiv$ \\
9  \>  \> $[$\=$\letv{dummy}{[\fst{s} := k]}{\ctext{loop}(k, \snd{s})}]$ \\


10 \> $\ctext{loop}$\=$(k, list) \equiv $\\
   \>         \>$[$\=$\letv{cell}{[!list]}{}$ \\
11 \>\>\> $\run{}\ctext{case}(cell,$\= 
            $\Nil \to [()],$ \\
12 \>\>\>\> $\Cons(f, tl) \to [$\=$\letv{dummy}{f(k)}{}$ \\
13  \>\>\>\> \> $\run{\ctext{loop}(k,tl)}])$ \\[0.5em]
14 \> $\ctext{new}\ctext{sub}(n) \equiv$ \=
          $[$\=$\letv{data}{\newref{\N}{n}}{}$ \\
15 \> \> \> $\letv{callbacks}{\newref{\listtype{(\N \to \monad{1})}}{\Nil}}{}$ \\
16 \> \> \> $(data, callbacks)]$
\end{tabbing}


In line 1, we state concrete type of the subject $A_s$ is a pair of a
pointer to a reference, and a pointer to a list of callback
functions. (This is \emph{not} an existential quantifier.  Since our
language is simply typed, we have no form of type abstraction and
simply use $A_s$ as an abbreviation.)  On line 2, we define the
three-place subject predicate, $sub(s,n,os)$. The first two subclauses
of the predicate's body describe the physical layout of the subject,
and assert that the first component of $s$ should point to $n$, and
that the second component of $s$ should be a linked list containing
the function pointers in $os$. (The $list$ predicate is described in
Section 3, when we give the definition of the iterator predicates.)

Then we require that $os$ be ``Good''. $Good$-ness is defined on lines
3 and 4, and says a sequence of predicates and functions is good when
every $(O,f)$ pair in the sequence satisfies the same validity
requirement the specification of $\ctext{register}$ demanded -- that
is, that each observer function $f$ update $O$ properly.  Note that we
interleave assertions and specifications to constrain the behavior of
code stored in the heap.

Next, we give the implementations of $\ctext{register}$ and
$\ctext{broadcast}$. $\ctext{register}$, on lines 5-7, adds its
argument to the list of callbacks. Though the code is trivial, its
correctness depends on the fact the $Good$ predicate holds for the
extended sequence --- we use the fact that the argument $f$ updates 
$O$ properly to establish that the extended list remains $Good$. 

$\ctext{broadcast}$, on lines 8-9, updates the subject's data field
(the first component), and then calls $\ctext{loop}$ (on lines 10-13)
to invoke all the callbacks. $\ctext{loop}(k, \snd{s})$ just recurs
over the list and calls each callback with argument $k$. The
correctness of this function also relies on the $Good$ predicate --
each time we call one of the functions in the observer list, we use
the hypothesis of its behavior given in $Good(os)$ to be able to make
progress in the proof.


Below, we give a simple piece of client code using this interface.


\begin{tabbing}
1 \qquad \= 
$\setof{\emp}$ \\
2 \> 
$\letv{s}{\ctext{newsub}(0)}{}$ \\
3 \> $\setof{sub(s, 0, \epsilon)}$ \\
4 \> $\letv{d}{\newref{\N}{(0)}}{}$ \\
5 \> $\letv{b}{\newref{\ctext{bool}}{(\ctext{true})}}{}$ \\
6 \> $\setof{sub(s, 0, \epsilon) * d \pointsto 0 * b \pointsto \ctext{true}}$\\
7 \> $\letv{()}{\ctext{register}(s, f)}{}$\\
8 \> $\setof{sub(s, 0, (double, f)\cdot\epsilon) * double(0) * b \pointsto \ctext{true}}$ \\
9 \> $\letv{()}{\ctext{register}(s, g)}{}$\\
10 \> $\setof{sub(s, 0, (even, g)\cdot(double, f)\cdot\epsilon) * double(0) * even(0)}$ \\
11 \> $\ctext{broadcast}(s, 5)$ \\
12 \> $\setof{sub(s, 5, (even, g)\cdot(double, f)\cdot\epsilon) * double(5) * even(5)}$ \\
13 \> $\setof{sub(s, 5, (even, g)\cdot(double, f)\cdot\epsilon) * d \pointsto 10 * b \pointsto \ctext{false}}$ 
\\[0.5em]
14 \> $f \qquad \qquad $\=$\equiv \lambda n:\N.\; [d := 2 \times n]$ \\
15 \> $double(n)$ \> $\equiv d \pointsto (2 \times n)$ \\
16 \> $g$ \> $\equiv \lambda x:\N.\; [b := even?(x)]$ \\
17 \> $even(n)$ \> $\equiv b \pointsto even?(n)$ \\
\end{tabbing}

% \vspace{-1em}
We start in the empty heap, and create a new subject $s$ on line 2.
On line 4, we create a new reference to $0$, and on line 5, we create
a reference to $\ctext{true}$. So on line 6, the state consists of a
subject state, and two references.  On line 7, we call
$\ctext{register}$ on the function $f$ (defined on line 14), which
sets $d$ to twice its argument. To the observer list in sub, we add
$f$ and the predicate $double$ (defined on line 15), which asserts
that indeed, $d$ points to two times the predicate argument. On line
8, we call $\ctext{register}$ once more, this time with the function
$g$ (defined on line 16) as its argument, which stores a boolean
indicating whether its argument was even into the pointer $b$. Again,
the state of $sub$ changes, and we equip $g$ with the $even$ predicate
(defined on line 17) indicating that $b$ points to a boolean
indicating whether the predicate argument was even or not. Since $d
\pointsto 0$ and $b \pointsto \ctext{true}$ are the same as
$double(0)$ and $even(0)$, so we can write them in this form on line
10.  We can now invoke $\ctext{broadcast}(s, 5)$ on line 11, and
correspondingly the states of all three components of the state shift
in line 12.  In line 13, we expand $double$ and $even$ to see $d$
points to 10 (twice 5), and $b$ points to $\ctext{false}$ (since 5
is odd).

\textbf{Discussion.} One nice feature of the proof of the
subject-observer implementation is that the proofs are totally
oblivious to the concrete implementations of the notification
callbacks, or to any details of the observer invariants. Just as
existential quantification hides the details of a module
implementation from the clients, the universal quantification in the
specification of $\ctext{register}$ and $\ctext{broadcast}$ hides all
details of the client callbacks from the proof of the implementation
-- since they are free variables, we are unable to make any
assumptions about the code or predicates beyond the ones explicitly
laid out in the spec. Another benefit of the passage to higher-order
logic is the smooth treatment of observers with differing invariants;
higher-order quantification lets us store and pass formulas around,
making it easy to allow each callback to have a totally different
invariant. 


% \input{htt-experiments}

\section{Related Work}
% \vspace{-0.5em}
The proof system is a synthesis of O'Hearn and
Reynolds's~\cite{sep-logic} work on separation logic, with Reynolds's
system of specification logic~\cite{spec-logic} for Algol, which
introduced the idea of turning Hoare triples into the atomic formulas
of a program logic. Birkedal, Biering, and Torp-Smith~\cite{hosl}
first extended separation logic to higher-order.

Parkinson developed a version of separation logic for Java in his
doctoral dissertation~\cite{parkinson-thesis}. His logic does not have
a notion of implications over specifications, instead using behavioral
subtyping to determine what specification dynamically dispatched
method calls could have. Parkinson and Bierman have also introduced a
notion of abstract predicate family~\cite{parkinson-bierman-05}
related to the higher-order quantification of Birkedal \emph{et al}.

% Nanevski, Morisett and Birkedal have developed Hoare Type
% Theory~\cite{htt}, which is a sophisticated dependently-typed
% functional language that, like our system, uses a monadic discipline
% to control effects.  Unlike our work, HTT takes advantage of type
% dependency to directly integrate specifications into the types of
% computation terms. Nanevski, Ahmad, Morisett and
% Birkedal~\cite{abstract-htt} have proposed using the existential
% quantification of their type theory to hide data representations,
% giving examples such as a malloc/free style memory allocator.

In addition to systems based on separation, there is also a line of
research based on the concept of object invariants and ownership.  The
Java modeling language (JML)~\cite{jml} and the Boogie
methodology~\cite{boogie} are two of the most prominent systems based
on this research stream. In Boogie, each object tracks its owner
object with a ghost field, and the ownership discipline enforces that
the heap have a tree structure. This allows the calculation of frame
properties without explosions due to aliasing, even though the
specification language remains ordinary first-order logic.

In his dissertation, Parkinson gave as an example a simple iterator
protocol, lacking the integration with composites we have exhibited.
Subsequently, we formalized a similar account of
iterators~\cite{iterator}, again lacking the integration with
composites. Jacobs, Meijer, Piessens and
Schulte~\cite{iterators-revisited} extend Boogie with new rules for
the coroutine constructs C\# uses to define iterators. Their solution
typifies the difficulties ownership-based approaches face with
iterators, which arise from the fact that iterators must have access
to the private state of a collection but may have differing
lifetimes. This work builds on Barnett and Naumann's generalization of
ownership to friendship~\cite{friends}, which allows object invariants
to have some dependency on non-owned objects.

The subject-observer pattern has been the focus of a great deal of effort,
given its prominence in important applications. Simultaneously with our own
initial formulation, Parkinson gave an example of verifying the
subject-observer protocol~\cite{parkinson-so}. Recently, Parkinson and
Distefano~\cite{jstar-parkinson-distefano} have implemented a tool to verify
these programs, and have demonstrated several examples including a verification
of a subject-observer pattern specified along these lines. The tool includes
automatic generation of loop invariants. 

The work of Barnett and Naumann is also capable of reasoning about the
subject-observer pattern, but only if all of the possible observers
are known at verification.  Leino and Schulte~\cite{boogie-sub-obs}
made use of Liskov and Wing's concept of history invariants or
monotonic predicates~\cite{liskov-wing} to give a more modular
solution. More recently, Shaner, Naumann and Leavens~\cite{ShanerLN07}
gave a ``gray-box'' treatment of the subject-observer pattern.
Instead of tracking the specifications of the observers in the
predicate, they give a model program that should approximates the
behavior of any actual notification method.

Pierik, Clarke and de Boer~\cite{creational-invariants} formalize another
extension to the Boogie framework which they name \emph{creation
  guards}, specifically to handle flyweights. They consider flyweights
an instance of a case where object invariants can be invalidated by
the allocation of new objects, and add guards to their specifications
to control allocation to the permitted cases. 



