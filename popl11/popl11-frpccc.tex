%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[preprint]{sigplanconf}

% The following \documentclass options may be useful:
%
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathpartir}
\usepackage{verbatim}

\newcommand{\ultrametric}{\mathbb{U}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\setof}[1]{\left\{{#1}\right\}}
\newcommand{\comprehend}[2]{\setof{{#1}\;|\;{#2}}}
\newcommand{\betterstate}[3]{{#2}\, {\sqsupseteq}^{#1} {#3}}
\newcommand{\futurestate}[4]{{#3} \gg^{#1}_{#2} {#4}}
\newcommand{\worsestate}[5]{{#4} {\sqsubseteq}^{(#1,#2)}_{#3} {#5}}
% \newcommand{\satisfies}[4]{({#1}, {#2}, {#3}) \;\mathsf{sat}\; {#4}}
\newcommand{\dom}[1]{\mathrm{dom}({#1})}

\newcommand{\ready}[3]{\mathsf{ready}({#1}, {#2}, {#3})}
\newcommand{\unready}[2]{\mathsf{unready}({#1}, {#2})}
\newcommand{\cells}[1]{\mathrm{cells}({#1})}
\newcommand{\refs}[1]{\mathrm{refs}({#1})}
\newcommand{\hasref}[3]{\mathsf{hasref}({#1}, {#2}, {#3})}
\renewcommand{\implies}{\Rightarrow}

\newcommand{\stream}[1]{\mathbf{#1}}
\newcommand{\term}[1]{\ensuremath{\mathtt{{#1}}}}
\newcommand{\spec}[4]{\setof{{#1}}\;{#2}\;\setof{{#3}.\;{#4}}}

\newcommand{\streams}{\mathit{streams}}
\newcommand{\thunks}{\mathit{thunks}}
\newcommand{\lags}{\mathit{lags}}
\newcommand{\locs}{\mathit{locs}}

\newcommand{\discrete}[1]{D({#1})}
\newcommand{\To}{\Rightarrow}
\newcommand{\shrink}{\rightsquigarrow}
\newcommand{\interp}[1]{[\![{#1}]\!]}
\newcommand{\unitval}{\left<\right>}

\newcommand{\floor}[1]{\left\lfloor{#1}\right\rfloor}

\newcommand{\interps}[1]{[\![{#1}]\!]_s}
\newcommand{\interpu}[1]{[\![{#1}]\!]_u}

\newcommand{\Head}{\mathit{Head}}
\newcommand{\Tail}{\mathit{Tail}}
\newcommand{\Code}{\mathit{Code}}
\newcommand{\Build}{\mathit{Build}}
\newcommand{\Local}{\mathit{Local}}
\newcommand{\Ref}{\mathit{Ref}}
\newcommand{\Ready}{\mathit{Ready}}
\newcommand{\Opt}{\mathit{Opt}}
\newcommand{\Stream}{\mathit{Stream}}
\newcommand{\NewStream}{\mathit{NewStream}}
\newcommand{\Mem}[1]{\mathit{Mem}(#1)}
\newcommand{\Seq}{\mathit{Seq}}
\newcommand{\Update}{\mathit{Update}}
\newcommand{\StableRef}{\mathit{StableRefs}}
\newcommand{\StableImp}{\mathit{StableImps}}
\newcommand{\Stable}{\mathit{Stable}}

\newcommand{\unittype}{\mathsf{unit}}
\newcommand{\celltype}[1]{\mathsf{cell}\;{#1}}
\newcommand{\opttype}[1]{\mathsf{option}\;{#1}}
\newcommand{\reftype}[1]{\mathsf{ref}\;{#1}}
\newcommand{\monad}[1]{\bigcirc{#1}}
\newcommand{\clock}{\mathsf{clock}}
\newcommand{\comp}[1]{\mathsf{code}\;{#1}}
\newcommand{\thunk}[1]{\mathsf{thunk}\;{#1}}
\newcommand{\streamtype}[1]{\mathsf{stream}\;{#1}}
\newcommand{\contracttype}{\mathsf{bool}}
\newcommand{\lolli}{\multimap}
\newcommand{\lollishrink}{-\!\!\!\,\bullet}
\newcommand{\valtype}[1]{\mathsf{value}\;{#1}}
\newcommand{\None}{\mathsf{None}}
\newcommand{\Some}[1]{\mathsf{Some}({#1})}
\newcommand{\stateok}[2]{\mathsf{updateok}({#1}, {#2})}

\newcommand{\counit}{\epsilon}
\newcommand{\tails}{\delta}

\newcommand{\powerset}[1]{\mathcal{P}(#1)}
\newcommand{\cellminus}[2]{\mathsf{cell}^{-}({#1}, {#2})}
\newcommand{\cellplus}[4]{\mathsf{cell}^{+}({#1}, {#2}, {#3}, {#4})}

\newenvironment{proof}[1][(Sketch)]{\noindent \textsc{Proof {#1}} }{}

\newcommand{\judge}[3][\Gamma]{{#1} \vdash {#2} : {#3}}
\newcommand{\judgec}[4][\Gamma]{{#1};{#2} \vdash {#3} : {#4}}
% Contractive commands
\newcommand{\const}[1]{\left<{#1}\right>}
\newcommand{\pair}[2]{({#1}, {#2})}
\newcommand{\fst}[1]{\pi_1{#1}}
\newcommand{\snd}[1]{\pi_2{#1}}
\newcommand{\unit}{()}
\newcommand{\letc}[3]{\mathsf{letc}\;{#1} = {#2}\;\mathsf{in}\;{#3}}
\newcommand{\fun}[2]{\lambda {#1}.\;{#2}}
\newcommand{\sfun}[2]{\hat{\lambda} {#1}.\;{#2}}

\newcommand{\Delays}{\mathbb{D}}
\newcommand{\U}{\mathsf{u}}
\newcommand{\D}{\mathsf{d}}

%
\newcommand{\fixme}[1]{\texttt{FIXME: {#1}}}
\newcommand{\head}[1]{\mathit{head}(#1)}
\newcommand{\tail}[2][]{\mathit{tail}^{#1}(#2)}
\newcommand{\ramify}[1]{\mathsf{U}(\mathtt{clock}, {#1})}

\newcommand{\einvariant}[3]{{#2} \stackrel{#1}{=} {#3}}
\newcommand{\satisfy}[2]{{#1}\;\mathrm{sat}\;{#2}}
\newcommand{\satisfyext}[2]{\mathrm{extsat}({#1}, {#2})}
\newcommand{\complete}[1]{\mathrm{complete}(#1)}

\newcounter{lineno}
\newenvironment{tabbingspec}{\setcounter{lineno}{0}\begin{tabbing}\refstepcounter{lineno}{\small \arabic{lineno}}\;\;\;\=}{\end{tabbing}}
\newcommand{\Newline}[1][0em]{\refstepcounter{lineno}\\[#1]{\small \arabic{lineno}}\>}

\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}

\begin{document}

\conferenceinfo{POPL '11}{date, City.} 
\copyrightyear{2011} 
\copyrightdata{[to be supplied]} 

\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{short description of paper}   % 'preprint' option specified.

\title{An Ultrametric Model of Reactive Programming}
\subtitle{Subtitle Text, if any}

\authorinfo{Neelakantan R. Krishnaswami}
           {Microsoft Research}
           {neelk@microsoft.com}
\authorinfo{Nick Benton}
           {Microsoft Research}
           {nick@microsoft.com}

\maketitle

\begin{abstract}
We describe a model of higher-order functional reactive programming
using ultrametric spaces, which provide a natural Cartesian closed
generalization of causal stream functions. We then show how reactive
programs may be implemented efficiently using an imperatively updated
dataflow graph and prove that this implementation is correct with
respect to the declarative semantics.
\end{abstract}

\category{CR-number}{subcategory}{third-level}

\terms
term1, term2

\keywords
keyword1, keyword2

\section{Introduction}

Typical interactive programs such as web applications, games, and GUIs
are written in a very imperative style. A program models dynamic
behavior by modifying state, and accepting callbacks to modify its own
state. These programs exhibit complex uses of aliasing, tricky control
flow through callback functions living in the heap, and in general are
very difficult to reason about. Part of the difficulty are the
inherent complexity of verifying programs using such powerful
features, but an even more fundamental problem is that it is not
immediately clear even what the semantics of such programs should be
--- and even the most powerful verification techniques are useless
without a specification for a program to meet.

One of the oldest and most successful proposals for giving semantics
to interactive programs is the paradigm of \emph{dataflow programming}. 
The basic idea is that instead of viewing a \fixme{figure out what 
to say!}



The contributions of this work are as follows:

\begin{itemize}
\item We give a new semantic model of functional reactive programming
  based on ultrametric spaces, which builds upon prior work in two
  ways. First, it generalizes existing models by supporting full
  cartesian closed structure, rather than more restrictive structures
  such as arrows, while still respecting important concepts such as
  the causality of stream functions (and in fact extending them
  naturally to higher-order).

  In particular, the use of metric spaces means that we can use
  Banach's contraction map theorem to interpret feedback. This means
  that unlike earlier semantics based on domain models of streams, we
  can restrict our semantics to only cover \emph{total, well-founded}
  stream programs. Furthermore, by using an abstract notion of
  contractiveness instead of an explicit notion of guardedness, our
  semantics lifts easily to model higher-type streams (e.g., streams
  of streams) and recursion at higher type. 

\item Next, we give a domain specific langauge for writing reactive
  programs. Since streams distribute over products and form a comonad,
  the co-Kleisli category of streams is also Cartesian closed, thus
  giving us \emph{two} natural notions of function for reactive
  programs.  Prior work~\cite{coiterative, essence-dataflow} has
  focused primarily on this category, but the interpretation of fixed
  points is significantly less natural in the co-Kleisli category than
  in the base category of ultrametric spaces.

  Instead, we extend Benton and Wadler's adjoint calculus for linear
  logic~\cite{benton-wadler} to give a language for writing reactive
  programs. The idea is to decompose the stream comonad into a pair of
  adjoint functors, which in the term calculus become modal operators
  connecting the two lambda calculi. We can then interpret fixed
  points in the category of ultrametrics (thereby retaining a simple
  equational theory for them) while still enabling programming
  implicitly with streams, as is common in dataflow languages such as
  Synchrone~\cite{synchrone}. Furthermore, we extend the adjoint
  calculus with additional judgements to track contractiveness, so
  that we can use typechecking to ensure that clients can only take
  fixed points of well-defined, strictly-contractive functions.

\item Second, we give a reasonably efficient implementation of this
  semantics in terms of an imperative dataflow graph, and prove the
  correctness of our implementation with respect to the semantics.  

  Furthermore, we believe our correctness proof for our library is of
  independent interest. It integrates techniques from verification ---
  such as separation logic and rely-guarantee -- with techniques from
  semantics, such as step-indexed models and logical relations. The
  upshot is that clients of the library can reason as if it were
  purely a mathematical object, even though it is implemented in terms
  of complex imperative higher-order code. The full suite of equations
  --- $\beta$, $\eta$ and fixed-point equations --- are all sound
  reasoning principles from the perspective of the client, even though
  the implementation is pervasively imperative. 

\item Finally, GUI toolkits often expose an essentially imperative
  interface to certain resources (such as the display). We address the
  question of how to smoothly integrate these kinds of effectful
  operations into our model. 
\end{itemize}

\section{Reactive Programs and Stream Transformers}

Reactive programs are usuall interpreted as \emph{stream
  transformers}. The idea is that a time-varying value of type $X$ can
be viewed as a stream of $X$s, and so a program that takes a
time-varying $X$ and produces a time-varying $Y$ is then a function
that takes a stream of $X$s and produces a stream of $Y$s. Then,
interactive programs can be specified in terms of the usual suite
of function operations on streams. 

\subsection{Causality}

However, the full function space on streams is too generous: there are
stream programs which do not have a sensible interpretation as an
interactive program. For example, consider the following simple example:

\begin{tabbingspec}
\term{profit : Price^\omega \to Order^\omega} \Newline
\term{profit\; prices = } \Newline
\;\;\= \term{let\; prices\_tomorrow = tail(prices) \;in} \Newline
    \> \term{if\; head(prices) < head(prices\_tomorrow)\;then} \Newline
    \> \qquad\= \term{Buy :: profit(tail(prices))} \Newline
    \> \term{else} \Newline
    \> \> \term{Sell :: profit(tail(prices))} 
\end{tabbingspec}

Here, the \term{profit} function receives a stream denoting a stream
of daily stock prices. If all the usual stream functions were
available, it could take the tail of this stream, and then compare the
head of today and tomorrow's stream to determine whether it should buy
or sell. Since compiler writers cannot yet generate code which travels
in time, we need some kind of semantic condition to explain which
stream functions should be ruled out, and which should be ruled in.

To do this, we appeal to the criterion of \emph{causality}~\cite{causality}: 
the output at time $n$ should depend only on the first $n$ inputs. We can
formalize this as follows, using the notation $\floor{xs}_n$ to denote the 
$n$-element prefix of the stream $xs$. 

\begin{definition}{(Causality)}
A stream function $f : X^\omega \to Y^\omega$ is said to be \emph{causal} when,
for all for all $n$ and streams $xs$ and $xs'$, if $\floor{xs}_n = \floor{xs'}_n$
then $\floor{f(xs)}_n = \floor{f(xs')}_n$.   
\end{definition}

This definition rules out the \term{tail} function, since its first
$n$ outputs are determined by its first $n+1$ inputs. For example, the
streams $1, 1, 1, \ldots$ and $1, 2, 3, \ldots$ agree on their first
output, but their tails disagree right away --- and so \term{tail} is
not causal function.

However, while this is an intuitive and appealing definition for
streams of basic types (such as integers or booleans), it is not
immediately clear how to generalize this definition. For example, what
might causality mean if we have a stream of \emph{streams}, or even a
stream of stream functions? 

\subsection{Guardedness and Fixed Points}

Next, we can also want to define streams by feedback or recursion; for
example, we may wish to define an increasing sequence of integers with
an expression like:
\begin{displaymath}
\term{nats = fix(\lambda xs.\; 0 :: map\;succ\;xs)}  
\end{displaymath}
  
The natural question is to ask when fixed points such as this are
well-defined. An operational way of thinking about this question is to
observe that the lambda expression \term{\lambda xs.\; 0 :: map\;succ\;xs} 
can produce its first output without looking at its input. So we can 
imagine implementing the fixed point by taking the output at time $n$ and 
feeding it back in as the input at time $n+1$, relying on the fact that at 
time 0 the input value does not matter.

\begin{definition}{(Guardedness)}
A function $f : X^\omega \to X^\omega$ is said to be \emph{guarded}
when there exists a $k > 0$ such that for all for all $n$ and streams
$xs$ and $xs'$, if $\floor{xs}_n = \floor{xs'}_n$ then
$\floor{f(xs)}_{n+k} = \floor{f(xs')}_{n+k}$.
\end{definition}

Then the following theorem about the existence and uniqueness of fixed
points easily follows: 

\begin{prop}{(Fixed Points of Guarded Functions)}
Every guarded endofunction $f : X^\omega \to X^\omega$ (where $X$ is a
nonempty set) has a unique fixed point.
\end{prop}

As with causality, guardedness is an intuitive and natural property,
but generalizations to higher types seem both useful and unobvious. For
example, we can imagine wanting to write a recursive \emph{function}:
\begin{displaymath}
\term{fib = fix(\lambda f\; \lambda (j,k).\; j :: f(k,j+k))}
\end{displaymath}
So the natural questions to ask are: what does guardedness mean at
higher type, and how can we interpret fixed points at higher type?

To answer these questions, we will use the basic properties of the
theory of metric spaces.

\section{An Ultrametric Model of Reactive Programs}

\subsection{Ultrametric Spaces in a Nutshell}

A complete 1-bounded \emph{ultrametric space} is a pair $(A, d_A)$,
where $A$ is a set and $d_A \in A \times A \to \R$ is a distance
function, satisfying the following four axioms:

\begin{itemize}
\item $d_A(x, y) = 0$ if and only if $x = y$
\item $d_A(x, x') \in [0,1]$
\item $d_A(x, x') = d_A(x', x)$
\item $d_A(x, x') \leq \max(d_A(x, y), d_A(y, x'))$
\end{itemize}

Completeness means that the limit of every Cauchy sequence in $A$
converges to a value in $A$. What distinguishes ultrametric spaces
from ordinary metric spaces is that the triangle inequality is more
stringent; instead of requiring the usual triangle inequality that
$d(x,x')$ be less than or equal to $d_A(x, y) + d_A(y, x')$,
ultrametrics require it to be less than or equal to $\max(d_A(x, y),
d_A(y, x'))$.

When there is no ambiguity, we will write $A$ for a metric space $(A, d_A)$. 

A map $f : A \to B$ between ultrametric spaces is \emph{nonexpansive} when 
it is non-distance-increasing:
\begin{displaymath}
  d_B(f\;x, f\;x') \leq d_A(x, x')
\end{displaymath}

A map $f : A \to B$ between ultrametric spaces is said to be
\emph{strictly contractive} when it shrinks the distance between 
any two points by a nonzero factor:
\begin{displaymath}
  \exists q \in [0,1).\; d_B(f\;x, f\;x') \leq q \cdot d_A(x, x')
\end{displaymath}

Complete 1-bounded ultrametric spaces and nonexpansive maps form a 
Cartesian closed category. The product metric on spaces $A$ and $B$
is given by the cartesian product of the underlying sets and the sup-metric
on pairs:
\begin{displaymath}
  d_{A \times B}((a,b), (a',b')) = \max \setof{d_A(a,a'), d_B(b,b')}
\end{displaymath}

Similarly, the exponential metric is given by the sup-metric over all
inputs:
\begin{displaymath}
  d_{A \to B}((a,b), (a',b')) = \sup \comprehend{d_B(f\;a,f\;a')}{a \in A}
\end{displaymath}

As a notational convenience, here we take $0$ as the formal
maximum of the empty set.

Any set $X$ can be trivially made into an ultrametric space $D(X)$ by
equipping it with the discrete distance function
\begin{displaymath}
d(x,x') = \left\{\begin{array}{ll}
                  0 & \mbox{if } x = x' \\
                  1 & \mbox{if } x \not= x' \\
                \end{array}
          \right.
\end{displaymath}

Given any ultrametric space $A$, we can define the metric space of
streams on $A$ by equipping the set $A^\omega$ with an ultrametric $d$
(which we will call the \emph{causal metric of streams}) as follows:
\begin{displaymath}
  d_{A^\omega}(as, as') = \sup \comprehend{2^{-n}\cdot d_A(as_n, as'_n)}{n \in \N}
\end{displaymath}

\noindent Furthemore, this is a functorial action; given any map $f : A \to B$, 
we can define a corresponding function $f^\omega : A^\omega \to B^\omega$ which
takes identities to identities and commutes with composition, by mapping
$f$ over the elements of the input stream.

The interpretation of the stream metric is easiest to understand in
the case of streams of discrete elements. In this case, the metric
says that two streams are closer, the later the time at which they
first disagree. So two streams which have differing values at time $0$
are at a distance of $1$, whereas two streams which never disagree
will have a distance of $0$ (and hence will be equal streams).

Finally, the one classical theorem about metric spaces which we will
make serious use of is Banach's contraction map theorem.
\begin{prop}{(Banach's Fixed Point Theorem)}
For any nonempty, complete metric space $A$ and strictly contractive
endofunction $f : A \to A$, there exists a unique fixed point of $f$.
\end{prop}

\subsection{From Ultrametrics to Functional Reactive Programs}

Now we can show that for streams of base type, the properties of maps
in the category of ultrametric spaces correspond exactly to the
properties of first-order reactive programs we discussed in the
previous section.

\begin{theorem}{(Causality is Nonexpansiveness)}
Suppose $X$ and $Y$ are sets. Then a function $f : X^\omega \to
Y^\omega$ is causal if and only if it is a nonexpansive function under
the causal metric of streams of elements of the discrete spaces $D(X)$
and $D(Y)$.
\end{theorem}

\begin{theorem}{(Guardedness is Contractiveness)}
Suppose $X$ and $Y$ are sets. Then a function $f : X^\omega \to
Y^\omega$ is guarded if and only if it is a strictly contractive
function under the causal metric of streams of elements of the
discrete spaces $D(X)$ and $D(Y)$.
\end{theorem}

The proof of these two theorems is nothing more than the unwinding of
a few definitions. However, the consequences of are quite dramatic! By
interpreting our programs in the category of ultrametric spaces:
\begin{enumerate}
\item We can interpret tuples and functions (with the full $\beta$ and
  $\eta$ rules) thanks to the Cartesian closure of this category.
\item Since streams are functorial, we can interpret streams of
  streams.
\item Furthermore, contractiveness gives an analogue of guardedness
  that makes sense at all types, and likewise Banach's fixed point
  theorem gives an interpretation of fixed points that makes sense at
  all types.
\end{enumerate}

\subsection{The Co-Kleisli Category of Streams}

However, when we look at synchronous dataflow languages like Lucid
Synchrone~\cite{synchrone}, we see that they do not offer this
API. Instead, in these languages time is implicit, and streams are
only very rarely directly manipulated. So instead of writing a program
like:
\begin{displaymath}
  \term{f = \fun{(xs,ys)}{map\;(+)\;(zip\;(xs,ys))}}
\end{displaymath}
We would instead write the program
\begin{displaymath}
  \term{f = \fun{(x,y)}{x + y}}
\end{displaymath}
with the expectation that it will be \emph{lifted} to operate over
streams. The reason for this choice is an essentially operational
one. Consider a program like the following: 
%
\begin{displaymath}
  \term{skew = \fun{xs}{(xs, 0 :: xs, 2 :: 3 :: xs)}}
\end{displaymath}
% 
This program accepts a stream \term{xs} as an input, and then
returns three streams --- the original stream \term{xs}; a stream
which first produces 0 and then the values of \term{xs}; and finally a
stream which yields 2, 3, and then the values of \term{xs}.

This means that if we implement this stream program as an imperative
program which modifies some state at each time step to dynamically
emit the values of the streams it computes, then at time $n+2$, this
program will also need to know the value of \term{xs} at times $n+1$
and $n$, in order to correctly emit the values of the second and third
components of its result. If programs which do unbounded buffering can
be written, then the danger of \emph{space leaks} arises, since the
runtime system may need to retain potentially arbitrary amounts of
history to compute values. By making time implicit and having the
compiler lift values to streams, it becomes difficult to write
programs which leak memory under this implementation strategy.

Happily, there is a clean mathematical semantics which captures the
essence of this class of restriction: namely, the co-Kleisli category
of streams over ultrametric spaces. 

To recall the details, a \emph{comonad} on a category $\ultrametric$
is a functor $S : \ultrametric \to \ultrametric$, equipped with two
natural transformations $\counit_A : S(A) \to A$ (the counit) and
$\tails_A : S(A) \to S(S(A))$ (the comultiplication) satisfying the
equations $\tails_A; S(\tails_A) = \tails_A; \tails_{S(A)}$ and
$\tails_A; S(\counit_A) = id = \tails_A; \counit_{T(A)}$.  (Here we
use the semicolon to indicate composition in diagrammatic order.)  In
the case of streams, the counit $\counit$ is the head function on
streams, and the comultiplication takes a stream and returns the a
stream containing the successive tails of the input stream. The action
of the stream functor is just the map functional for streams.
\footnote{The ``Kleisli triple'' formulation for monads is perhaps
  more familiar to functional programmers, which is given in terms of
  an extension operator sending maps $f : A \to T(B)$ to $\term{bind}(f)  : T(A) \to
  T(B)$.  Dually, the \emph{coextension} for comonads sends maps $f :
  S(A) \to B$ to $f^\dagger S(A) \to S(B)$, and can be defined as
  $f^\dagger = \delta; S(f)$.}

We will also make use of the fact that streams distribute over
products and vice-versa--- that is, $S(A \times B) \simeq S(A) \times
S(B)$, meaning that a stream of pairs is isomorphic to a pair of
streams. We will write \term{unzip} and \term{zip} for the components
of this isomorphism, corresponding to the similarly-named functions in
functional programs.

The co-Kleisli category $\ultrametric^S$ of a comonad $S : \ultrametric
\to \ultrametric$ is the category of free $S$-coalgebras. Its objects
are the objects of $\ultrametric$, but each map $f : A \to B$ in
$\ultrametric^S$ is a morphism $f : S(A) \to B$ in
$\ultrametric$. Amazingly, this category is also cartesian closed; the
identity, composition, projection, pairing, currying and evaluation
maps are defined in Figure~\ref{cokleisli-defs}.

The motivation for working in this category is that a map $e : A \to
B$ in this category can be thought of as the interpretation of a
synchronous dataflow program with a free variable of type $A$, as is
usual for categorical semantics. The fact we \emph{implicitly} lift
variables to streams means we need to treat the free variable as a
stream value. So we want a morphism $e : A \to B$ to be represented
by a function $e : S(A) \to B$ in the underlying category $\ultrametric$. 
Intuitively, the meaning of the term can be read as saying ``at each
instant, $e$ will have a value of type $B$ depending on the values
that its free variable will take on''.

Then, the way that we can interpret the dynamic behavior of a term is
via the coextension; $e^\dagger$ is a map $e : S(A) \to S(B)$ in
$\ultrametric$, and is defined to be the function we get by feeding
$e$ the successive tails of the original input stream. So this says
that if the input $A$ takes on the values $as = [a_0, a_1, a_2,
  \ldots]$, then the output results will be $[e(as), e(\tail{as}),
  e(\tail{\tail{as}}), \ldots]$.

However, while this category is Cartesian closed, and so can interpret
pairs and function spaces, it does not have coproducts, and so cannot
interpret sums. An even bigger difficulty is that the instantaneous
interpretation of terms makes it very difficult to interpret
operations acting on streams of streams. The key difficulty is that
there is no map in this category which takes a stream of streams, and
whose coextension yields the head of the stream of streams.

More concretely, $\ultrametric^S$ does not have a map which
corresponds to taking an input like $[[0,1,2,\ldots], [0,3,6,\ldots],
  [0,5,10,\ldots], \ldots]$ and returns 0 on the first time step, 1 on
the second, 2 on the third, and so on. The reason is that this
sequence is generated by the coextension of a map of type $S(\N) \to
\N$ (in $\ultrametric^S$), and so at time 0 it receives the argument
$[[0,1,2,\ldots], [0,3,6,\ldots], [0,5,10,\ldots], \ldots]$, and at
time 1 it receives the argument $[0,3,6,\ldots], [0,5,10,\ldots],
\ldots]$ and so on. So at time 1, we no longer remember what the
argument at the time 0 was, and so cannot return the right answer.

This, in turn, makes defining fixed point operators very difficult: a
variable of type $A \lolli A$\footnote{we use $A \lolli A$ to denote
the exponential in $\ultrametric^S$, for reasons that will shortly
become clear} denotes a stream of functions. If all of these
functions are contractive, and we take fixed point of coextensions
pointwise, then we get a stream of streams --- at which point we
discover that we can never look at the \emph{second} element of any of
the results.

This temporal amnesia is the very reason that space leaks become
harder to program, so this is not an accidental difficulty. 

\begin{figure}
\begin{mathpar}
  \begin{array}{lcl}
    \mathsf{id}   & = & \counit \\
    \mathsf{f; g}  & = & \tails; S(f); g \\[1em]

    \mathsf{fst} & = & \mathsf{unzip}; \fst{}; \counit \\
    \mathsf{snd} & = & \mathsf{unzip}; \snd{}; \counit \\
    \mathsf{pair}(f,g) & = & (f,g) \\[1em]

    \mathsf{curry}(f) & = & \lambda(\mathsf{zip}; f) \\
    \mathsf{eval}     & = & \mathsf{unzip}; (\counit \times \mathit{id}); \mathit{eval} \\
 \end{array}
\end{mathpar}
\caption{Definition of operations in the co-Kleisli category}
\label{cokleisli-defs}
\end{figure}

\subsection{Adjoint Logic}

At this point, we have two views of reactive programming. One, which
we might call ``FRP-style'', has a very simple semantics in terms of
ultrametrics and stream values, and naturally supports fixed points at
any type. However, it seems difficult to implement efficiently.  On
the other hand, we can take a ``synchronous dataflow'' view to support
more efficient implementation techniques, at the price of making it
difficult to give good semantics to feedback. So it is natural to ask
if there is some way of combining the strengths of the two approaches.
We meet this goal by adapting Benton and Wadler's~\cite{benton-wadler}
adjoint model of the semantics of linear logic. Originally developed
to give a model of linear logic, it turns out to be abstract enough
that it applies naturally to the setting of dataflow programming as
well. 
\begin{definition}{} 
An \emph{adjoint model} is specified by:
\begin{enumerate}
\item A cartesian closed category $(\ultrametric, 1, \times, \To)$. 
\item A symmetric monoidal closed category $(\ultrametric^S, I, \otimes, \lolli)$.
\item A symmetric monoidal adjunction $((-)^\omega, V, \eta, ?, m, n)$ from $\ultrametric$ to $\ultrametric^S$. 
\end{enumerate}
\end{definition}
In our setting, we take the CCC to be the category of ultrametric
spaces, and take the monoidal closed structure in the definition to be
the cartesian closed structure of the co-Kleisli category (since any
Cartesian product is automatically a monoidal product as well). Then,
to get the adjunction, we we can exploit the fact that any comonad is
the composition of a pair of adjoint functors, and decompose the
stream functor into a pair of functors that go between the category of
ultrametric spaces and the co-Kleisli category of streams over
it. Then, the functors $(-)^\omega : \ultrametric^S \to \ultrametric$
and $V(-) : \ultrametric \to \ultrametric^S$ are defined as:
\begin{mathpar}
  \begin{array}{lcl} 
    V(X)           & = & X \\
    V(f : X \to Y) & = & \epsilon_X; f \\[1em]

    A^\omega             & = & S(X) \\
    (f : A \to B)^\omega & = & \delta_{A}; S(f) = f^\dagger\\
  \end{array}
\end{mathpar}

Intuitively $V(f)$ takes a function $f$ from the general FRP world and
embeds it into the synchronous dataflow world by having it act at each
instant on the head of the stream. The action of the other half of the
adjoint $(g)^\omega$ takes a synchronous dataflow function, and turns
it into a general function on streams, via the coextension operation. 
Verifying that $V(f)^\omega = S(f)$ is immediate from the definitions. 

Furthemore, we have the isomorphisms that $V(X \times Y) \simeq V(X)
\otimes V(Y)$ and $X \To B^\omega \simeq (V(X) \lolli B)^\omega$. This
latter isomorphism is actually the key property which lets us switch
our view of a program between the synchronous and the functional
reactive views.

For example, under the adjoint view, it becomes very straightforward 
to solve the puzzle at the end of the last subsection. The natural type of
a fixed point operator in $\ultrametric^S$ is $(A \lollishrink A) \lolli S(A)$,
which at each instant yields a stream corresponding to the fixed point 

\section{DSL}

\fixme{This needs to be reworked in light of the new adjoint logic presentation}

\begin{mathpar}
\inferrule*[right=CConst]
          {\judge{e}{A}}
          {\judgec{\Delta}{\const{e}}{A}}
\and
\inferrule*[right=CApp]
          {\judgec{\Delta}{c}{A \shrink B} \\
           \judge[\Gamma, \Delta]{e}{A}}
          {\judgec{\Delta}{c\;e}{B}}
\and
\inferrule*[right=CPair]
          {\judgec{\Delta}{c}{A} \\ 
           \judgec{\Delta}{c'}{B}}
          {\judgec{\Delta}{\pair{c}{c'}}{A \times B}}
\and
\inferrule*[right=CFst]
          {\judgec{\Delta}{c}{A \times B}}
          {\judgec{\Delta}{\fst{c}}{A}}
\and
\inferrule*[right=CSnd]
          {\judgec{\Delta}{c}{A \times B}}
          {\judgec{\Delta}{\snd{c}}{A}}
\and
\inferrule*[right=CUnit]
          { }
          {\judgec{\Delta}{\unit}{1}}
\and
\inferrule*[right=CLet]
          {\judgec{\Delta}{c}{A} \\ 
           \judgec[\Gamma, x:A]{\Delta}{c'}{B}}
          {\judgec{\Delta}{\letc{x}{c}{c'}}{B}}
\and
\inferrule*[right=CLam]
          {\judgec[\Gamma, x:A]{\Delta}{c}{b}}
          {\judgec{\Delta}{\fun{x:A}{c}}{A \To B}}
\and
\inferrule*[right=CSLam]
          {\judgec{\Delta, x:A}{c}{B}}
          {\judgec{\Delta}{\sfun{x:A}{c}}{A \shrink B}}
\end{mathpar}

\section{The Programming Language and Library}

\section{The Implementation}

We have two cartesian closed categories in play, which are
represented a bit differently. 

\begin{mathpar}
  \begin{array}{lcl}
    \interpu{1}           & = & \unittype \\
    \interpu{X \times Y}  & = & \interpu{X} \star \interpu{Y} \\
    \interpu{X \to Y}     & = & \interpu{X} \to \interpu{Y} \\
    \interpu{X \shrink Y} & = & \interpu{X} \to \interpu{Y} \\
    \interpu{A^\omega}     & = & \comp{\streamtype{\interps{A}}}
    \\[1em]
    \interps{I}           & = & \unittype \\
    \interps{A \otimes B} & = & \interps{A} \star \interps{B} \\
    \interps{A \lolli B}  & = & \streamtype{\interps{A}} \to \comp{\streamtype{\interps{B}}} \\
    \interps{A \lollishrink\, B}  & = & \streamtype{\interps{A}} \to \comp{\streamtype{\interps{B}}} \\
    \interps{S(A)}        & = & \streamtype{\interps{A}}  \\
    \interps{\valtype{X}} & = & \interpu{X} 
    \\[1em]
    \streamtype{\tau}     & = & \celltype{\opttype{\tau}} \\
  \end{array}
\end{mathpar}

Most of the types of ultrametric world can be represented using ML
types. One interesting addition is that we have a type of strictly
contractive functions $A \shrink B$. This is represented in exactly
the same way as the ordinary ML function space, but must be kept
isolated in order to support their extra operation of fixed points.

The interesting case is the $A^\omega$ case which represents the type
of streams of values of type $A$, which are drawn from the objects of
the co-Kleisli category of streams over ultrametric spaces.  This
clause is interpreted as $\comp{\interps{A}}$, which can be understood
as follows. In the category of ultrametric spaces, we want to
\emph{understand} streams as having no temporal status at all -- they
are pure values. However, we want to \emph{represent} streams by
imperative data structures which change over time. This conflict is
resolved by representing an infinite stream as a computation which
constructs and initalizes a stream data structure. So each time we
evaluate a value of type $A^\omega$, we construct a fresh stream,
ready to begin counting regardless of when it was created.

In contrast, the interpretation of the co-Kleisli category is one in
which time plays a very significant part. The intuition is that a
dataflow graph realizes a collections of streams in the following way
--- the state of the dataflow graph is the seed of an unfold.  So time
is essentially a hidden argument to everything.

The argument to a function $A \lolli B$ comes as a stream, and the
return value is term of type $\comp{\streamtype{B}}$. The code
constructor permits the implementation to read and extend the dataflow
graph, doing some initialization to return a stream cell. Surpisingly,
this result is a stream, and not a point value, the way that the
intepretation of morphisms in the co-Kleisli category might initially
suggest. The corresponding clause of our logical relation in fact
takes the coextension of the function argument, and requires that the
result $d$-approximate the result of applying a stream to the lifting. 

\section{The Program Invariant}

\subsection{Discussion}

Q: What is the underlying intuition?

\noindent A: A stream is a node in a dataflow graph, which represents
a stream value.

The values are coinductively generated, and the \emph{state} of the
dataflow graph gives the \emph{seed} of the unfold operation which
generates the stream. That is, we want a recursive specification 
which looks something like this: 

\begin{tabbing}
$\mathit{Stream}(xs, \term{x}, \theta) \triangleq$  \\
\;\;\=$\setof{H(\theta)}$  \\
    \>\term{read(x)} \\
    \>$\setof{\term{a}.\;\exists \theta'. H(\theta') \land a = \head{xs} \land 
                           \mathit{Stream}(\tail{xs},\term{x}, \theta')}$ \\
\end{tabbing}

When we try to put this into practice, several complications to this
simple picture arise.

\begin{enumerate}

\item First, we have values other than streams -- we need to support
  units, tuples, streams, and most significantly \emph{functions}.
  To handle the wide variety of types of object we can construct, we
  will need to use a logical relation which relates a mathematical
  value to a program value \emph{plus} the state of the dataflow 
  graph. 

  In fact, since we have two categories of expressions, we will need
  two mutually-recursive logical relations, one for values which
  belong to the category of ultrametrics, and the other for values
  which belong to the co-Kleisli category of streams.

\item Next, multiple streams may share the dataflow graph.

  If we have two cells $a$ and $b$, both of whose code reads a cell
  $c$, then on a timestep in which we read both $a$ and $b$, one of
  them will $c$ in a state when it has already been updated by the
  other one. Therefore we need to specify what makes this interference
  safe --- we need to say what it means for a subgraph to ``already
  have been updated''. To do this, we must specify the values a term's
  dependencies will take on as they are evaluated, so that our
  specification can say that we are happy with a dataflow cell as long
  as it produces the appropriate value. We call this the \emph{rely},
  in analogy to rely-guarantee. Then, we can say that a state is
  ``more defined'' than another one when more of its cells are ready,
  and give a Kripke relation which asserts that an expression good in
  one state is also good in all ``more defined'' states.

  Since we relate mathematical values to program values, and this
  relation includes the memory, we are lead to the further
  complication that stream cells in the dataflow graph may have types
  which are larger than the type of the current clause of the logical
  relation. Therefore, we cannot define our logical relation by
  induction on the type structure --- we need to borrow some ideas
  from step-indexed models, and define the rely mutually-recursively
  with the logical relation.

\item We also build our dataflow graph incrementally and dynamically,
  adding new cells even as current cells are evaluted. So we need to
  know that operations which extend the graph will not break existing
  code. So the Kripke relation on memories also needs to support a
  notion of the dataflow graph getting bigger, in addition to getting
  more defined.

\item Since we are implementing streams with stateful dataflow cells,
  we need to ensure that there are no cyclic dependencies in the flow
  graph. Therefore we need to ensure that a cell will not write to
  anything which depends upon it, to prevent event storms.

\item Our dataflow graph is stateful: operations like accumulators and
  fixed points are implemented with imperative references, which give
  a form of state whose access does not create new dependencies.

  Furthermore, these references may need to updated by a collaboration
  between dataflow cells. For example, in the code for \term{fix}, we
  want to feed outputs at time $n$ as inputs at time $n+1$. We do this
  with a reference cell which is read by the input, and written to by
  the output cell. So the getter and the setter of the cell are
  different, and the output should not write to the reference before
  it knows the input has read it.

\item The next problem connected with state in the dataflow graphs is
  that in order to correctly realize a stream, a cell with auxilliary
  state must have been evaluated on every tick. For example, consider
  a cell $c$ which realizes the stream 1, 2, 3, \ldots by maintaining
  a counter which it updates. If the cell is not evaluated at a
  particular time step, then the values it produces will fall behind
  the global clock.

  However, not all cells need to be evaluated every time step. For
  example, suppose we have a cell $c'$ which produces $2, 4, 6,
  \ldots$ by reading $c$ and returning twice its value. In this case,
  the computation at $c'$ is stateless, and so it does not need to be
  evaluated except on the ticks when its value is actually needed.

  So our program should maintain a list of the state-modifying cells in
  the dataflow graph, so that the event loop can evaluate the ones
  that have yet to be read before it advances time by a step. Furthermore, 
  every stateful cell also needs to register itself with the global clock
  cell, so that when the clock is ticked, they will invalidate themselves
  properly. 

\item Finally, when computing the fixed point of a stream function
  $f$, we proceed by giving $f$ a dummy input on the first time step,
  and giving the output at time $n$ as the input at time $n+1$. This
  means that each stream object in our rely may also contain a delay
  -- that is, it may lack a value (ie, be $\None$) at this time step,
  and only contain values beginning at the next time step.
\end{enumerate}

\subsection{The Formal Specification}

In Figures \ref{logical-relation} and \ref{satisfaction-relation}, we
give three mutually-dependent definitions of values and memory
states. To explain this relation, we will proceed in multiple passes,
first giving a quick intuitive reading of each of the three
definitions, and then proceeding to describe them in detail.

The relation $U^d_A(v, \term{v})$ relates elements $v$ of a metric
space $A$ to a concrete program term \term{v}. Intuitively, it can be
read as saying ``the semantic value $v$ is approximated by the
computational value \term{v} to at least distance $d$''. 

Next, the relation $V^d_A(v, \term{v}, \mu, \sigma)$ relates values in
the co-Kleisli category of streams to the program terms \term{v}. The
relation $V^d_A(v, \term{v}, \mu, \sigma)$ has the intuitive reading
``the semantic value $v$ is approximated by computational value
\term{v} and memory state $\mu$ to at least distance $d$. Furthermore,
the use of \term{v} may involve evaluating the cells in
$\sigma$''. These two relations are defined in
Figure~\ref{logical-relation}.

A memory state is a pair $(\theta, R)$, where $\theta$ is a formula
describing a dataflow network, and $R$ is a \emph{rely} describing the
stream of semantic values each cell in the network is expected to
produce. To relate these two, we have a satisfaction relation
$\satisfy{(\theta, R)}{d}$, which can be read as saying, roughly, that
$\theta$ implements the streams in $R$ to distance $d$. This
satisfaction relation is the most complicated part of the definition,
since we have to account for all of the issues discussed in the
previous section. Our satisfaction relation is given in
Figure~\ref{satisfaction-relation}.

\subsubsection{The Logical Relation}

In Figure~\ref{logical-relation}, we give the two logical relations,
one for programs representing values in the category of ultrametric
spaces, and one for values in the co-Kleisli category. The relation
for ultrametric values is given in lines 1-6. 

The clauses for the two function types on lines 3 and 4 look like
identical --- they have the standard Kripke semantics ``in future
worlds, take related arguments to related results''. However,
segregating the two types ensures that we know that inhabitants of $X
\shrink Y$ are contractive, which means that we know the
\emph{mathematical} fixed points of $X \shrink X$ exist. 

On lines 5-6, we give the relation for streams $A^\omega$. The
relation $\Build^{d'}_A(vs, code, \mu, \emptyset, \U)$ says that in
all well-formed memories, running \term{code} will build a fresh piece
of dataflow graph to represent the stream value $vs$, with no static
dependencies on any existing piece of the graph (the $\emptyset$) and
without any delays in the output (the $\U$). This allows multiple uses
of the stream at different times to all see the same sequence of
mathematical values. 

Explaining what the $\Build^{d}_A(vs, code, (\theta,R), \sigma, L)$
relation (defined on lines 18-24) does involves some unavoidable
forward references, since it depends on the memory, whose details are
explained in the following subsection. It says that if we begin in a
state which satisfies the formula $\theta$ (line 19), executing
\term{code} will put us in a greater state line and return a fresh
dataflow cell (line 21).  This cell will realize the stream $vs$, and
its static dependencies will be bounded by $\sigma$ (lines
22,25-26). Furthermore, all the cells $u$ updated during the execution
of \term{code} are either in the static dependencies $\sigma$, or 
are fresh cells (lines 23,27-29). Finally, 

Then, on lines 13-24

\subsubsection{The Memory}

We specify a rely $R$ as 9-tuple:

\begin{enumerate}
\item A finite set of stream cells and metric types $C$. 
\item An assignment of streams values $V_S : C \to \mathit{Value}^\omega$ 
  of the appropriate type to each cell.\footnote{We should write this 
  as a dependent product, with a stream cell being an element of type 
  $\Sigma A:\mathrm{type}.\;\celltype{\opttype{\interps{A}}}$, and $V_S$ having
  the type $V_S : \Pi (A, \_) \in C.\; A^\omega$. However, we will suppress these 
  dependencies to reduce notational clutter.}
\item A ``delay flag'' $D : C \to \Delays$ which says for each cell whether 
  its output is delayed $\D$ or whether it is undelayed $\U$. 
\item A set of reference cells $L$. 
\item An assignment of a stream of values $V^L : L \to \mathit{(1 +
  \mathit{Value})^\omega}$ for each local reference. Note in
  particular that our local state is given as a stream of
  \emph{options}: this is because we might want to use some state for
  ``only a little while''. (This arises in the correctness proof of the 
  \term{cons} function, for example.)
\item A ``getter'' for each reference cell $G : L \rightharpoonup
  C$. This is the cell which will read that reference cell. Note that
  this is a \emph{partial} function, which means that there can be
  reference cells which are not yet going to be read by anyone. This
  will let us build up the dataflow graph incrementally, while still
  remaining within the rely. 
\item A ``setter'' for each reference cell $S : L \rightharpoonup
  C$. This is the cell which has responsibility for writing the next
  value of the reference cell. Like the getter $G$, the getter $S$ is
  also partial. However, we require that its domain be a superset of $G$'s ---
  that is, we will always define getters before setters. 

\item A function $\Delta : C \to \powerset{C}$, giving the ``static
  dependencies'' of each cell. The intuition for this relation is that
  for each cell \term{c}, then $\term{c'} \in \Delta(\term{c})$ tells
  us that \term{c'} is a cell in the \emph{current} heap which the
  evaluation of \term{c} may update. (That is, the effect of reading
  \term{c} may create new cells it depends on, but since they are not
  in the current heap they do not appear in $\Delta(\term{c})$.)

  The conditions on this function are as follows. First, viewed as a
  relation, $\Delta$ must be a partial order, to ensure that there
  will be no cycles in the dependency graph. 

\item A function $\Delta^L : L \to \powerset{C}$, which describes the 
  static dependencies of the contents of each reference cell. The reason
  we need to track the dependencies of reference contents is that 
  cells may read references and use their contents, and so we need to 
  know what the dependencies for each value may be. 
\end{enumerate}

Often, we will be dealing with multiple relies, and as a matter of
notation we will name the appropriate component using the name
superscripted with the rely. So if $R$ is a rely, then we will write
$C_R$ for its cells, $V_R$ for the values of the cells, and so on.

We equip relies with a partial order as follows. We say that $R
\sqsubseteq R'$, when $C_R \subseteq C_{R'}$ and $L_R \subseteq L_{R'}$, and
furthermore each function is extended pointwise. That is, if $c \in
\dom{R}$, then $V_{R'}(c) = V_{R}(c)$, and similarly for $D_S$, $L$,
$V_L$, $G$, $S$, and $\Delta$. (Note in particular that the static
dependencies for a given cell do not grow -- the extension order for
$\Delta$ is more stringent than simply extension of the partial order.)

Furthermore, for any rely, we can also define its tail
$\tail{R}$. First, the footprints of the heap are unchanged
$C_{\tail{R}} = C_R$ and $L_{\tail{R}}$. Second, the reference values
all go to their tails $V^L_{\tail{R}} =
\fun{\term{r}}{\tail{V^L_R(\term{r})}}$.  Third, the stream values for
cells go to their tails if they are not delayed, and remain unchanged
if they are. $V_{\tail{R}} = \fun{\term{c}}{\mathsf{if}\;D_R(\term{c})
  =
  \U\;\mathsf{then}\;\tail{V_R(\term{c})}\;\mathsf{else}\;V_R(\term{c})}$.
Fourth, the delay flag becomes $\U$ for all the cells, $D_{\tail{R}} = \fun{\term{c}}{\U}$. 
Finally, the other components --- $\Delta, \Delta^L, S, G$ --- remain unchanged. 

Given this, we can explain the satisfaction relation in
Figure~\ref{satisfaction-relation}. We say when a graph $\phi$
\emph{satisfies} a rely $R$ to distance $d$, (written
``$\satisfy{(\phi, R)}{d}$'') when:
\begin{itemize}
  \item The cells of the graph are equal to $C_R$, and the references
    of the graph are equal to $L_R$. This is lines 2-3. 
  \item Every cell in the graph is derivably either ready or unready. (Line 4)
  \item Every ready cell which touches state has the \term{clock} in its
    footprint. (Lines 5-6)
  \item The reference \term{i} contains a subset of the cells which 
    read or write state. (Line 7)
  \item Every reference has contents realizing the head of its
    stream if its setter is unready, and the head of the tail of its
    stream if its setter is ready. We say nothing about a reference
    if it does not yet have a setter. (Line 9)
  \item Each cell \term{c} in the graph realizes a stream of values corresponding
    to $V_R(\term{c})$, out to distance $d$. (Line 8)
\end{itemize}

These conditions are formalized in Figure~\ref{satisfaction-relation}. 

Notice that the satisfaction relation \emph{does not require} the
contents of references without setters to realize the expected values
for those references. Similarly, the clause of the satisfaction
relation for the update list \term{i} is imprecise --- it only
requires a \emph{subset} of the state-accessing cells. This is a
deliberate design decision: the reason we make this choice is to let
incomplete networks be extensions of complete ones. This lets us use
our logical relation to say something the behavior of programs which
run in incomplete dataflow networks. This enables us to write programs
which evaluate some cells while building another part of a dataflow
network.

As we will see, the way that we reconcile this with our desire to say
that closed programs always build complete, closed networks, is to
require that every cell and function in the relation must always
``make things better''. That is, we never admit values into our
relation which increase the number of references without setters, or
which increase the number of stateful references which do not appear
in the update list \term{i}. Then, since we start a program in a
complete dataflow graph, we can only procede to complete graphs.

Having described the extension ordering for relies, and the
satisfaction relation, we can now describe the extension ordering for
memories (i.e., pairs of a state formula and a rely). This is given in
Figure~\ref{memory-orderings}. On line 1, we introduce the set
$\Mem{d}$, which are just pairs of formulas and relies in the
satisfaction relation. Then, on lines 2-7, we describe what the
ordering for valid memories is. First, the relies must lie in the rely
extension ordering, and then the formulas must satisfy a number of
extra conditions. First, anything ready in the smaller state must
remain ready in the larger one. Second, all of the code in the cells
must be the same in the smaller memory and its extension.  Finally,
any reference which does not have a defined writer must be unchanged
(be the same physical program value) in the smaller and larger states.

Then, on lines 8-10, we describe the \emph{temporal} ordering of
states.  The idea is that the Kripke ordering seen so far describes
how a dataflow graph can change \emph{during} a time step, and the
temporal ordering describes how it changes \emph{upon} a tick of the
clock. The idea is that to advance time one step, we update the clock
to make it invalid. This propagates a wave of invalidations throughout
the dataflow network, leaving it ready to compute the values of the
next time step. Then, two memories are in the relation
$\futurestate{d}{n}{\mu'}{\mu}$ when $\mu'$ is a state that could lie
$n$ steps in the future of $\mu$. What this means is that if $n = 0$,
then $\mu'$ and $\mu$ merely need to be in the Kripke order (line 9).
However, if $n = m + 1$, then there needs to be a state $\mu_0$ which
(a) larger is in the Kripke order with respect to $\mu$, and whose
tail is $n$ steps away in time from $\mu'$. Here, $\tail{\theta, R}$
is $(U(\term{clock}, \theta), \tail{R})$, in accordance with the idea
that the event loop updates the clock to propagate this notification
out to the rest of the flow graph. Finally, one additional condition
we impose on this order is that we only tick on \emph{complete}
memories (i.e., ones in which all the references have getters and
setters). 

Now, we can finally describe the $\Stream^d_A(\term{v}, (\theta, R))$
clause on line 8 of Figure~\ref{satisfaction-relation}. It says that
for any $n$ less than the log of $1/(2 \times d)$, reading the stream
cell should return the $n$-th value of the stream. (Or, if the stream
is delayed, it should return $\None$ on the first timestep, and the
$n-1$st value of the stream at subsequent times.) It does this by
appeal to the $\Head^d_A(\term{v}, \mu)$ predicate, defined on lines
12-18. This predicate says that if we have a heap implementing the
dataflow network in $\theta$, then reading it should return the
appropriate value --- either the head of the stream, or $\None$,
depending on whether the stream is lagged or not. Regardless, the
updated cells in the network should either be in $\Delta(\term{v})$ or
new cells (this is the meaning of the $\Update$ predicate, defined on
27-29 of Figure~\ref{logical-relation}). Furthermore, reading \term{v}
should change neither the set of references lacking getter or setters,
nor the set of cells touching mutable state but not appearing in
\term{i} (this is the $\Stable$ predicate, defined on lines 30-38 of 
Figure~\ref{logical-relation}). 



\subsection{Theorems}

\begin{prop}{(Kripke Monotonicity)}
We have that for all $d' \geq d$
\begin{enumerate}
\item If $U^d_X(v, \term{v})$ then $U^{d'}_X(v, \term{v})$. 
\item If $V^d_A(v, \term{v}, \mu, \sigma)$ then $V^{d'}_A(v, \term{v}, \mu, \sigma)$. 
\item If $\satisfy{\mu}{d}$ then $\satisfy{\mu}{d'}$. 
\item If $V^d_A(v, \term{v}, \mu, \sigma)$ and $\betterstate{d}{\mu'}{\mu}$, then $V^d_A(v, \term{v}, \mu',\sigma)$.
\item If $V^d_A(v, \term{v}, \mu, \sigma)$ and $\sigma' \sqsupseteq \sigma$, then $V^d_A(v, \term{v}, \mu,\sigma')$.
\end{enumerate}
\end{prop}

\begin{lemma}{(Approximation Lemma)}
\begin{enumerate}
  \item If $\forall d' > d.\; U^{d'}_A(v, \term{v})$ then $U^d_A(v, \term{v})$. 
  \item If $\forall d' > d.\; V^{d'}_A(v, \term{v}, \mu, \sigma)$ then $V^d_A(v, \term{v}, \mu, \sigma)$. 
  \item If $\forall d' > d.\; \satisfy{\mu}{d'}$ then $\satisfy{\mu}{d}$. 
\end{enumerate}
\end{lemma}

\begin{lemma}{(Induction Lemma)}
\begin{enumerate}
  \item If $\forall d' > 2\cdot d.\; U^{d'}_A(v, \term{v}) \implies U^{d'/2}_A(v, \term{v})$ then $U^d_A(v, \term{v})$. 
  \item If $\forall d' > 2\cdot d.\; V^{d'}_A(v, \term{v}, \mu) \implies V^{d'}_A(v, \term{v}, \mu)$ then $V^d_A(v, \term{v}, \mu)$. 
  \item If $\forall d' > 2\cdot d.\; \satisfy{\mu}{d'} \implies \satisfy{\mu}{d'/2}$ then $\satisfy{\mu}{d}$. 
\end{enumerate}
\end{lemma}

In the following lemmas, we use the notation $[f|x:v]$ to denote the function which extends $f$'s domain 
by $x$ and gives it the value $v$ at that point. 

\begin{lemma}{(Reference Allocation)}
Suppose that $\satisfy{(\theta, R)}{d}$. Then suppose that $\theta' =
\theta \otimes \mathsf{ref}(\term{r,v})$.  Then for $R' \sqsupseteq R$
such that $R'$ is the same as $R$, except that $L_{R'} = L_R$ and 
$V_{R'} = [V_R|\term{r}:vs]$ for some $vs$, we have that 
$\satisfyext{(\theta', R')}{d}$.
\end{lemma}


\begin{lemma}{(Cell Allocation)} \\
Suppose that $\satisfy{(\theta, R)}{d}$ and
$\satisfyext{(\theta,R)}{d}$ and let $\theta' = \theta \otimes
\cellminus{\term{c}}{\term{code}}$. Further suppose we have $R'
\sqsupseteq R$ such that $C_{R'} = C_R \cup \setof{\term{c}}$, $L_{R'}
= L_R$, $G_{R'} = [G_R | \term{r_i : c}]$ for some set of references
indexed by $I$, and $S_{R'} = [G_R | \term{r_j : c}]$ for some set of
references indexed by $J$.

Then if we can show that for all $d' > 2 \cdot d$,
$\satisfy{(\theta',R')}{d'}$ implies $\Stream^{d'/2}_A(\term{c},
(\theta', R'))$, we can conclude that $\satisfy{(\theta',R')}{d}$. 
\end{lemma}



\begin{figure}
\begin{tabbingspec}
$U^d_1(\unitval, \unitval) = $ true 
\Newline[1em]

$U^d_{X \times Y}((x,y), (\term{x}, \term{y})) = U^d_X(x, \term{x}) \land U^d_Y(y, \term{y})$ 
\Newline[1em]

$U^d_{X \To Y}(f, \term{f}) = \forall d' > 2\cdot d, v, \term{v}.\; 
    U^{d'}_X(v, \term{v}) \implies U^{d'}_Y(f\;v, \term{f\;v})$ 
\Newline[1em]

$U^d_{X \shrink Y}(f, \term{f}) = \forall d' > 2\cdot d, v, \term{v}.\; 
    U^{d'}_X(v, \term{v}) \implies U^{d'}_Y(f\;v, \term{f\;v})$ 
\Newline[1em]

$U^d_{A^\omega}(vs, \term{code}) = $ \Newline
\;\;\= $\forall d' > 2\cdot d, \mu \in \Mem{d'}.\;\Build^{d'}_A(vs, \term{code}, \mu, \emptyset, \U)$ 
\Newline[1em]

$V^d_I(\unitval, \unitval, \mu, \sigma) = $ true
\Newline[1em]

$V^d_{A \otimes B}((a,b), \term{(a,b)}, \mu, \sigma) = V^d_A(a, \term{a}, \mu, \sigma) \land V^d_B(b, \term{b}, \mu, \sigma)$
\Newline[1em]

$V^d_{A \lolli B}(f, \term{f}, \mu, \sigma) = $ \Newline
\> $\forall d' > 2\cdot d, \betterstate{d'}{\mu'}{\mu}, \sigma' \supseteq \sigma, v, \term{v}$ \Newline
\> \;\;\=$V^{d'}_{S(A)}(vs, \term{v}, \mu', \sigma') \implies $ \Newline
\>     \> \;\;$\exists L \sqsupseteq D_R(\term{v}).\; \Build^{d'}_{B}(f\;vs, \term{f\;v}, \theta', \sigma', L)$
\Newline[1em]

$V^d_{A \lollishrink B}(f, \term{f}, \mu, \sigma) = $ \Newline
\> $\forall d' > 2\cdot d, \betterstate{d'}{\mu'}{\mu}, \sigma' \supseteq \sigma, v, \term{v}.$ \Newline
\> \> $V^{d'}_{S(A)}(vs, \term{v}, \mu', \sigma') \implies 
      \Build^{d'}_{B}(f\;vs, \term{f\;v}, \mu', \sigma', \U)$
\Newline[1em]

$V^d_{\valtype{X}}(v, \term{v}, \mu, \sigma) = U^d_{X}(v, \term{v})$ 
\Newline[1em]

$V^d_{S(A)}(vs, \term{v}, (\theta,R), \sigma) = V_R(\term{v}) = vs \land \Delta_R(\term{v}) \subseteq \sigma$ 
\Newline[1em]

$\Build^d_A(vs, \term{code}, (\theta,R) \;\mathrm{as}\;\mu, \sigma, L) = $ \Newline
\> $\{H(\theta) \land \satisfyext{(\theta,R)}{d} \}$ \Newline
\> $\term{code}$ \Newline
\> $\{(\term{a},\_).\;\exists$\=$\betterstate{d}{(\theta,R')\;\mathrm{as}\;\mu'}{\mu}, u.\; H(\theta') \land \term{a} \in C_{R'} - C_R \;\land$ \Newline
\> \> $\NewStream^d_A(vs, \term{a}, \mu', \sigma, L) \;\land$ \Newline
\> \> $\Update(u, \mu, \mu', \term{a}) \;\land$ \Newline
\> \> $\Stable(\mu, \mu')\}$
\Newline[1em]

$\NewStream^d_A(vs, \term{a}, (\theta, R), \sigma, L) = $\Newline
\> $D_{R}(\term{a}) = L \land \Delta_{R}(\term{a}) \subseteq \sigma \land 
    V^d_{S(A)}(vs, \term{a}, (\theta,R), \Delta_{R}(\term{a}))$
\Newline[1em]

$\Update(u, (\theta, R), (\theta', R'), \term{a}) = $ \Newline
\> $\forall \term{c}.\;\ready{\theta'}{c}{-} \land \unready{\theta}{c} \iff c \in u \;\land$ \Newline
\> $\forall \term{c} \in u.\; \term{c} \in \Delta_R(\term{a}) \vee \term{c} \in (C_{R'} - C_R)$ 
\Newline[1em]

$\Stable(\mu, \mu') = \StableRef(\mu, \mu') \land \StableImp(\mu, \mu')$ \Newline[1em]

$\StableRef((\theta,R), (\theta'R')) = $ \Newline
\> $\forall \term{r} \in L_{R'}.\; S_{R'}(\term{r}) \mbox{ undef} \iff \term{r} \in L_R \land  S_{R}(\term{r}) \mbox{ undef}$ and \Newline
\> $\forall \term{r} \in L_{R'}.\; G_{R'}(\term{r}) \mbox{ undef} \iff \term{r} \in L_R \land  G_{R}(\term{r}) \mbox{ undef}$ and \Newline
\> $\forall \term{r} \in L_{R} - \dom{G_R}.\;\exists \term{v}.\;\hasref{\theta}{\term{r}}{\term{v}} \land \hasref{\theta'}{\term{r}}{\term{v}}$
\Newline[1em]

$\StableImp((\theta, R), (\theta',R'))\} = $ \Newline
\> $\forall I,I'.\;\hasref{\theta}{\term{i}}{I} \land \hasref{\theta}{\term{i}}{I'} \implies $ \Newline
\>\;\;\= $[I' - S^{-1}_{R'}(L_{R'}) - G^{-1}_{R}(L_{R})] = [I - S^{-1}_{R}(L_{R}) - G^{-1}_{R}(L_{R})]$ \Newline
\> \> and $\forall \term{c} \in I'.\; \unready{\theta'}{\term{c}} \implies \term{c} \in I \land \unready{\theta}{c}$

\end{tabbingspec}
\caption{The Logical Relation}
\label{logical-relation}  
\end{figure}

\begin{figure}
\begin{tabbingspec}
$\satisfy{(\theta,R)}{d} \triangleq$ \Newline
\;\;\=$\cells{\theta} = C_R$ and \Newline
    \>$\refs{\theta} = L_R$ and \Newline
    \>$\forall \term{c}:A \in C_R.\;$\=$\unready{\theta}{\term{c}} \vee \exists \term{v}.\; $\=$\ready{\theta}{\term{c}}{\term{v}}$ and \Newline
    \>$\forall \term{c} \in G_R(L_R) \cup S_R(L_R).$ \Newline
    \> \> $\ready{\theta}{\term{c}}{-} \implies \term{clock} \in \mathsf{deps}(\theta, \term{c})$ and \Newline
    \>$\exists I.\;\hasref{\theta}{\term{i}}{I} \land I \subseteq (G_R(L_R) \cup S_R(L_R))$ and \Newline
    \>$\forall \term{c}:A \in \cells{\theta}.\;\Stream^d_A(\term{c}, (\theta, R))$ and \Newline
    \>$\forall \term{r}:A \in L_R.\; \Local^d_A(\term{r}, (\theta, R))$ 
\Newline[1em]

$\Stream^d_A(\term{v}, \mu) = $ \Newline
\> $\forall d' > 2\cdot d, n \leq \log(1/d'), \futurestate{d'}{n}{\mu'}{\mu}.\; \Head^{d'\cdot 2^n}_A(\term{v}, \mu')$ \Newline[1em]

$\Head^d_A(\term{v}, (\theta,R)\;\mathrm{as}\;\mu) = $\Newline
\> $\{H(\theta) \land \satisfyext{\mu}{d}\} $\Newline
\> \term{read\;v} \Newline
\> $\{(\term{a},\_).\;\exists$\=$\betterstate{d}{(\theta', R')\;\mathrm{as}\;\mu'}{\mu}, u.\; H(\theta') \;\land$ \Newline
\> \> $\Ready^d_A(\term{v}, \term{a}, \mu') \;\land$ \Newline
\> \> $\Update(u, \mu, \mu', \term{v}) \;\land$ \Newline 
\> \> $\Stable(\mu, \mu') \}$ 
\Newline[1em]

$\Local^d_A(\term{r}, (\theta, R)) = $ \Newline
\> $\forall d' > 2\cdot d.\; S_R(\term{r})$ defined $\implies$ \Newline
\> \;\;\= $\unready{\theta}{S_R(\term{r})} \implies$ \Newline
\> \>\;\;\= $\Ref^{d'}_A(\head{V^R_L(r)}, \term{r}, \mu, \Delta_R(S_R(\term{r})))$ and \Newline
\> \> $\ready{\theta}{S_R(\term{r})}{-} \implies $\Newline
\> \> \> $\Ref^{d'}_A(\head{\tail{V^R_L(r)}}, \term{r}, \mu, \Delta_R(S_R(\term{r})))$ \Newline
\> \> \> $\;\land\; \exists \term{v}.\;\ready{\theta}{G_R(\term{r})}{\term{v}})$
\Newline[1em]


$\Ref^d_A(v, \term{r}, (\theta,R), \sigma) = $ \Newline
\> $\exists \term{v}.\;\hasref{\theta}{\term{r}}{\term{v}} \land \Opt^{d'}_A(v, \term{v}, (\theta,R), \sigma)$ 
\Newline[1em]

$\Ready^d_A(\term{c}, \term{v}, (\theta,R)) = $ \Newline
\> $\Opt^{d}_A(\mbox{if } D_R(\term{v}) \mbox{ then } \None \mbox{ else } \Some{V_R(\term{v})}, 
              \term{a}, (\theta,R), \Delta_R(\term{c}))$ 
\Newline[1em]


$\Opt^d_A(v, \term{v}, \mu, \sigma) = $ \Newline
\> $(v = \None \land \term{v} = \None) \;\vee$ \Newline
\> $(\exists v', \term{v'}.\; v = \Some{v'} \land \term{v} = \Some{\term{v'}} \land V^d_A(v', \term{v'}, \mu, \sigma))$ 
\Newline[1em]

$\satisfyext{(\theta,R)}{d} = $ \Newline
\> $\forall \term{r}:A \in L_R.\; S_R(\term{r}) \mbox{ undef} \implies \Ref^d_A(\head{V^L_R(\term{r})}, \term{r}, (\theta,R))$
\end{tabbingspec}
\caption{The Satisfaction Relation}
\label{satisfaction-relation}
\end{figure}

\begin{figure}
\begin{tabbingspec}
$\Mem{d} = \comprehend{(\theta,R)}{\satisfy{(\theta,R)}{d}}$ 
\Newline[1em]

$(\sqsupseteq^d) \subseteq \Mem{d} \times \Mem{d}$ \Newline[0.2em]

$\betterstate{d}{(\theta',R')}{(\theta,R)}$ iff \Newline
\;\;\= $R' \sqsupseteq R$ and \Newline
\> $\forall \term{c}:A \in C_R, \term{v}.\; \ready{\theta}{\term{c}}{\term{v}} \implies \ready{\theta'}{\term{c}}{\term{v}}$ and \Newline
\> $\forall \term{c}:A \in C_R, \term{code}.\; \mathsf{code}(\theta, \term{c}, \term{code}) \implies \mathsf{code}(\theta', \term{c}, \term{code})$ and \Newline
\> $\forall \term{r}\in E_R, \term{v}.\; \mathsf{ref}(\theta, \term{r}, \term{v}) \iff 
                                         \mathsf{ref}(\theta', \term{r}, \term{v}) $
\Newline[1em]

$\futurestate{d}{0}{\mu'}{\mu} \;\;\;\,= \betterstate{d}{\mu'}{\mu}$ \Newline
$\futurestate{d}{n+1}{\mu'}{\mu} = $ \Newline
\> $\exists \mu_0 \in \Mem{d}.\; \betterstate{d}{\mu_0}{\mu} \land \complete{\mu_0} \land \futurestate{2\cdot d}{n}{\mu'}{\tail{\mu_0}}$ 
\Newline[1em]

$\complete{\theta,R} = $ \Newline
\> $\forall \term{r} \in L_R. \exists \term{c} \in C_R.\; S_R(\term{r}) = \term{c} \land \exists \term{v}.\;\ready{\theta}{\term{c}}{\term{v}}$ 


\end{tabbingspec}
\caption{Orderings on Memories}
\label{memory-orderings}
\end{figure}

\section{Related Work}

In our work, we use ultrametric spaces to give semantics to streamt
ransformers. In the special case of functions from streams to
streams, causality and nonexpansiveness precisely coincide, but due to
cartesian closure function types at all orders are well-defined.
Furthermore, to support efficient implementation, we also need to make
use of the co-Kleisli category over the stream comonad, which we 
connect to the base category via an adjunction. 

There are four main strands of related work: synchronous dataflow
languages, purely functional reactive programming systems, imperative
FRP systems, and metric methods in denotational semantics.

The family of synchronous dataflow languages (such as
Esterel~\cite{esterel}, Lustre~\cite{lustre}, and Lucid
Synchrone~\cite{synchrone}) are languages based on a model of
synchronous time. The discrete ultrametric semantics we use is related
to these systems, most especially to Lucid Synchrone (which supports
higher-order functions).

Pouzet and Caspi~\cite{coiterative} extended synchronous dataflow
programming to higher order with their co-iterative semantics. They
illustrated how that this generated a Cartesian closed category (of
size-preserving functions), which they used to interpret
functions. Uustalu and Vene~\cite{essence-dataflow} subsequently
observed that size-preserving functions could be understood more
abstractly as the co-Kleisli category of streams. However, in both of
these works, feedback was handled in a somewhat \emph{ad hoc} fashion.

The proper treatment of this issue is delicate, and disentangling the
two main pieces of it kept us busy for a long time. First, we use
ultrametrics to give a semantic criterion for causality, which permits
us to avoid explicitly looking at the syntax of a program to identify
dependencies. This is essential for the smooth treatment of
higher-order functions, since the dependencies may not be statically
apparent in this case.

Second, we needed to make explicit use of the adjunction between the
base category of ultrametric spaces and the co-Kleisli category of
streams. The reason is that taking the fixed point of (the coextension
of) a contractive function $A \lollishrink A$ in the co-Kleisli
category yields a \emph{stream} of values. Essentially, feedback lets
us take a function and turn it into a stream, but since we are
receiving a stream of functions as an input, the natural fixed point
operator will yield a new stream at each timestep.  However, there are
no maps in the co-Kleisli category which take a stream of streams and
fix the outputs as the successive elements of the stream at a
particular time --- and this is precisely what we need to do enumerate
the elements of a fixed point over time.

This is why we have two categories -- the operations we need for
reative programming live naturally in both. This permits us to
preserve the equational theory of the programming language:
programmers can reasoning using full rules for $\beta\eta$-equality,
as well as the equations for fixed points. We do not need to
compromise on our reasoning principles in any way, which is quite
remarkable given the low-level, imperative nature of our
implementation.

Functional reactive programming was introduced by Eliot and
Hudak~\cite{fran}, and was given a semantics in terms of event streams
and unrestricted functions over them. In this and subsequent
work~\cite{courtney-thesis}, the semantics of fixed points were given
denotationally. This gives semantics to all FRP expressions, including
non-well-founded programs (which will go into infinite loops). 

One notable feature of this line work is a treatment of continuous
time. We believe that our proof framework should extend to proving an
sampling theorem as in Wan and Hudak~\cite{frp-first-principles}.  On
the semantic side, continuous streams can be modelled as functions
$\mathbb{R} \to A$, and the causal ultrametric extends naturally to
this case. On the implementation side, the \term{clock} can supply
time deltas (in contrast to its current delivery of pure ticks).

Due to the problem of space leaks, arrowized FRP~\cite{arrowized-frp}
was introduced in order to restrict the set of definable stream
transformers to the the causal ones. The restriction to arrows is
roughly equivalent to first-order functional programming, though
Nilsson~\emph{et al.}  introduced additional combinators to recover
higher-order and dynamic behavior. Our semantics gives a way of
eliminating these restrictions and admitting higher-order and dynamic
behavior in a very uniform way.

Metric methods were introduced into semantics in the late
1970s~\cite{nivat} and early 1980s, in order to simplify the
denotational semantics of
concurrency~\cite{concurrency-semantics}. The applications to stream
programming were recognized early, but not followed up on: in a
surprisingly little-cited 1985 paper~\cite{metric-dataflow}, de Bakker
and Kok proposed an ultrametric semantics for a language of
first-order stream programs over integers. In their paper, they wrote
``We think there are no problems when we allow functions of higher
order[\ldots]''.  This is a conjecture which we have attended to, a
full quarter-century later: we can confirm that it turns out to be
true, but only if we make it true twice over!

More recently, Birkedal and his coworkers~\cite{birkedal-ultrametrics}
have used ultrametric models to give semantics to sequential programs
involving advanced features such as higher-order state and
polymorphism, and has suggested connections between these ideas and
the more operationally-flavored technique of step-indexed
models~\cite{appel-mcallester}. Our logical relation can be seen as
similar to these models, only ``one level down'' -- we define
\emph{values} as fixed points of metric spaces, whereas Birkedal
\emph{et al.} define \emph{types} as fixed points of metric spaces.

There is also a fascinating and suggestive paper by
Escardo~\cite{escardo-metric}, who observed that metric models seem to
correspond to PCF extended with timeout operators. Since cancels and
interrupt operations pervade interactive programs, this suggests we
should investigate whether they can be supported without harming the
reasoning principles of the language.

\appendix

\section{Implementation}

We give the implementation of the code for the dynamic lambda calculus
in Figure~\ref{cokleisli-implementation}, and the implementation of the
program for the static lambda calculus in Figure~\ref{ultrametric-implementation}. 

The implementation is relatively straightforward, with two exceptions:
the implementation of the \term{zip} operation. Given two stream
cells, it returns a cell which pairs the successive elements of its
two inputs. This is a tricky function to verify, since the function
must work with lagged inputs, and we may receive a pair of inputs in
which one component is lagged and the other not. However, our
mathematical specification does not mention delays in it at all. So
even if one input yields $(a_0, a_1, a_2, \ldots)$ and the other
yields $(\None, b_0, b_1, b_2, \ldots)$, the programmer is still
entitled to assume that $a_0$ is paired with $b_0$, and that $a_1$ is
paired with $b_1$, and so on.

To implement this, \term{zip} tests the two inputs, and introduces an
artificial delay, if one cell is delayed and the other is not.
Otherwise, it simply returns a cell which performs the pairing. Since
implementing a delay uses auxilliary state, we need to register the
cell --- but we only \term{register} the cell in the case it needs the
state. This reduces the number of cells that will get forced at the
end of each trip through the event loop, and so lets the dataflow
graph remain as lazy as we can manage. 

Secondly, the implementation of \term{fix} is quite subtle. It works
takes a stream of functions, and returns a stream of streams.

Our actual implementation does some further optimizations not visible
in this source. In particular, we exploit the isomorphism $S(A \times
B) \simeq S(A) \times S(B)$ to maintain the context in product form.
This permits us to avoid excessive conservatism in managing dependencies.

\begin{figure}
\begin{tabbing}
\term{id = \lambda xs.\;cell(read\;xs)} 
\\[1em]

\term{compose\;f\;g =\lambda as.\; do\;}
 \=\term{bs \leftarrow f(as);} \\
 \>\term{cs \leftarrow g(bs);} \\
 \>\term{return(cs)} 
\\[1em]

\term{one\;xs = cell(return(\Some{\unitval}))}
\\[1em]

\term{pair\;f\;g = \lambda as.\;do\;}
  \=\term{bs \leftarrow f(as);} \\
  \>\term{cs \leftarrow g(as);} \\
  \>\term{zip(bs,cs)}
\\[1em]

\term{fst = \lambda abs.\;cell(do}
  \= \term{ab' \leftarrow read(abs);} \\
  \>\term{case \; ab'\; of} \\
  \>\term{\;\None \to return(\None)} \\
  \>\term{\;\Some{a,b} \to return(\Some{a}))} 
\\[1em]


\term{snd = \lambda abs.\;cell(do}
  \= \term{ab' \leftarrow read(abs);} \\
  \>\term{case \; ab'\; of} \\
  \>\term{\;\None \to return(\None)} \\
  \>\term{\;\Some{a,b} \to return(\Some{b}))} 
\\[1em]

\term{eval = \lambda fas.\;do}\;
              \=\term{fs \leftarrow \;fst(fas);}\\
              \>\term{as \leftarrow \;snd(fas);}\\
              \>\term{cell(do\;}\=\term{f' \leftarrow read(fs)}\\
              \>                  \>\term{case\;f'\;of}\\
              \>                  \>\term{\;\None \to return\;\None} \\
              \>                  \>\term{\;\Some{f} \to do\;}\=\term{bs \leftarrow f(as);} \\
              \>                  \>                            \>$\term{read(bs)})$ 
\\[1em]

\term{curry\;f =\lambda as.\;cell(\mathsf{Some}(\lambda bs.\;do\;}
  \=\term{abs \leftarrow zip(as,bs);}\\
  \>\term{f(abs)))}
\end{tabbing}
\caption{The Implementation of the co-Kleisli Category}
\label{cokleisli-implementation}
\end{figure}

\begin{figure}
\begin{tabbing}
\term{zip(as, bs) =} \\
\;\;\= \term{do\;}\=\term{a' \leftarrow read(as);} \\
\>              \>\term{b' \leftarrow read(bs);} \\
\>              \>\term{case \;(a',b')\;of}\\ 
\>           \>\;\;\=\term{(\None, \None)} \\
\>           \>    \>\term{(\Some{\_}, \Some{\_}) \to} \\
\>           \>    \> \qquad \term{cell(do\;}\=\term{a' \leftarrow read(as);} \\
\>           \>    \>                      \>\term{b' \leftarrow read(bs);} \\
\>           \>    \>                      \>\term{case \;(a',b')\; of} \\
\>           \>    \>                      \>\;\;\=\term{(\Some{a}, \Some{b}) \to return(\Some{(a,b)})} \\
\>           \>    \>                      \>\;\;\=\term{(\_, \_) \to return(\None))} \\
\>           \>    \>\term{(\None, \Some{\_}) \to}\\
\>           \>    \> \qquad\term{do\;}\=
                                      \term{r \leftarrow ref(\None);} \\
\>           \>    \>               \>\term{abs \leftarrow cell(do\;}\=\term{a \leftarrow read(as);} \\
\>           \>    \>               \>                               \>\term{new \leftarrow read(bs);} \\
\>           \>    \>               \>                               \>\term{old \leftarrow !r;}\\
\>           \>    \>               \>                               \>\term{r := new;}\\
\>           \>    \>               \>                               \>\term{return(a,old));}\\
\>           \>    \>               \>\term{register(abs);} \\
\>           \>    \>               \>\term{return(abs)} \\
\>           \>    \>\term{(\Some{\_}, \None) \to}\\
\>           \>    \> \qquad\term{do\;}\=
                                      \term{r \leftarrow ref(\None);} \\
\>           \>    \>               \>\term{abs \leftarrow cell(do\;}\=\term{b \leftarrow read(bs);} \\
\>           \>    \>               \>                               \>\term{new \leftarrow read(as);} \\
\>           \>    \>               \>                               \>\term{old \leftarrow !r;}\\
\>           \>    \>               \>                               \>\term{r := new;}\\
\>           \>    \>               \>                               \>\term{return(old,b));}\\
\>           \>    \>               \>\term{register(abs);} \\
\>           \>    \>               \>\term{return(abs)} 
\\[1em]

\term{register(xs) = do\;}\=\term{dummy \leftarrow read(xs);}\\
                          \>\term{lst \leftarrow !i;} \\
                          \>\term{i := pack(xs) :: lst} 
\end{tabbing}
\caption{Utility Functions for the co-Kleisli Code}
\label{cokleisli-util}
\end{figure}

\begin{figure}
\begin{tabbing}
$\term{fix} : (A \lollishrink A) \lolli S(A)$ \\
\term{fix = \lambda fs.\;cell(do}
  \=\term{f' \leftarrow read(fs);} \\
  \>\term{case\;f'\;of}\\
  \>\;\;\=\term{\None \to return(\None)}\\
  \>    \>\term{Some(f) \to} \\
  \>    \>\;\;\term{do} \=\term{r \leftarrow ref(\None);}\\
  \>    \>  \>\term{input \leftarrow cell(do\;}\=\term{() \leftarrow clock;}\\
  \>    \>  \>                                 \>\term{v \leftarrow !r;}\\
  \>    \>  \>                                 \>\term{return(v));}\\
  \>    \>  \>\term{pre \leftarrow f(input);}\\
  \>    \>  \>\term{out \leftarrow cell(do\;}\=\term{\_ \leftarrow read(input);} \\
  \>    \>  \>                               \>\term{v \leftarrow read(pre);}\\
  \>    \>  \>                               \>\term{r := v;}\\
  \>    \>  \>                               \>\term{return(v));}\\
  \>    \>  \>\term{register(out);}\\
  \>    \>  \>\term{return(\Some{out})})
\end{tabbing}
\caption{Implementing Fixed Points in the co-Kleisli Category}  
\label{cokleisli-implementation-2}
\end{figure}


\begin{figure}
\begin{tabbing}

\term{id = \fun{x}{x}} 
\\[1em]
\term{compose\;f\;g = \fun{x}{g(f(x))}}
\\[1em]
\term{one = \fun{x}{\unitval}}
\\[1em]
\term{fst = \fun{(x,y)}{x}}
\\[1em]
\term{snd = \fun{(x,y)}{y}}
\\[1em]
\term{pair\;f\;g = \fun{x}{(f(x),g(x))}}
\\[1em]
\term{eval = \fun{(f,x)}{f(x)}}
\\[1em]
\term{curry\;f = \fun{x}{\fun{y}{f(x,y)}}}
\\[1em]
\term{cons = \lambda x.\; cell(return(\Some{f}))} \\[0.1em]
with 
\term{f = \lambda ys.\;do} \= \term{r \leftarrow \Some{x};} \\
\>     \term{zs \leftarrow cell(}\= 
      \term{do} \=\term{old \leftarrow !r;} \\
\> \> \> \term{new \leftarrow read(ys);} \\
\> \> \> \term{case\;old\;of} \\
\> \> \> \;\;\= \term{\None \to return(new)} \\
\> \> \> \>     \term{\Some{\_} \to do} \=\term{r := new;} \\
\> \> \> \>  \>                           \term{return(old));} \\                 
\> \term{register(zs);} \\
\> \term{return(zs)}\\


\\
\term{head\;thunk =} \\
\;\;\term{do\;}
  \=\term{xss \leftarrow thunk;} \\
  \>\term{xs' \leftarrow read(xss);} \\
  \>\term{case\;xs'\;of} \\
  \>\;\;\=\term{\Some{xs} \to return(xs)}\\
  \>    \>\term{\None \to ERROR} (invariant ensures this case cannot happen)
\end{tabbing}
\caption{The Implementation of the Ultrametric Category}
\label{ultrametric-implementation}
\end{figure}

\begin{figure}
\begin{tabbing}
$(-)^\omega : U^S(A,B) \to U(A^\omega, B^\omega)$ \\
\term{omega\;f = \lambda athunk.\;do} \=\term{as \leftarrow athunk;} \\
                                      \>\term{bs \leftarrow f(as);} \\
                                      \>\term{return(bs)} 
\\[1em]

$\mathit{Value} : U(X,Y) \to U^S(\valtype{X}, \valtype{Y})$ \\[0.2em]
\term{value\;f = \lambda xs.\;cell(do} \=\term{x' \leftarrow read(xs);}\\
                                       \>\term{case\;x'\;of}\\
                                       \>\;\;\=\term{\None \to return(\None)} \\
                                       \>\>    \term{\Some{x} \to return(\Some{f(x)}))}

  
\end{tabbing}
\caption{Implementing the Adjunction}
\label{adjoint-implementation}  
\end{figure}


\acks

Acknowledgments, if needed.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\bibliography{popl11-frpccc}


\end{document}
