%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[preprint]{sigplanconf}

% The following \documentclass options may be useful:
%
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathpartir}
\usepackage{verbatim}

\newcommand{\ultrametric}{\mathbb{U}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\setof}[1]{\left\{{#1}\right\}}
\newcommand{\comprehend}[2]{\setof{{#1}\;|\;{#2}}}
\newcommand{\betterstate}[3]{{#2}\, {\sqsupseteq}^{#1} {#3}}
\newcommand{\futurestate}[4]{{#3} \gg^{#1}_{#2} {#4}}
\newcommand{\worsestate}[5]{{#4} {\sqsubseteq}^{(#1,#2)}_{#3} {#5}}
% \newcommand{\satisfies}[4]{({#1}, {#2}, {#3}) \;\mathsf{sat}\; {#4}}
\newcommand{\dom}[1]{\mathrm{dom}({#1})}

\newcommand{\ready}[3]{\mathsf{ready}({#1}, {#2}, {#3})}
\newcommand{\unready}[2]{\mathsf{unready}({#1}, {#2})}
\newcommand{\cells}[1]{\mathrm{cells}({#1})}
\newcommand{\refs}[1]{\mathrm{refs}({#1})}
\newcommand{\hasref}[3]{\mathsf{hasref}({#1}, {#2}, {#3})}
\renewcommand{\implies}{\Rightarrow}

\newcommand{\stream}[1]{\mathbf{#1}}
\newcommand{\term}[1]{\ensuremath{\mathtt{{#1}}}}
\newcommand{\spec}[4]{\setof{{#1}}\;{#2}\;\setof{{#3}.\;{#4}}}

\newcommand{\streams}{\mathit{streams}}
\newcommand{\thunks}{\mathit{thunks}}
\newcommand{\lags}{\mathit{lags}}
\newcommand{\locs}{\mathit{locs}}

\newcommand{\discrete}[1]{D({#1})}
\newcommand{\To}{\Rightarrow}
\newcommand{\shrink}{\rightsquigarrow}
\newcommand{\interp}[1]{[\![{#1}]\!]}
\newcommand{\unitval}{\left<\right>}

\newcommand{\floor}[1]{\left\lfloor{#1}\right\rfloor}

\newcommand{\interps}[1]{[\![{#1}]\!]_s}
\newcommand{\interpu}[1]{[\![{#1}]\!]_u}

\newcommand{\Head}{\mathit{Head}}
\newcommand{\Tail}{\mathit{Tail}}
\newcommand{\Code}{\mathit{Code}}
\newcommand{\Build}{\mathit{Build}}
\newcommand{\Local}{\mathit{Local}}
\newcommand{\Ref}{\mathit{Ref}}
\newcommand{\Opt}{\mathit{Opt}}
\newcommand{\Stream}{\mathit{Stream}}
\newcommand{\Mem}[1]{\mathit{Mem}(#1)}
\newcommand{\Seq}{\mathit{Seq}}
\newcommand{\Update}{\mathit{Update}}
\newcommand{\StableRef}{\mathit{StableRefs}}
\newcommand{\StableImp}{\mathit{StableImps}}
\newcommand{\Stable}{\mathit{Stable}}

\newcommand{\unittype}{\mathsf{unit}}
\newcommand{\celltype}[1]{\mathsf{cell}\;{#1}}
\newcommand{\opttype}[1]{\mathsf{option}\;{#1}}
\newcommand{\reftype}[1]{\mathsf{ref}\;{#1}}
\newcommand{\monad}[1]{\bigcirc{#1}}
\newcommand{\clock}{\mathsf{clock}}
\newcommand{\comp}[1]{\mathsf{code}\;{#1}}
\newcommand{\thunk}[1]{\mathsf{thunk}\;{#1}}
\newcommand{\streamtype}[1]{\mathsf{stream}\;{#1}}
\newcommand{\contracttype}{\mathsf{bool}}
\newcommand{\lolli}{\multimap}
\newcommand{\lollishrink}{-\!\!\!\,\bullet}
\newcommand{\valtype}[1]{\mathsf{value}\;{#1}}
\newcommand{\None}{\mathsf{None}}
\newcommand{\Some}[1]{\mathsf{Some}({#1})}
\newcommand{\stateok}[2]{\mathsf{updateok}({#1}, {#2})}

\newcommand{\counit}{\epsilon}
\newcommand{\tails}{\delta}

\newcommand{\powerset}[1]{\mathcal{P}(#1)}
\newcommand{\cellminus}[2]{\mathsf{cell}^{-}({#1}, {#2})}
\newcommand{\cellplus}[4]{\mathsf{cell}^{+}({#1}, {#2}, {#3}, {#4})}

\newenvironment{proof}[1][(Sketch)]{\noindent \textsc{Proof {#1}} }{}

\newcommand{\judge}[3][\Gamma]{{#1} \vdash {#2} : {#3}}
\newcommand{\judgec}[4][\Gamma]{{#1};{#2} \vdash {#3} : {#4}}
% Contractive commands
\newcommand{\const}[1]{\left<{#1}\right>}
\newcommand{\pair}[2]{({#1}, {#2})}
\newcommand{\fst}[1]{\pi_1{#1}}
\newcommand{\snd}[1]{\pi_2{#1}}
\newcommand{\unit}{()}
\newcommand{\letc}[3]{\mathsf{letc}\;{#1} = {#2}\;\mathsf{in}\;{#3}}
\newcommand{\fun}[2]{\lambda {#1}.\;{#2}}
\newcommand{\sfun}[2]{\hat{\lambda} {#1}.\;{#2}}

\newcommand{\Delays}{\mathbb{D}}
\newcommand{\U}{\mathsf{u}}
\newcommand{\D}{\mathsf{d}}

%
\newcommand{\fixme}[1]{\texttt{FIXME: {#1}}}
\newcommand{\head}[1]{\mathit{head}(#1)}
\newcommand{\tail}[2][]{\mathit{tail}^{#1}(#2)}
\newcommand{\ramify}[1]{\mathsf{U}(\mathtt{clock}, {#1})}

\newcommand{\einvariant}[3]{{#2} \stackrel{#1}{=} {#3}}
\newcommand{\satisfy}[2]{{#1}\;\mathrm{sat}\;{#2}}
\newcommand{\satisfyext}[2]{\mathrm{extsat}({#1}, {#2})}
\newcommand{\complete}[1]{\mathrm{complete}(#1)}

\newcounter{lineno}
\newenvironment{tabbingspec}{\setcounter{lineno}{0}\begin{tabbing}\refstepcounter{lineno}{\small \arabic{lineno}}\;\;\;\=}{\end{tabbing}}
\newcommand{\Newline}[1][0em]{\refstepcounter{lineno}\\[#1]{\small \arabic{lineno}}\>}

\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}

\begin{document}

\conferenceinfo{POPL '11}{date, City.} 
\copyrightyear{2011} 
\copyrightdata{[to be supplied]} 

\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{short description of paper}   % 'preprint' option specified.

\title{An Ultrametric Model of Reactive Programming}
\subtitle{Subtitle Text, if any}

\authorinfo{Neelakantan R. Krishnaswami}
           {Microsoft Research}
           {neelk@microsoft.com}
\authorinfo{Nick Benton}
           {Microsoft Research}
           {nick@microsoft.com}

\maketitle

\begin{abstract}
We describe a model of higher-order functional reactive programming
using ultrametric spaces, which provide a natural Cartesian closed
generalization of causal stream functions. We then show how reactive
programs may be implemented efficiently using an imperatively updated
dataflow graph and prove that this implementation is correct with
respect to the declarative semantics.
\end{abstract}

\category{CR-number}{subcategory}{third-level}

\terms
term1, term2

\keywords
keyword1, keyword2

\section{Introduction}

Typical interactive programs such as web applications, games, and GUIs
are written in a very imperative style. A program models dynamic
behavior by modifying state, and accepting callbacks to modify its own
state. These programs exhibit complex uses of aliasing, tricky control
flow through callback functions living in the heap, and in general are
very difficult to reason about. Part of the difficulty are the
inherent complexity of verifying programs using such powerful
features, but an even more fundamental problem is that it is not
immediately clear even what the semantics of such programs should be
--- and even the most powerful verification techniques are useless
without a specification for a program to meet.

One of the oldest and most successful proposals for giving semantics
to interactive programs is the paradigm of \emph{dataflow programming}. 
The basic idea is that instead of viewing a \fixme{figure out what 
to say!}



The contributions of this work are as follows:

\begin{itemize}
\item We give a new semantic model of functional reactive programming
  based on ultrametric spaces, which builds upon prior work in two
  ways. First, it generalizes existing models by supporting full
  cartesian closed structure, rather than more restrictive structures
  such as arrows, while still respecting important concepts such as
  the causality of stream functions (and in fact extending them
  naturally to higher-order).

  In particular, the use of metric spaces means that we can use
  Banach's contraction map theorem to interpret feedback. This means
  that unlike earlier semantics based on domain models of streams, we
  can restrict our semantics to only cover \emph{total, well-founded}
  stream programs. Furthermore, by using an abstract notion of
  contractiveness instead of an explicit notion of guardedness, our
  semantics lifts easily to model higher-type streams (e.g., streams
  of streams) and recursion at higher type. 

\item Next, we give a domain specific langauge for writing reactive
  programs. Since streams distribute over products and form a comonad,
  the co-Kleisli category of streams is also Cartesian closed, thus
  giving us \emph{two} natural notions of function for reactive
  programs.  Prior work~\cite{coiterative, essence-dataflow} has
  focused primarily on this category, but the interpretation of fixed
  points is significantly less natural in the co-Kleisli category than
  in the base category of ultrametric spaces.

  Instead, we extend Benton and Wadler's adjoint calculus for linear
  logic~\cite{benton-wadler} to give a language for writing reactive
  programs. The idea is to decompose the stream comonad into a pair of
  adjoint functors, which in the term calculus become modal operators
  connecting the two lambda calculi. We can then interpret fixed
  points in the category of ultrametrics (thereby retaining a simple
  equational theory for them) while still enabling programming
  implicitly with streams, as is common in dataflow languages such as
  Synchrone~\cite{synchrone}. Furthermore, we extend the adjoint
  calculus with additional judgements to track contractiveness, so
  that we can use typechecking to ensure that clients can only take
  fixed points of well-defined, strictly-contractive functions.

\item Second, we give a reasonably efficient implementation of this
  semantics in terms of an imperative dataflow graph, and prove the
  correctness of our implementation with respect to the semantics.  

  Furthermore, we believe our correctness proof for our library is of
  independent interest. It integrates techniques from verification ---
  such as separation logic and rely-guarantee -- with techniques from
  semantics, such as step-indexed models and logical relations. The
  upshot is that clients of the library can reason as if it were
  purely a mathematical object, even though it is implemented in terms
  of complex imperative higher-order code. The full suite of equations
  --- $\beta$, $\eta$ and fixed-point equations --- are all sound
  reasoning principles from the perspective of the client, even though
  the implementation is pervasively imperative. 

\item Finally, GUI toolkits often expose an essentially imperative
  interface to certain resources (such as the display). We address the
  question of how to smoothly integrate these kinds of effectful
  operations into our model. 
\end{itemize}

\section{Reactive Programs and Stream Transformers}

Reactive programs are usuall interpreted as \emph{stream
  transformers}. The idea is that a time-varying value of type $X$ can
be viewed as a stream of $X$s, and so a program that takes a
time-varying $X$ and produces a time-varying $Y$ is then a function
that takes a stream of $X$s and produces a stream of $Y$s. Then,
interactive programs can be specified in terms of the usual suite
of function operations on streams. 

\subsection{Causality}

However, the full function space on streams is too generous: there are
stream programs which do not have a sensible interpretation as an
interactive program. For example, consider the following simple example:

\begin{tabbingspec}
\term{profit : Price^\omega \to Order^\omega} \Newline
\term{profit\; prices = } \Newline
\;\;\= \term{let\; prices\_tomorrow = tail(prices) \;in} \Newline
    \> \term{if\; head(prices) < head(prices\_tomorrow)\;then} \Newline
    \> \qquad\= \term{Buy :: profit(tail(prices))} \Newline
    \> \term{else} \Newline
    \> \> \term{Sell :: profit(tail(prices))} 
\end{tabbingspec}

Here, the \term{profit} function receives a stream denoting a stream
of daily stock prices. If all the usual stream functions were
available, it could take the tail of this stream, and then compare the
head of today and tomorrow's stream to determine whether it should buy
or sell. Since compiler writers cannot yet generate code which travels
in time, we need some kind of semantic condition to explain which
stream functions should be ruled out, and which should be ruled in.

To do this, we appeal to the criterion of \emph{causality}~\cite{causality}: 
the output at time $n$ should depend only on the first $n$ inputs. We can
formalize this as follows, using the notation $\floor{xs}_n$ to denote the 
$n$-element prefix of the stream $xs$. 

\begin{definition}{(Causality)}
A stream function $f : X^\omega \to Y^\omega$ is said to be \emph{causal} when,
for all for all $n$ and streams $xs$ and $xs'$, if $\floor{xs}_n = \floor{xs'}_n$
then $\floor{f(xs)}_n = \floor{f(xs')}_n$.   
\end{definition}

This definition rules out the \term{tail} function, since its first
$n$ outputs are determined by its first $n+1$ inputs. For example, the
streams $1, 1, 1, \ldots$ and $1, 2, 3, \ldots$ agree on their first
output, but their tails disagree right away --- and so \term{tail} is
not causal function.

However, while this is an intuitive and appealing definition for
streams of basic types (such as integers or booleans), it is not
immediately clear how to generalize this definition. For example, what
might causality mean if we have a stream of \emph{streams}, or even a
stream of stream functions? 

\subsection{Guardedness and Fixed Points}

Next, we can also want to define streams by feedback or recursion; for
example, we may wish to define an increasing sequence of integers with
an expression like:
\begin{displaymath}
\term{nats = fix(\lambda xs.\; 0 :: map\;succ\;xs)}  
\end{displaymath}
  
The natural question is to ask when fixed points such as this are
well-defined. An operational way of thinking about this question is to
observe that the lambda expression \term{\lambda xs.\; 0 :: map\;succ\;xs} 
can produce its first output without looking at its input. So we can 
imagine implementing the fixed point by taking the output at time $n$ and 
feeding it back in as the input at time $n+1$, relying on the fact that at 
time 0 the input value does not matter.

\begin{definition}{(Guardedness)}
A function $f : X^\omega \to X^\omega$ is said to be \emph{guarded}
when there exists a $k > 0$ such that for all for all $n$ and streams
$xs$ and $xs'$, if $\floor{xs}_n = \floor{xs'}_n$ then
$\floor{f(xs)}_{n+k} = \floor{f(xs')}_{n+k}$.
\end{definition}

Then the following theorem about the existence and uniqueness of fixed
points easily follows: 

\begin{prop}{(Fixed Points of Guarded Functions)}
Every guarded endofunction $f : X^\omega \to X^\omega$ (where $X$ is a
nonempty set) has a unique fixed point.
\end{prop}

As with causality, guardedness is an intuitive and natural property,
but generalizations to higher types seem both useful and unobvious. For
example, we can imagine wanting to write a recursive \emph{function}:
\begin{displaymath}
\term{fib = fix(\lambda f\; \lambda (j,k).\; j :: f(k,j+k))}
\end{displaymath}
So the natural questions to ask are: what does guardedness mean at
higher type, and how can we interpret fixed points at higher type?

To answer these questions, we will use the basic properties of the
theory of metric spaces.

\section{An Ultrametric Model of Reactive Programs}

\subsection{Ultrametric Spaces in a Nutshell}

A complete 1-bounded \emph{ultrametric space} is a pair $(A, d_A)$,
where $A$ is a set and $d_A \in A \times A \to \R$ is a distance
function, satisfying the following four axioms:

\begin{itemize}
\item $d_A(x, y) = 0$ if and only if $x = y$
\item $d_A(x, x') \in [0,1]$
\item $d_A(x, x') = d_A(x', x)$
\item $d_A(x, x') \leq \max(d_A(x, y), d_A(y, x'))$
\end{itemize}

Completeness means that the limit of every Cauchy sequence in $A$
converges to a value in $A$. What distinguishes ultrametric spaces
from ordinary metric spaces is that the triangle inequality is more
stringent; instead of requiring the usual triangle inequality that
$d(x,x')$ be less than or equal to $d_A(x, y) + d_A(y, x')$,
ultrametrics require it to be less than or equal to $\max(d_A(x, y),
d_A(y, x'))$.

When there is no ambiguity, we will write $A$ for a metric space $(A, d_A)$. 

A map $f : A \to B$ between ultrametric spaces is \emph{nonexpansive} when 
it is non-distance-increasing:
\begin{displaymath}
  d_B(f\;x, f\;x') \leq d_A(x, x')
\end{displaymath}

A map $f : A \to B$ between ultrametric spaces is said to be
\emph{strictly contractive} when it shrinks the distance between 
any two points by a nonzero factor:
\begin{displaymath}
  \exists q \in [0,1).\; d_B(f\;x, f\;x') \leq q \cdot d_A(x, x')
\end{displaymath}

Complete 1-bounded ultrametric spaces and nonexpansive maps form a 
Cartesian closed category. The product metric on spaces $A$ and $B$
is given by the cartesian product of the underlying sets and the sup-metric
on pairs:
\begin{displaymath}
  d_{A \times B}((a,b), (a',b')) = \max \setof{d_A(a,a'), d_B(b,b')}
\end{displaymath}

Similarly, the exponential metric is given by the sup-metric over all
inputs:
\begin{displaymath}
  d_{A \to B}((a,b), (a',b')) = \sup \comprehend{d_B(f\;a,f\;a')}{a \in A}
\end{displaymath}

As a notational convenience, here we take $0$ as the formal
maximum of the empty set.

Any set $X$ can be trivially made into an ultrametric space $D(X)$ by
equipping it with the discrete distance function
\begin{displaymath}
d(x,x') = \left\{\begin{array}{ll}
                  0 & \mbox{if } x = x' \\
                  1 & \mbox{if } x \not= x' \\
                \end{array}
          \right.
\end{displaymath}

Given any ultrametric space $A$, we can define the metric space of
streams on $A$ by equipping the set $A^\omega$ with an ultrametric $d$
(which we will call the \emph{causal metric of streams}) as follows:
\begin{displaymath}
  d_{A^\omega}(as, as') = \sup \comprehend{2^{-n}\cdot d_A(as_n, as'_n)}{n \in \N}
\end{displaymath}

\noindent Furthemore, this is a functorial action; given any map $f : A \to B$, 
we can define a corresponding function $f^\omega : A^\omega \to B^\omega$ which
takes identities to identities and commutes with composition, by mapping
$f$ over the elements of the input stream.

The interpretation of the stream metric is easiest to understand in
the case of streams of discrete elements. In this case, the metric
says that two streams are closer, the later the time at which they
first disagree. So two streams which have differing values at time $0$
are at a distance of $1$, whereas two streams which never disagree
will have a distance of $0$ (and hence will be equal streams).

Finally, the one classical theorem about metric spaces which we will
make serious use of is Banach's contraction map theorem.
\begin{prop}{(Banach's Fixed Point Theorem)}
For any nonempty, complete metric space $A$ and strictly contractive
endofunction $f : A \to A$, there exists a unique fixed point of $f$.
\end{prop}

\subsection{From Ultrametrics to Functional Reactive Programs}

Now we can show that for streams of base type, the properties of maps
in the category of ultrametric spaces correspond exactly to the
properties of first-order reactive programs we discussed in the
previous section.

\begin{theorem}{(Causality is Nonexpansiveness)}
Suppose $X$ and $Y$ are sets. Then a function $f : X^\omega \to
Y^\omega$ is causal if and only if it is a nonexpansive function under
the causal metric of streams of elements of the discrete spaces $D(X)$
and $D(Y)$.
\end{theorem}

\begin{theorem}{(Guardedness is Contractiveness)}
Suppose $X$ and $Y$ are sets. Then a function $f : X^\omega \to
Y^\omega$ is guarded if and only if it is a strictly contractive
function under the causal metric of streams of elements of the
discrete spaces $D(X)$ and $D(Y)$.
\end{theorem}

The proof of these two theorems is nothing more than the unwinding of
a few definitions. However, the consequences of are quite dramatic! By
interpreting our programs in the category of ultrametric spaces:
\begin{enumerate}
\item We can interpret tuples and functions (with the full $\beta$ and
  $\eta$ rules) thanks to the Cartesian closure of this category.
\item Since streams are functorial, we can interpret streams of
  streams.
\item Furthermore, contractiveness gives an analogue of guardedness
  that makes sense at all types, and likewise Banach's fixed point
  theorem gives an interpretation of fixed points that makes sense at
  all types.
\end{enumerate}

\subsection{The Co-Kleisli Category of Streams}

However, when we look at synchronous dataflow languages like Lucid
Synchrone~\cite{synchrone}, we see that they do not offer this
API. Instead, in these languages time is implicit, and streams are
only very rarely directly manipulated. So instead of writing a program
like:
\begin{displaymath}
  \term{f = \fun{(xs,ys)}{map\;(+)\;(zip\;(xs,ys))}}
\end{displaymath}
We would instead write the program
\begin{displaymath}
  \term{f = \fun{(x,y)}{x + y}}
\end{displaymath}
with the expectation that it will be \emph{lifted} to operate over
streams. The reason for this choice is an essentially operational
one. Consider a program like the following: 
%
\begin{displaymath}
  \term{skew = \fun{xs}{(xs, 0 :: xs, 2 :: 3 :: xs)}}
\end{displaymath}
% 
This program accepts a stream \term{xs} as an input, and then
returns three streams --- the original stream \term{xs}; a stream
which first produces 0 and then the values of \term{xs}; and finally a
stream which yields 2, 3, and then the values of \term{xs}.

This means that if we implement this stream program as an imperative
program which modifies some state at each time step to dynamically
emit the values of the streams it computes, then at time $n+2$, this
program will also need to know the value of \term{xs} at times $n+1$
and $n$, in order to correctly emit the values of the second and third
components of its result. 

If programs which do unbounded buffering can be written, then the
danger of \emph{space leaks} arises, since the runtime system may need
to retain potentially arbitrary amounts of history to compute
values. By making time implicit and having the compiler lift values to
streams, it becomes harder to write programs which unintentionally
leak memory under this implementation strategy.

Happily, there is a natural semantic domain corresponding to these
lifted programs. This is the co-Kleisli category of streams over
ultrametric spaces is again a Cartesian closed category. A co-Kleisli
category $\ultrametric^S$ of a comonad $S$ is the category of free
$S$-coalgebras. Its objects are the objects of $\ultrametric$, but
each map $f : A \to B$ in $\ultrametric^S$ is a morphism $f : S(A) \to
B$ in $\ultrametric$.

The motivation for working with the co-Kleisli category is that a map
$e : A \to B$ in this category can be thought of as a term
representing a synchronous dataflow program with a free variable of
type $A$, as is usual for categorical semantics. However, since we
have committed to implicitly lifting variables to streams, we want to
know that the free variable is a stream value. So the meaning of the
term can be read as saying ``at each instant, $e$ will have a value of
type $B$ depending on the values that its free variable will take
on''.

To recall the details, a \emph{comonad} on a category $\ultrametric$
is a functor $S : \ultrametric \to \ultrametric$, equipped with two
natural transformations $\counit_A : S(A) \to A$ (the counit) and
$\tails_A : S(A) \to S(S(A))$ (the comultiplication) satisfying the
equations $\tails_A; S(\tails_A) = \tails_A; \tails_{S(A)}$ and
$\tails_A; S(\counit_A) = id = \tails_A; \counit_{T(A)}$.  (Here we
use the semicolon to indicate composition in diagrammatic order.)  In
the case of streams, the counit $\counit$ is the head function on
streams, and the comultiplication takes a stream and returns the a
stream containing the successive tails of the input stream. The action
of the stream functor is just the map functional for streams.

Furthermore, streams distribute over products (therefore is a
Cartesian functor) --- that is, $S(A \times B) \simeq S(A) \times
S(B)$, meaning that a stream of pairs is isomorphic to a pair of
streams. We will write \term{unzip} and \term{zip} for the components
of this isomorphism, corresponding to the similarly-named functions in
functional programs.

The identity map $\mathsf{id_A} : A \to A$ in $\ultrametric^S$ is the
counit $\counit_A$, and composition of two maps $f : A \to B$ and $g :
B \to C$ in $\ultrametric^S$ is $\mathsf{f;g} = \delta; S(f); g$. The
intuitive reading of the identity map is the instantaneous value of a
stream variable is its value \emph{right now}. The interpretation of
composition $\mathsf{f;g}$ (i.e., substitution) is that we first lift
the term $f$ to determine a stream of values by taking the tails of
its free variables to determine $f$'s sequence of values over time, and
then plug that into $g$. 

Amazingly, this category is also cartesian closed; the projection,
pairing, currying and evaluation maps are defined in
Figure~\ref{cokleisli-defs}.  Checking that these operations do indeed
satisfy the appropriate equations is a routine verification.

However, while this category is Cartesian closed, and so can interpret
pairs and function spaces, it does not have coproducts, and so cannot
interpret sums. What is worse is that the instantaneous interpretation
of terms makes it difficult to interpret operations acting on streams
of streams. The key difficulty is that there is no map in this
category which takes a stream of streams, and whose coextension yields
the head of the stream of streams.

This makes interpreting fixed points very difficult -- the natural
fixed point operator inherited from the category of ultrametrics, gets
the type $(A \lollishrink A) \lolli S(A)$ in the co-Kleisli
category. This means that we can't take a stream of functions and then
simply yield the elements of the first function's fixed point. 


\begin{figure}
\begin{mathpar}
  \begin{array}{lcl}
    \mathsf{id}   & = & \counit \\
    \mathsf{f;g}  & = & \tails; S(f); g \\[1em]

    \mathsf{fst} & = & \mathsf{unzip}; \fst{}; \counit \\
    \mathsf{snd} & = & \mathsf{unzip}; \snd{}; \counit \\
    \mathsf{pair}(f,g) & = & (f,g) \\[1em]

    \mathsf{curry}(f) & = & \lambda(\mathsf{zip}; f) \\
    \mathsf{eval}     & = & \mathsf{unzip}; (\counit \times \mathit{id}); \mathit{eval} \\
 \end{array}
\end{mathpar}
\caption{Definition of operations in the co-Kleisli category}
\label{cokleisli-defs}
\end{figure}




\section{DSL}

\fixme{This needs to be reworked in light of the new adjoint logic presentation}

\begin{mathpar}
\inferrule*[right=CConst]
          {\judge{e}{A}}
          {\judgec{\Delta}{\const{e}}{A}}
\and
\inferrule*[right=CApp]
          {\judgec{\Delta}{c}{A \shrink B} \\
           \judge[\Gamma, \Delta]{e}{A}}
          {\judgec{\Delta}{c\;e}{B}}
\and
\inferrule*[right=CPair]
          {\judgec{\Delta}{c}{A} \\ 
           \judgec{\Delta}{c'}{B}}
          {\judgec{\Delta}{\pair{c}{c'}}{A \times B}}
\and
\inferrule*[right=CFst]
          {\judgec{\Delta}{c}{A \times B}}
          {\judgec{\Delta}{\fst{c}}{A}}
\and
\inferrule*[right=CSnd]
          {\judgec{\Delta}{c}{A \times B}}
          {\judgec{\Delta}{\snd{c}}{A}}
\and
\inferrule*[right=CUnit]
          { }
          {\judgec{\Delta}{\unit}{1}}
\and
\inferrule*[right=CLet]
          {\judgec{\Delta}{c}{A} \\ 
           \judgec[\Gamma, x:A]{\Delta}{c'}{B}}
          {\judgec{\Delta}{\letc{x}{c}{c'}}{B}}
\and
\inferrule*[right=CLam]
          {\judgec[\Gamma, x:A]{\Delta}{c}{b}}
          {\judgec{\Delta}{\fun{x:A}{c}}{A \To B}}
\and
\inferrule*[right=CSLam]
          {\judgec{\Delta, x:A}{c}{B}}
          {\judgec{\Delta}{\sfun{x:A}{c}}{A \shrink B}}
\end{mathpar}

\section{The Programming Language and Library}

\section{The Implementation}

We have two cartesian closed categories in play, which are
represented a bit differently. 

\begin{mathpar}
  \begin{array}{lcl}
    \interpu{1}           & = & \unittype \\
    \interpu{X \times Y}  & = & \interpu{X} \star \interpu{Y} \\
    \interpu{X \to Y}     & = & \interpu{X} \to \interpu{Y} \\
    \interpu{X \shrink Y} & = & \interpu{X} \to \interpu{Y} \\
    \interpu{A^\omega}     & = & \comp{\streamtype{\interps{A}}}
    \\[1em]
    \interps{I}           & = & \unittype \\
    \interps{A \otimes B} & = & \interps{A} \star \interps{B} \\
    \interps{A \lolli B}  & = & \streamtype{\interps{A}} \to \comp{\streamtype{\interps{B}}} \\
    \interps{A \lollishrink\, B}  & = & \streamtype{\interps{A}} \to \comp{\streamtype{\interps{B}}} \\
    \interps{S(A)}        & = & \streamtype{\interps{A}}  \\
    \interps{\valtype{X}} & = & \interpu{X} 
    \\[1em]
    \streamtype{\tau}     & = & \celltype{\opttype{\tau}} \\
  \end{array}
\end{mathpar}

Most of the types of ultrametric world can be represented using ML
types. One interesting fact is that we have a type of strictly
contractive functions $A \shrink B$. This is represented in exactly
the same way as the ordinary ML function space, but must be kept
isolated in order to support the extra operation of fixed points.

The interesting case is the $A^\omega$ case which represents the type
of streams of values of type $A$, which are drawn from the objects of
the co-Kleisli category of streams over ultrametric spaces.  This
clause is interpreted as $\comp{\interps{A}}$, which can be understood
as follows. In the category of ultrametric spaces, we want to
\emph{understand} streams as having no temporal status at all -- they
are pure values. However, we want to \emph{represent} streams by
imperative data structures which change over time. This conflict is
resolved by representing an infinite stream as a computation which
constructs and initalizes a stream data structure. So each time we
evaluate a value of type $A^\omega$, we construct a fresh stream,
ready to begin counting regardless of when it was created.

In contrast, the interpretation of the co-Kleisli category is one in
which time plays a very significant part. The intuition is that a
dataflow graph realizes a collections of streams in the following way
--- the state of the dataflow graph is the seed of an unfold.  So time
is essentially a hidden argument to everything.

The argument to a function $A \lolli B$ comes as a stream, and the
return value is term of type $\comp{\streamtype{B}}$. The code
constructor permits the implementation to read and extend the dataflow
graph, doing some initialization to return a stream cell. Surpisingly,
this result is a stream, and not a point value, the way that the
intepretation of morphisms in the co-Kleisli category might initially
suggest. The corresponding clause of our logical relation in fact
takes the coextension of the function argument, and requires that the
result $d$-approximate the result of applying a stream to the lifting. 

\section{The Program Invariant}

\subsection{Discussion}

Q: What is the underlying intuition?

\noindent A: A stream is a node in a dataflow graph, which represents a stream value. 

The values are coinductively generated, and the \emph{state} of the
dataflow graph gives the \emph{seed} of the unfold operation which
generates the stream. That is, we want a recursive specification 
which looks something like this: 

\begin{tabbing}
$\mathit{Stream}(xs, \term{x}, \theta) \triangleq$  \\
\;\;\=$\setof{H(\theta)}$  \\
    \>\term{read(x)} \\
    \>$\setof{\term{a}.\;\exists \theta'. H(\theta') \land a = \head{xs} \land 
                           \mathit{Stream}(\tail{xs},\term{x}, \theta')}$ \\
\end{tabbing}

When we try to put this into practice, several complications to this
simple picture arise.

\begin{enumerate}

\item First, we have values other than streams -- we need to support
  units, tuples, streams, and most significantly \emph{functions}.
  To handle the wide variety of types of object we can construct, we
  will need to use a logical relation which relates a mathematical
  value to a program value \emph{plus} the state of the dataflow 
  graph. 

  In fact, since we have two categories of expressions, we will need
  two mutually-recursive logical relations, one for values which
  belong to the category of ultrametrics, and the other for values
  which belong to the co-Kleisli category of streams.

\item Next, multiple streams may share the dataflow graph.

  If we have two cells $a$ and $b$, both of whose code reads a cell
  $c$, then on a timestep in which we read both $a$ and $b$, one of
  them will $c$ in a state when it has already been updated by the
  other one. Therefore we need to specify what makes this interference
  safe --- we need to say what it means for a subgraph to ``already
  have been updated''. To do this, we must specify the values a term's
  dependencies will take on as they are evaluated, so that our
  specification can say that we are happy with a dataflow cell as long
  as it produces the appropriate value. We call this the \emph{rely},
  in analogy to rely-guarantee. Then, we can say that a state is
  ``more defined'' than another one when more of its cells are ready,
  and give a Kripke semantics which asserts that an expression good in
  one state is also good in all ``more defined'' states.

  Since we relate mathematical values to program values, and this
  relation includes the memory, we are lead to the further
  complication that stream cells in the dataflow graph may have types
  which are larger than the type of the current clause of the logical
  relation. Therefore, we cannot define our logical relation by
  induction on the type structure --- we need to borrow some ideas
  from step-indexed models, and define the rely mutually-recursively
  with the logical relation.

\item Next, we need to make our Kripke relation on states more complex
  than merely increasing as cells get evaluated. Since we build our
  dataflow graph incrementally and dynamically, we need to know that
  operations which add cells to the graph will not break existing
  code. So the Kripke relation on memories also needs to support a
  notion of the dataflow graph getting bigger, in addition to getting
  more defined.

\item Since we are implementing streams with stateful dataflow 
  cells, we need to ensure that there are no cyclic dependencies 
  in the flow graph. Therefore we need to ensure that a cell will
  not write to anything which depends upon it, to avoid event storms. 

\item Our dataflow graph is not pure; operations like accumulators and
  fixed points are implemented with imperative reference cells.  Each
  reference is read by a 

  Furthermore, cells may depend upon reference cells owned by cells
  which don't exist yet! For example, in the code for \term{fix}, the
  input uses a reference cell owned by the output, which will write
  its values into this cell for the input to feed back in. But at the
  time of creation of the input cell, the output has yet to be
  created.

\item The next problem connected with state in the dataflow graphs is
  that in order to correctly realize a stream, a cell with auxilliary
  state must have been evaluated on every tick. For example, consider
  a cell $c$ which realizes the stream 1, 2, 3, \ldots by maintaining
  a counter which it updates. If the cell is not evaluated at a
  particular time step, then the values it produces will fall behind
  the global clock.

  However, not all cells need to be evaluated every time step. For
  example, suppose we have a cell $c'$ which produces $2, 4, 6,
  \ldots$ by reading $c$ and returning twice its value. In this case,
  the computation at $c'$ is stateless, and so it does not need to be
  evaluated except on the ticks when its value is actually needed.

  So our program must maintain a list of the state-modifying cells in
  the dataflow graph, so that the event loop can evaluate the ones
  that have yet to be read before it advances time by a step.

\item Finally, when computing the fixed point of a stream function
  $f$, we proceed by giving $f$ a dummy input on the first time step,
  and giving the output at time $n$ as the input at time $n+1$. This
  means that each stream object in our rely may also contain a delay
  -- that is, it may lack a value (ie, be $\None$) at this time step,
  and only contain values beginning at the next time step.

  Here, the relational case for functions must also be adjusted, to 
  ensure that 
\end{enumerate}

\subsection{The Formal Specification}

In Figures \ref{logical-relation} and \ref{satisfaction-relation}, we
give three mutually-dependent definitions of values and memory
states. To explain this relation, we will proceed iteratively, first
giving a quick intuitive reading of each of the three definitions, 
and then proceeding to describe them in detail. 

The relation $U^d_A(v, \term{v})$ relates elements $v$ of a metric
space $A$ to a concrete program term \term{v}. Intuitively, it can be
read as saying ``the semantic value $v$ is approximated by the
computational value \term{v} to at least distance $d$''. 

Next, the relation $V^d_A(v, \term{v}, \mu)$ relates values in the
co-Kleisli category of streams to the program terms \term{v}. The
relation $V^d_A(v, \term{v}, \mu)$ has the intuitive reading ``the
semantic value $v$ is approximated by computational value \term{v} and
memory state $\mu$ to at least distance $d$''.

A memory state is a pair $(\theta, R)$, where $\theta$ is a formula
describing a dataflow network, and $R$ is a \emph{rely} describing the
stream of semantic values each cell in the network is expected to
produce. To relate these two, we have a satisfaction relation
$\satisfy{(\theta, R)}{d}$, which can be read as saying, roughly, that
$\theta$ implements the streams in $R$ to distance $d$. This
satisfaction relation is the most complicated part of the definition,
since we have to account for all of the issues discussed in the
previous section. Our satisfaction relation is given in
Figure~\ref{satisfaction-relation}.

We specify a rely $R$ as 8-tuple:

\begin{enumerate}
\item A finite set of stream cells and metric types $C$. 
\item An assignment of streams values $V_S : C \to \mathit{Value}^\omega$ 
  of the appropriate type to each cell.\footnote{We should write this 
  as a dependent product, with a stream cell being an element of type 
  $\Sigma A:\mathrm{type}.\;\celltype{\opttype{\interps{A}}}$, and $V_S$ having
  the type $V_S : \Pi (A, \_) \in C.\; A^\omega$. However, we will suppress these 
  dependencies to reduce notational clutter.}
\item A ``delay flag'' $D : C \to \Delays$ which says for each cell whether 
  its output is delayed $\D$ or whether it is undelayed $\U$. 
\item A set of reference cells $L$. 
\item An assignment of a stream of values $V^L : L \to \mathit{(1 +
  \mathit{Value})^\omega}$ for each local reference. Note in
  particular that our local state is given as a stream of
  \emph{options}: this is because we might want to use some state for
  ``only a little while''. (This arises in the correctness proof of the 
  \term{cons} function, for example.)
\item A ``getter'' for each reference cell $G : L \rightharpoonup
  C$. This is the cell which will read that reference cell. Note that
  this is a \emph{partial} function, which means that there can be
  reference cells which are not yet going to be read by anyone. This
  will let us build up the dataflow graph incrementally, while still
  remaining within the rely. 
\item A ``setter'' for each reference cell $S : L \rightharpoonup
  C$. This is the cell which has responsibility for writing the next
  value of the reference cell. Like the getter $G$, the getter $S$ is
  also partial. However, we require that its domain be a superset of $G$'s ---
  that is, we will always define getters before setters. 

\item A function $\Delta \to \powerset{C}$, giving the
  ``static dependencies'' of each cell. The intuition for this
  relation is that for each cell \term{c}, the inhabitation of the
  relation at $\Delta(\term{c'},\term{c})$ tells us that \term{c'} is
  a cell in the \emph{current} heap which the evaluation of \term{c} may
  update. 

  The conditions on this function are as follows. First, viewed as a
  relation, $\Delta$ must be a partial order, to ensure that there
  will be no cycles in the dependency graph. Second, these are only
  the \emph{static} dependencies in that the evaluation of \term{c}
  may in turn create new cells which are evaluated, but which do not
  appear in $\Delta$. 
\end{enumerate}

Often, we will be dealing with multiple relies, and as a matter of
notation we will name the appropriate component using the name
superscripted with the rely. So if $R$ is a rely, then we will write
$C_R$ for its cells, $V_R$ for the values of the cells, and so on.

We equip relies with a partial order as follows. We say that $R
\sqsubseteq R'$, when $C_R \subseteq C_{R'}$ and $L_R \subseteq L_{R'}$, and
furthermore each function is extended pointwise. That is, if $c \in
\dom{R}$, then $V_{R'}(c) = V_{R}(c)$, and similarly for $D_S$, $L$,
$V_L$, $G$, $S$, and $\Delta$. (Note in particular that the static
dependencies for a given cell do not grow -- the extension order for
$\Delta$ is more stringent than simply extension of the partial order.)

Given this, we can explain the satisfaction relation in
Figure~\ref{satisfaction-relation}. We say when a graph $\phi$
\emph{satisfies} a rely $R$ to distance $d$, (written
``$\satisfy{(\phi, R)}{d}$'') when:
\begin{itemize}
  \item The cells of the graph are equal to $C_R$, and the references
    of the graph are equal to $L_R$. This is lines 2-3. 
  \item Every cell in the graph is derivably either ready or unready. (Line 4)
  \item The reference \term{i} contains a subset of the cells which 
    read or write state. (Line 5)
  \item Every reference has contents realizing the head of its
    stream if its setter is unready, and the head of the tail of its
    stream if its setter is ready. (Line 6)
  \item It is never the case that a setter is ready while the
    corresponding getter is unready. (This is to prevent a setter
    from updating a reference before the getter sees it.)
  \item Each cell \term{c} in the graph realizes a stream of values corresponding
    to $V_R(\term{c})$, out to distance $d$. (Line 6)

\end{itemize}

These conditions are formalized in Figure~\ref{satisfaction-relation}. 

Notice that this satisfaction relation does not say what the values of
references without setters are. Likewise, the imperative update list
\term{i} is less precise that the event loop will eventually want.

This is a deliberate design decision: the reason we make this choice
is to let us say what our programs will do given incompletely
constructed networks -- and we will set up our logical relation to
guarantee they will only ever make things better. This way, we can
start off with a trivially complete input, and know that we will
construct a complete output, even though in the middle of evaluation
the dataflow graph may go through many stages of temporary
incompleteness.


\subsection{Theorems}

\begin{prop}{(Kripke Monotonicity)}
We have that for all $d' \geq d$
\begin{enumerate}
\item If $U^d_X(v, \term{v})$ then $U^{d'}_X(v, \term{v})$. 
\item If $V^d_A(v, \term{v}, \mu)$ then $V^{d'}_A(v, \term{v}, \mu)$. 
\item If $\satisfy{\mu}{d}$ then $\satisfy{\mu}{d'}$. 
\item If $V^d_A(v, \term{v}, \mu)$ and $\betterstate{d}{\mu'}{\mu}$, then $V^d_A(v, \term{v}, \mu')$.
\end{enumerate}
\end{prop}

\begin{lemma}{(Approximation Lemma)}
\begin{enumerate}
  \item If $\forall d' > d.\; U^{d'}_A(v, \term{v})$ then $U^d_A(v, \term{v})$. 
  \item If $\forall d' > d.\; V^{d'}_A(v, \term{v}, \mu)$ then $V^d_A(v, \term{v}, \mu)$. 
  \item If $\forall d' > d.\; \satisfy{\mu}{d'}$ then $\satisfy{\mu}{d}$. 
\end{enumerate}
\end{lemma}

\begin{lemma}{(Induction Lemma)}
\begin{enumerate}
  \item If $\forall d' > 2\cdot d.\; U^{d'}_A(v, \term{v}) \implies U^{d'/2}_A(v, \term{v})$ then $U^d_A(v, \term{v})$. 
  \item If $\forall d' > 2\cdot d.\; V^{d'}_A(v, \term{v}, \mu) \implies V^{d'}_A(v, \term{v}, \mu)$ then $V^d_A(v, \term{v}, \mu)$. 
  \item If $\forall d' > 2\cdot d.\; \satisfy{\mu}{d'} \implies \satisfy{\mu}{d'/2}$ then $\satisfy{\mu}{d}$. 
\end{enumerate}
\end{lemma}

In the following lemmas, we use the notation $[f|x:v]$ to denote the function which extends $f$'s domain 
by $x$ and gives it the value $v$ at that point. 

\begin{lemma}{(Reference Allocation)}
Suppose that $\satisfy{(\theta, R)}{d}$. Then suppose that $\theta' =
\theta \otimes \mathsf{ref}(\term{r,v})$.  Then for $R' \sqsupseteq R$
such that $R'$ is the same as $R$, except that $L_{R'} = L_R$ and 
$V_{R'} = [V_R|\term{r}:vs]$ for some $vs$, we have that 
$\satisfyext{(\theta', R')}{d}$.
\end{lemma}


\begin{lemma}{(Cell Allocation)} \\
Suppose that $\satisfy{(\theta, R)}{d}$ and
$\satisfyext{(\theta,R)}{d}$ and let $\theta' = \theta \otimes
\cellminus{\term{c}}{\term{code}}$. Further suppose we have $R'
\sqsupseteq R$ such that $C_{R'} = C_R \cup \setof{\term{c}}$, $L_{R'}
= L_R$, $G_{R'} = [G_R | \term{r_i : c}]$ for some set of references
indexed by $I$, and $S_{R'} = [G_R | \term{r_j : c}]$ for some set of
references indexed by $J$.

Then if we can show that for all $d' > 2 \cdot d$,
$\satisfy{(\theta',R')}{d'}$ implies $\Stream^{d'/2}_A(\term{c},
(\theta', R'))$, we can conclude that $\satisfy{(\theta',R')}{d}$. 
\end{lemma}



\begin{figure}
\begin{tabbingspec}
$U^d_1(\unitval, \unitval) = $ true 
\Newline[1em]

$U^d_{X \times Y}((x,y), (\term{x}, \term{y})) = U^d_X(x, \term{x}) \land U^d_Y(y, \term{y})$ 
\Newline[1em]

$U^d_{X \To Y}(f, \term{f}) = \forall d' > 2\cdot d, v, \term{v}.\; 
    U^{d'}_X(v, \term{v}) \implies U^{d'}_Y(f\;v, \term{f\;v})$ 
\Newline[1em]

$U^d_{X \shrink Y}(f, \term{f}) = \forall d' > 2\cdot d, v, \term{v}.\; 
    U^{d'}_X(v, \term{v}) \implies U^{d'}_Y(f\;v, \term{f\;v})$ 
\Newline[1em]

$U^d_{A^\omega}(vs, \term{code}) = $ \Newline
\;\;\= $\forall d' > 2\cdot d, \mu \in \Mem{d'}.\;\Build^{d'}_A(vs, \term{code}, \mu, \U, \emptyset)$ 
\Newline[1em]

$V^d_I(\unitval, \unitval, \mu) = $ true
\Newline[1em]

$V^d_{A \otimes B}((a,b), \term{(a,b)}, \mu) = V^d_A(a, \term{a}, \mu) \land V^d_B(b, \term{b}, \mu)$
\Newline[1em]

$V^d_{A \lolli B}(f, \term{f}, \mu) = $ \Newline
\> $\forall d' > 2\cdot d, \betterstate{d'}{\mu'}{\mu}, v, \term{v}$ \Newline
\> \;\;\=$V^{d'}_{S(A)}(vs, \term{v}, \mu') \implies $ \Newline
\>     \> \;\;$\exists L \sqsupseteq D_R(\term{v}).\; \Build^{d'}_{B}(f\;vs, \term{f\;v}, \theta', L, \Delta'(\term{v}))$
\Newline[1em]

$V^d_{A \lollishrink B}(f, \term{f}, \mu) = $ \Newline
\> $\forall d' > 2\cdot d, \betterstate{d'}{\mu'}{\mu}, v, \term{v}.$ \Newline
\> \> $V^{d'}_{S(A)}(vs, \term{v}, \mu') \implies 
      \Build^{d'}_{B}(f\;vs, \term{f\;v}, \mu', \U, \Delta'(\term{v}))$
\Newline[1em]

$V^d_{\valtype{X}}(v, \term{v}, \mu) = U^d_{X}(v, \term{v})$ 
\Newline[1em]

$V^d_{S(A)}(vs, \term{v}, \mu) = V_R(\term{v}) = vs$ 
\Newline[1em]

$\Build^d_A(vs, \term{code}, (\theta,R) \;\mathrm{as}\;\mu, L, Z) = $ \Newline
\> $\{H(\theta) \land \satisfyext{(\theta,R)}{d} \}$ \Newline
\> $\term{code}$ \Newline
\> $\{(\term{a},\_).\;\exists$\=$\betterstate{d}{(\theta,R')}{(\theta,R)}, u.\; H(\theta') \;\land$ \Newline
\> \> $V^d_{S(A)}(vs, \term{a}, (\theta',R')) \;\land$\Newline
\> \> $\term{a} \in C_{R'} \land D_{R'}(\term{a}) = L \;\land $ \Newline
\> \> $\Delta_{R'}(\term{a}) \supseteq \bigcup\limits_{\term{c} \in Z} \Delta_{R}(\term{c}) \;\land$ \Newline
\> \> $\Update(u, (\theta, R), (\theta', R'), \term{a}) \;\land$ \Newline
\> \> $\Stable((\theta, R), (\theta',R'))\}$
\Newline[1em]

$\Update(u, (\theta, R), (\theta', R'), \term{a}) = $ \Newline
\> $\forall \term{c}.\;\ready{\theta'}{c}{-} \land \unready{\theta}{c} \iff c \in u \;\land$ \Newline
\> $\forall \term{c} \in u.\; \term{c} \in \Delta_R(\term{a}) \vee \term{c} \in (C_{R'} - C_R)$ 
\Newline[1em]

$\Stable(\mu, \mu') = \StableRef(\mu, \mu') \land \StableImp(\mu, \mu')$ \Newline[1em]

$\StableRef((\theta,R), (\theta'R')) = $ \Newline
\> $\forall \term{r} \in L_{R'}.\; S_{R'}(\term{r}) \mbox{ undef} \iff \term{r} \in L_R \land  S_{R}(\term{r}) \mbox{ undef}$ and \Newline
\> $\forall \term{r} \in L_{R'}.\; G_{R'}(\term{r}) \mbox{ undef} \iff \term{r} \in L_R \land  G_{R}(\term{r}) \mbox{ undef}$ and \Newline
\> $\forall \term{r} \in L_{R} - \dom{G_R}.\;\exists \term{v}.\;\hasref{\theta}{\term{r}}{\term{v}} \land \hasref{\theta'}{\term{r}}{\term{v}}$
\Newline[1em]

$\StableImp((\theta, R), (\theta',R'))\} = $ \Newline
\> $\forall I,I'.\;\hasref{\theta}{\term{i}}{I} \land \hasref{\theta}{\term{i}}{I'} \implies $ \Newline
\>\;\;\= $[I' - S^{-1}_{R'}(L_{R'}) - G^{-1}_{R}(L_{R})] = [I - S^{-1}_{R}(L_{R}) - G^{-1}_{R}(L_{R})]$ \Newline
\> \> and $\forall \term{c} \in I'.\; \unready{\theta'}{\term{c}} \implies \term{c} \in I \land \unready{\theta}{c}$
\end{tabbingspec}
\caption{The Logical Relation}
\label{logical-relation}  
\end{figure}

\begin{figure}
\begin{tabbingspec}
$\satisfy{(\theta,R)}{d} \triangleq$ \Newline
\;\;\=$\cells{\theta} = C_R$ and \Newline
    \>$\refs{\theta} = L_R$ and \Newline
    \>$\forall \term{c}:A \in C_R.\;$\=$\unready{\theta}{\term{c}} \vee \exists \term{v}.\; $\=$\ready{\theta}{\term{c}}{\term{v}}$ and \Newline
    \>$\exists I.\;\hasref{\theta}{\term{i}}{I} \land I \subseteq (G_R(L_R) \cup S_R(L_R))$ and \Newline
    \>$\forall \term{c}:A \in \cells{\theta}.\;\Stream^d_A(\term{c}, (\theta, R))$ and \Newline
    \>$\forall \term{r}:A \in L_R.\; \Local^d_A(\term{r}, (\theta, R))$ 
\Newline[1em]

$\Mem{d} = \comprehend{(\theta,R)}{\satisfy{(\theta,R)}{d}}$ 
\Newline[1em]

$(\sqsupseteq^d) \subseteq \Mem{d} \times \Mem{d}$ \Newline[0.2em]

$\betterstate{d}{(\theta',R')}{(\theta,R)}$ iff \Newline
\> $R' \sqsupseteq R$ and \Newline
\> $\forall \term{c}:A \in C_R, \term{v}.\; \ready{\theta}{\term{c}}{\term{v}} \implies \ready{\theta'}{\term{c}}{\term{v}}$ and \Newline
\> $\forall \term{c}:A \in C_R, \term{code}.\; \mathsf{code}(\theta, \term{c}, \term{code}) \implies \mathsf{code}(\theta', \term{c}, \term{code})$ and \Newline
\> $\forall \term{r}\in E_R, \term{v}.\; \mathsf{ref}(\theta, \term{r}, \term{v}) \iff 
                                         \mathsf{ref}(\theta', \term{r}, \term{v}) $
\Newline[1em]

$\Stream^d_A(\term{v}, \mu) = $ \Newline
\> $\forall d' > 2\cdot d, n \leq \log(1/d'), \futurestate{d'}{n}{\mu'}{\mu}.\; \Head^{d'\cdot 2^n}_A(\term{v}, \mu')$ 
\Newline[1em]

$\futurestate{d}{0}{\mu'}{\mu} \;\;\;\,= \betterstate{d}{\mu'}{\mu}$ \Newline
$\futurestate{d}{n+1}{\mu'}{\mu} = $ \Newline
\> $\exists \mu_0 \in \Mem{d}.\; \betterstate{d}{\mu_0}{\mu} \land \complete{\mu_0} \land \futurestate{2\cdot d}{n}{\mu'}{\tail{\mu_0}}$ 
\Newline[1em]

$\Head^d_A(\term{v}, (\theta,R)) = $\Newline
\> $\{H(\theta) \land \satisfyext{(\theta,R)}{d}\} $\Newline
\> \term{read\;v} \Newline
\> $\{(\term{a},\_).\;\exists$\=$\betterstate{d}{(\theta', R')}{(\theta,R)}, u.\; H(\theta') \;\land$ \Newline
\> \> $\Opt^{d}_A(\mbox{if } D_R(\term{v}) \mbox{ then } \None \mbox{ else } \Some{V_R(\term{v})}, \term{a}, \mu') \;\land$ \Newline
\> \> $\Update(u, (\theta,R), (\theta', R'), \term{v}) \;\land$ \Newline 
\> \> $\Stable((\theta,R), (\theta',R')) \}$ 
\Newline[1em]

$\Local^d_A(\term{r}, (\theta, R)) = $ \Newline
\> $\forall d' > 2\cdot d.\; S_R(\term{r})$ defined $\implies$ \Newline
\> \;\;\= $\unready{\theta}{S_R(\term{r})} \implies \Ref^{d'}_A(\head{V^R_L(r)}, \term{r}, \mu)$ and \Newline
\> \> $\ready{\theta}{S_R(\term{r})}{-} \implies $(\=$\Ref^{d'}_A(\head{\tail{V^R_L(r)}}, \term{r}, \mu)$ \Newline
\> \> \> $\;\land\; \exists \term{v}.\;\ready{\theta}{G_R(\term{r})}{\term{v}})$
\Newline[1em]


$\Ref^d_A(v, \term{r}, (\theta,R)) = \exists \term{v}.\;\hasref{\theta}{r}{\term{v}} \land \Opt^{d'}_A(v, \term{v}, (\theta,R))$ 
\Newline[1em]

$\Opt^d_A(v, \term{v}, \mu) = $ \Newline
\> $(v = \None \land \term{v} = \None) \;\vee$ \Newline
\> $(\exists v', \term{v'}.\; v = \Some{v'} \land \term{v} = \Some{\term{v'}} \land V^d_A(v', \term{v'}, \mu))$ 
\Newline[1em]

$\complete{\theta,R} = $ \Newline
\> $\forall \term{r} \in L_R. \exists \term{c} \in C_R.\; S_R(\term{r}) = \term{c} \land \exists \term{v}.\;\ready{\theta}{\term{c}}{\term{v}}$ 
\Newline[1em]

$\satisfyext{(\theta,R)}{d} = $ \Newline
\> $\forall \term{r}:A \in L_R.\; S_R(\term{r}) \mbox{ undef} \implies \Ref^d_A(\head{V^L_R(\term{r})}, \term{r}, (\theta,R))$
\end{tabbingspec}
\caption{The Satisfaction Relation}
\label{satisfaction-relation}
\end{figure}


\section{Future Work}




\section{Related Work}

In our work, we use ultrametric spaces to give semantics to streamt
ransformers. In the special case of functions from streams to
streams, causality and nonexpansiveness precisely coincide, but due to
cartesian closure function types at all orders are well-defined.
Furthermore, to support efficient implementation, we also need to make
use of the co-Kleisli category over the stream comonad, which we 
connect to the base category via an adjunction. 

There are four main strands of related work: synchronous dataflow
languages, purely functional reactive programming systems, imperative
FRP systems, and metric methods in denotational semantics.

The family of synchronous dataflow languages (such as
Esterel~\cite{esterel}, Lustre~\cite{lustre}, and Lucid
Synchrone~\cite{synchrone}) are languages based on a model of
synchronous time. The discrete ultrametric semantics we use is related
to these systems, most especially to Lucid Synchrone (which supports
higher-order functions).

Pouzet and Caspi~\cite{coiterative} extended synchronous dataflow
programming to higher order with their co-iterative semantics. They
illustrated how that this generated a Cartesian closed category (of
size-preserving functions), which they used to interpret
functions. Uustalu and Vene~\cite{essence-dataflow} subsequently
observed that size-preserving functions could be understood more
abstractly as the co-Kleisli category of streams. However, in both of
these works, feedback was handled in a somewhat \emph{ad hoc} fashion.

The proper treatment of this issue is delicate, and disentangling the
two main pieces of it kept us busy for a long time. First, we use
ultrametrics to give a semantic criterion for causality, which permits
us to avoid explicitly looking at the syntax of a program to identify
dependencies. This is essential for the smooth treatment of
higher-order functions, since the dependencies may not be statically
apparent in this case.

Second, we needed to make explicit use of the adjunction between the
base category of ultrametric spaces and the co-Kleisli category of
streams. The reason is that taking the fixed point of (the coextension
of) a contractive function $A \lollishrink A$ in the co-Kleisli
category yields a \emph{stream} of values. Essentially, feedback lets
us take a function and turn it into a stream, but since we are
receiving a stream of functions as an input, the natural fixed point
operator will yield a new stream at each timestep.  However, there are
no maps in the co-Kleisli category which take a stream of streams and
fix the outputs as the successive elements of the stream at a
particular time --- and this is precisely what we need to do enumerate
the elements of a fixed point over time.

This is why we have two categories -- the operations we need for
reative programming live naturally in both. This permits us to
preserve the equational theory of the programming language:
programmers can reasoning using full rules for $\beta\eta$-equality,
as well as the equations for fixed points. We do not need to
compromise on our reasoning principles in any way, which is quite
remarkable given the low-level, imperative nature of our
implementation.

Functional reactive programming was introduced by Eliot and
Hudak~\cite{fran}, and was given a semantics in terms of event streams
and unrestricted functions over them. In this and subsequent
work~\cite{courtney-thesis}, the semantics of fixed points were given
denotationally. This gives semantics to all FRP expressions, including
non-well-founded programs (which will go into infinite loops). 

One notable feature of this line work is a treatment of continuous
time. We believe that our proof framework should extend to proving an
sampling theorem as in Wan and Hudak~\cite{frp-first-principles}.  On
the semantic side, continuous streams can be modelled as functions
$\mathbb{R} \to A$, and the causal ultrametric extends naturally to
this case. On the implementation side, the \term{clock} can supply
time deltas (in contrast to its current delivery of pure ticks).

Due to the problem of space leaks, arrowized FRP~\cite{arrowized-frp}
was introduced in order to restrict the set of definable stream
transformers to the the causal ones. The restriction to arrows is
roughly equivalent to first-order functional programming, though
Nilsson~\emph{et al.}  introduced additional combinators to recover
higher-order and dynamic behavior. Our semantics gives a way of
eliminating these restrictions and admitting higher-order and dynamic
behavior in a very uniform way.

Metric methods were introduced into semantics in the late
1970s~\cite{nivat} and early 1980s, in order to simplify the
denotational semantics of
concurrency~\cite{concurrency-semantics}. The applications to stream
programming were recognized early, but not followed up on: in a
surprisingly little-cited 1985 paper~\cite{metric-dataflow}, de Bakker
and Kok proposed an ultrametric semantics for a language of
first-order stream programs over integers. In their paper, they wrote
``We think there are no problems when we allow functions of higher
order[\ldots]''.  This is a conjecture which we have attended to, a
full quarter-century later: we can confirm that it turns out to be
true, but only if we make it true twice over!

More recently, Birkedal and his coworkers~\cite{birkedal-ultrametrics}
have used ultrametric models to give semantics to sequential programs
involving advanced features such as higher-order state and
polymorphism, and has suggested connections between these ideas and
the more operationally-flavored technique of step-indexed
models~\cite{appel-mcallester}. Our logical relation can be seen as
similar to these models, only ``one level down'' -- we define
\emph{values} as fixed points of metric spaces, whereas Birkedal
\emph{et al.} define \emph{types} as fixed points of metric spaces.

There is also a fascinating and suggestive paper by
Escardo~\cite{escardo-metric}, who observed that metric models seem to
correspond to PCF extended with timeout operators. Since cancels and
interrupt operations pervade interactive programs, this suggests we
should investigate whether they can be supported without harming the
reasoning principles of the language.

\appendix

\section{Implementation}

We give the implementation of the code for the dynamic lambda calculus
in Figure~\ref{cokleisli-implementation}, and the implementation of the
program for the static lambda calculus in Figure~\ref{ultrametric-implementation}. 

The implementation is relatively straightforward, with two exceptions:
the implementation of the \term{zip} operation. Given two stream
cells, it returns a cell which pairs the successive elements of its
two inputs. This is a tricky function to verify, since the function
must work with lagged inputs, and we may receive a pair of inputs in
which one component is lagged and the other not. However, our
mathematical specification does not mention delays in it at all. So
even if one input yields $(a_0, a_1, a_2, \ldots)$ and the other
yields $(\None, b_0, b_1, b_2, \ldots)$, the programmer is still
entitled to assume that $a_0$ is paired with $b_0$, and that $a_1$ is
paired with $b_1$, and so on.

To implement this, \term{zip} tests the two inputs, and introduces an
artificial delay, if one cell is delayed and the other is not.
Otherwise, it simply returns a cell which performs the pairing. Since
implementing a delay uses auxilliary state, we need to register the
cell --- but we only \term{register} the cell in the case it needs the
state. This reduces the number of cells that will get forced at the
end of each trip through the event loop, and so lets the dataflow
graph remain as lazy as we can manage. 

Secondly, the implementation of \term{fix} is quite subtle. It works
takes a stream of functions, and returns a stream of streams.

Our actual implementation does some further optimizations not visible
in this source. In particular, we exploit the isomorphism $S(A \times
B) \simeq S(A) \times S(B)$ to maintain the context in product form.
This permits us to avoid excessive conservatism in managing dependencies.

\begin{figure}
\begin{tabbing}
\term{id = \lambda xs.\;return(xs)} 
\\[1em]

\term{compose\;f\;g =\lambda as.\; do\;}
 \=\term{bs \leftarrow f(as);} \\
 \>\term{cs \leftarrow g(bs);} \\
 \>\term{return(cs)} 
\\[1em]

\term{one\;xs = cell(return(\Some{\unitval}))}
\\[1em]

\term{pair\;f\;g = \lambda as.\;do\;}
  \=\term{bs \leftarrow f(as);} \\
  \>\term{cs \leftarrow g(as);} \\
  \>\term{zip(bs,cs)}
\\[1em]

\term{fst = \lambda abs.\;cell(do}
  \= \term{ab' \leftarrow read(abs);} \\
  \>\term{case \; ab'\; of} \\
  \>\term{\;\None \to return(\None)} \\
  \>\term{\;\Some{a,b} \to return(\Some{a}))} 
\\[1em]


\term{snd = \lambda abs.\;cell(do}
  \= \term{ab' \leftarrow read(abs);} \\
  \>\term{case \; ab'\; of} \\
  \>\term{\;\None \to return(\None)} \\
  \>\term{\;\Some{a,b} \to return(\Some{b}))} 
\\[1em]

\term{eval = \lambda fas.\;do}\;
              \=\term{fs \leftarrow \;fst(fas);}\\
              \>\term{as \leftarrow \;snd(fas);}\\
              \>\term{cell(do\;}\=\term{f' \leftarrow read(fs)}\\
              \>                  \>\term{case\;f'\;of}\\
              \>                  \>\term{\;\None \to return\;\None} \\
              \>                  \>\term{\;\Some{f} \to do\;}\=\term{bs \leftarrow f(as);} \\
              \>                  \>                            \>$\term{read(bs)})$ 
\\[1em]

\term{curry\;f =\lambda as.\;cell(\mathsf{Some}(\lambda bs.\;do\;}
  \=\term{abs \leftarrow zip(as,bs);}\\
  \>\term{f(abs)))}
\\[1em]

\term{zip(as, bs) =} \\
\;\;\= \term{do\;}\=\term{a' \leftarrow read(as);} \\
\>              \>\term{b' \leftarrow read(bs);} \\
\>              \>\term{case \;(a',b')\;of}\\ 
\>           \>\;\;\=\term{(\None, \None)} \\
\>           \>    \>\term{(\Some{\_}, \Some{\_}) \to} \\
\>           \>    \> \qquad \term{cell(do\;}\=\term{a' \leftarrow read(as);} \\
\>           \>    \>                      \>\term{b' \leftarrow read(bs);} \\
\>           \>    \>                      \>\term{case \;(a',b')\; of} \\
\>           \>    \>                      \>\;\;\=\term{(\Some{a}, \Some{b}) \to return(\Some{(a,b)})} \\
\>           \>    \>                      \>\;\;\=\term{(\_, \_) \to return(\None))} \\
\>           \>    \>\term{(\None, \Some{\_}) \to}\\
\>           \>    \> \qquad\term{do\;}\=
                                      \term{r \leftarrow ref(\None);} \\
\>           \>    \>               \>\term{abs \leftarrow cell(do\;}\=\term{a \leftarrow read(as);} \\
\>           \>    \>               \>                               \>\term{new \leftarrow read(bs);} \\
\>           \>    \>               \>                               \>\term{old \leftarrow !r;}\\
\>           \>    \>               \>                               \>\term{r := new;}\\
\>           \>    \>               \>                               \>\term{return(a,old));}\\
\>           \>    \>               \>\term{register(abs);} \\
\>           \>    \>               \>\term{return(abs)} \\
\>           \>    \>\term{(\Some{\_}, \None) \to}\\
\>           \>    \> \qquad\term{do\;}\=
                                      \term{r \leftarrow ref(\None);} \\
\>           \>    \>               \>\term{abs \leftarrow cell(do\;}\=\term{b \leftarrow read(bs);} \\
\>           \>    \>               \>                               \>\term{new \leftarrow read(as);} \\
\>           \>    \>               \>                               \>\term{old \leftarrow !r;}\\
\>           \>    \>               \>                               \>\term{r := new;}\\
\>           \>    \>               \>                               \>\term{return(old,b));}\\
\>           \>    \>               \>\term{register(abs);} \\
\>           \>    \>               \>\term{return(abs)} 
\\[1em]

\term{register(xs) = do\;}\=\term{dummy \leftarrow read(xs);}\\
                          \>\term{lst \leftarrow !i;} \\
                          \>\term{i := pack(xs) :: lst} 
\end{tabbing}
\caption{The Implementation of the co-Kleisli Category}
\label{cokleisli-implementation}
\end{figure}

\begin{figure}
\begin{tabbing}
$\term{fix} : (A \lollishrink A) \lolli S(A)$ \\
\term{fix = \lambda fs.\;cell(do}
  \=\term{f' \leftarrow read(fs);} \\
  \>\term{case\;f'\;of}\\
  \>\;\;\=\term{\None \to return(\None)}\\
  \>    \>\term{Some(f) \to} \\
  \>    \>\;\;\term{do} \=\term{r \leftarrow ref(\None);}\\
  \>    \>  \>\term{input \leftarrow cell(do\;}\=\term{() \leftarrow clock;}\\
  \>    \>  \>                                 \>\term{v \leftarrow !r;}\\
  \>    \>  \>                                 \>\term{return(v));}\\
  \>    \>  \>\term{pre \leftarrow f(input);}\\
  \>    \>  \>\term{out \leftarrow cell(do\;}\=\term{\_ \leftarrow read(input);} \\
  \>    \>  \>                               \>\term{v \leftarrow read(pre);}\\
  \>    \>  \>                               \>\term{r := v;}\\
  \>    \>  \>                               \>\term{return(v));}\\
  \>    \>  \>\term{register(out);}\\
  \>    \>  \>\term{return(\Some{out})})\\
\end{tabbing}
\caption{Implementing Fixed Points in the co-Kleisli Category}  
\label{cokleisli-implementation-2}
\end{figure}


\begin{figure}
\begin{tabbing}

\term{id = \fun{x}{x}} 
\\[1em]
\term{compose\;f\;g = \fun{x}{g(f(x))}}
\\[1em]
\term{one = \fun{x}{\unitval}}
\\[1em]
\term{fst = \fun{(x,y)}{x}}
\\[1em]
\term{snd = \fun{(x,y)}{y}}
\\[1em]
\term{pair\;f\;g = \fun{x}{(f(x),g(x))}}
\\[1em]
\term{eval = \fun{(f,x)}{f(x)}}
\\[1em]
\term{curry\;f = \fun{x}{\fun{y}{f(x,y)}}}
\\[1em]
\term{cons = \lambda x.\; cell(return(\Some{f}))} \\[0.1em]
with 
\term{f = \lambda ys.\;do} \= \term{r \leftarrow \Some{x};} \\
\>     \term{zs \leftarrow cell(}\= 
      \term{do} \=\term{old \leftarrow !r;} \\
\> \> \> \term{new \leftarrow read(ys);} \\
\> \> \> \term{case\;old\;of} \\
\> \> \> \;\;\= \term{\None \to return(new)} \\
\> \> \> \>     \term{\Some{\_} \to do} \=\term{r := new;} \\
\> \> \> \>  \>                           \term{return(old));} \\                 
\> \term{register(zs);} \\
\> \term{return(zs)}\\


\\
\term{head\;thunk =} \\
\;\;\term{do\;}
  \=\term{xss \leftarrow thunk;} \\
  \>\term{xs' \leftarrow read(xss);} \\
  \>\term{case\;xs'\;of} \\
  \>\;\;\=\term{\Some{xs} \to return(xs)}\\
  \>    \>\term{\None \to ERROR} (invariant ensures this case cannot happen) \\

\end{tabbing}
\caption{The Implementation of the Ultrametric Category}
\label{ultrametric-implementation}
\end{figure}

\begin{figure}
\begin{tabbing}
$(-)^\omega : U^S(A,B) \to U(A^\omega, B^\omega)$ \\
\term{omega\;f = \lambda athunk.\;do} \=\term{as \leftarrow athunk;} \\
                                      \>\term{bs \leftarrow f(as);} \\
                                      \>\term{return(bs)} 
\\[1em]

$\mathit{Value} : U(X,Y) \to U^S(\valtype{X}, \valtype{Y})$ \\[0.2em]
\term{value\;f = \lambda xs.\;cell(do} \=\term{x' \leftarrow read(xs);}\\
                                       \>\term{case\;x'\;of}\\
                                       \>\;\;\=\term{\None \to return(\None)} \\
                                       \>\>    \term{\Some{x} \to return(\Some{f(x)}))}

  
\end{tabbing}
\caption{Implementing the Adjunction}
\label{adjoint-implementation}  
\end{figure}


\acks

Acknowledgments, if needed.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\bibliography{popl11-frpccc}


\end{document}
