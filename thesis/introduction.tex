\chapter{Introduction}

My thesis is that it is possible to give modular correctness proofs of
interesting higher-order imperative programs using higher-order
separation logic.

\section{Motivation}

It is more difficult to reason about programs that use aliasing than
ones that do not use mutable shared data. It is more difficult to
reason about programs that use higher order features than first order
programs. Put both together, and matters become both challenging and
interesting for formal verification, since the combination yields
languages more complex than the sum of their parts. 

Techniques to reason about purely functional programs, which
extensively use higher-order methods but eschew mutable state, are a
well-developed branch of programming language theory, with a
long and successful history.

Historically, techniques to reason about mutable state have lagged
behind, but some years ago O'Hearn and Reynolds introduced separation
logic~\citep{sep-logic}, which has proven successful at enabling
correctness proofs of even such intricate imperative programs as
garbage collectors~\citep{gc-proof}. Separation logic has historically
focused on low-level programming languages that lack a strong type
discipline and allow the use of techniques such as casting pointers to
integers.

Even high level languages that allow the use of state are prone to
aliasing errors, since (with a few exceptions) type systems do not
track interference properties. Some languages, such as
Haskell~\cite{haskell-report}, isolate all side-effects within a
particular family of monadic types. While this preserves reasoning
principles for functional code, we still face the problem that
reasoning about monadic code remains as difficult as ever. Haskell's
type discipline imprisons mutation and state, but does not
rehabilitate them.

Of course, we can combine language features combinatorially, and if
this particular combination had no applications, then it would be only
of technical interest. In fact, though, higher-order state pervades
the design of common programs. For example, graphical user interfaces
(GUIs) are typically structured as families of callbacks that operate
over the shared data structures representing the interface and
program. These callbacks are structured using the subject-observer
pattern~\cite{gof}, which uses collections of callbacks to implicitly
synchronize mutable data structures across a program. So higher-order
state not only poses a technical challenge, but also offers a set of
tantalizingly practical examples to motivate the technical
development.

In this dissertation, I develop a model higher-order imperative
programming language, and develop a program logic for it. I
demonstrate the power of my program logic by verifying a series of
interesting examples, culminating in the correctness proof of a
library for event-driven programming, which despite an imperative (and
higher-order) implementation, nevertheless permits clients to reason
about it using simple and powerful equational reasoning principles.

\section{Programming Language}

The first contribution of my dissertation is to give a denotational
semantics for a predicative version of System
$F^\omega$~\cite{girard-thesis}, extended with a monadic type
constructor for state in Pfenning-Davies~\cite{pfenning-davies}
style. This language is an instance of Moggi's~\cite{moggi-monads}
monadic metalanguage, as all heap effects and nontermination live
within the monadic type constructor.

Higher-order imperative programs usually contain a substantial
functional part in addition to whatever imperative operations they
perform. As a result, it is convenient to be able to reason
equationally about programs when possible (including induction
principles for inductive types), and by giving a denotational
semantics, I can easily show the soundness of these equational
properties.

Ths semantics of this language is the focus of Chapter 2. In
particular, I show that a programming language including references to
polymorphic values, nevertheless gives rise to a domain equation which
can be solved using the classical techniques of Smyth and
Plotkin~\cite{smyth-plotkin}.

\section{Higher Order Separation Logic}

The main tool I develop to prove programs correct is a higher order
separation logic for a higher-order imperative programming
language. The details of the logic are given in Chapter 3, but 
I will say a few words to set the stage now. 

As I mentioned earlier, we can reason about the functional part of
programs using the standard $\beta\eta$-theory of the lambda calculus,
but to reason about imperative computations, I introduce a
specification logic, in the style of Reynolds' specification logic for
Algol~\citep{spec-logic}.

In this logic, the basic form of specification is the Hoare triple
over an imperative computation $\spec{P}{c}{a:A}{Q}$. Here, $P$ is the
precondition, $c$ is the computation that we are specifying, and $Q$
is the post- condition. The binder $a:A$ names the return value of the
computation, so that we can mention it in the post-condition.

The assertion logic for the pre- and post-conditions is higher order
separation logic~\citep{hosl}. This is a substructural logic that
extends ordinary logic with three spatial connectives to enable
reasoning about the aliasing behavior of data. The Hoare triples
themselves form the atomic propositions of a first-order
intuitionistic logic of specifications. The quantifiers range over the
sorts of the assertion logic, so that we can universally and
existentially quantify over assertions and expressions.

As a teaser example, consider the specification of a counter module.

\begin{tabbing}
$\exists \alpha : \star$ \\
$\exists \mathtt{create} : \unittype \to \monad{\alpha}$ \\
$\exists \mathtt{next} : \alpha \to \monad{\N}$ \\
$\exists $\= $\mathit{counter} : \alpha \times \N \To \assert$ \\
\> $\mspec{\emp}{\mathtt{create}()}{a:\alpha}{\mathit{counter}(a, 0)}$ \\
\> $\specand$ \\
\> $\forall c:\alpha, n:\N.\; \mspec{counter(c, n)}{\mathtt{next}(c)}{a:\N}{a = n \land \mathit{counter}(c, n+1)}$
\\
\end{tabbing} 

The idea is that in our program logic, we can assert the existence of
an abstract type of counters $\alpha$, operations $\mathtt{create}$
and $\mathtt{next}$ (which create and advance counters, respectively),
and a state predicate $\mathit{counter}(c, n)$ (which asserts that the
counter $c$ has value $n$).

The specifications are Hoare triples --- the triple for $\mathtt{create}$ asserts
that from an empty heap, calling $\mathtt{create}$ creates a counter initialized
to 0, and the triple for $\mathtt{next}(c)$ asserts that if $\mathtt{next}$ is called
on $c$, in a state where it has value $n$, then the return value will be $n$ and
the state of the counter will be advanced to $n+1$. 

Now, here are two possible implementations for the existential witnesses:

\begin{displaymath}
\begin{array}{lcl}
\alpha & \equiv & \reftype{\N} \\[0.5em]
counter & \equiv & \lambda (r, n).\; (r \pointsto n) \\[0.5em]
\mathtt{create} & \equiv & \lambda ().\; [\newref{\N}{0}] \\[0.5em]
\mathtt{next}  & \equiv & \lambda r.\; [\letv{n}{[!r]}{
                                        \letv{()}{[r := n + 1]}{n}}] \\
\end{array}
\end{displaymath}

and also                                        
\begin{displaymath}
\begin{array}{lcl}
\alpha & \equiv & \reftype{\N} \\[0.5em]
counter & \equiv & \lambda (r, n).\; (r \pointsto n + 7) \\[0.5em]
\mathtt{create} & \equiv & \lambda ().\; [\newref{\N}{7}] \\[0.5em]
\mathtt{next}  & \equiv & \lambda r.\; [\letv{n}{[!r]}{
                                        \letv{()}{[r := n + 1]}{n - 7}}] \\
\end{array}
\end{displaymath}

So in our program logic, the implementations of modules consist of the
witnesses to the existential formulas. This is morally an application
of Mitchell and Plotkin's identification of data abstraction and
existential types~\cite{mitchell-plotkin} --- in this view, linking a
module with a client is nothing but an application of the existential
elimination rule of ordinary logic.  Note that instead of abstracting
only over types, we also abstract over the \emph{heap}.

In order for a verification methodology to scale up to even
modestly-sized programs, it must be modular. There are three informal
senses in which I use the word ``modular'', each of which is supported
by different features of this logic, and are (mostly) illustrated in
this example.

\begin{enumerate}
\item First, we should be able to verify programs library by
  library. That is, we should be able to give the specification of a
  library, and prove both that implementations satisfy the
  specification without knowing anything about the clients that use
  it, and likewise prove the correctness of a client program without
  knowing anything about the details of the implementation (beyond
  what is promised by the specification). 

  In the example above, I tackle this problem by making use of the
  fact that the presence of existential specifications in our
  specification logic lets use the Mitchell-Plotkin encoding of
  modules as existentials. So once I prove a client against this
  specification, I can swap between implementations without having
  to re-verify the client. 

\item Related to this is a modular treatment of aliasing. Ordinary
  Hoare logic becomes non-modular when asked to treat mutable, shared
  state, because we must explicitly specify the aliasing or
  non-aliasing of every variable and mutable data structure in our
  pre- and post-conditions. Besides the quadratic dependence on the
  size of programs, the relevant set of variables grows whenever a
  subprogram is embedded in a larger one. Separation logic resolves
  this problem via the frame rule, a feature which we carry forward in
  our logic.

  In our example, counters have state, and so the predicates for
  distinct counters need to be disjoint. So even in the simplest
  possible example, it is useful to be able to have the power of
  separation logic available.

\item Finally, it is important to ensure that the abstractions we
  introduce compose. Benton~\cite{benton-modularity} described a
  situation where he was able to prove that a memory allocator and the
  rest of the program interacted only through a particular interface,
  but when he tried to divide the rest of the program into further
  modules, he encountered difficulties, because it was unclear how to
  share the resources and responsibility for upholding the contract
  with the allocator.

  While the previous example was too simple to illustrate these kinds
  of problems, I do exploit the expressiveness of my program logic to
  introduce several new \emph{specification patterns} for verifying
  these kinds of programs. In the following section, I will describe
  some of the patterns I discovered.
\end{enumerate}

\section{Verifying Programs}

The final test of a program logic is what we can prove with it, and so
accordingly I have devoted a great deal of effort to not only prove
metatheorems \emph{about} the logic, but also theorems \emph{in} my
logic.

\subsection{Design Patterns}

One prominent source of examples of higher-order state arises in
object-oriented programs. One way of translating objects into
functional language is by viewing objects as records of functions
(methods) accessing common hidden state (fields). As a result, common
programming idioms in object-oriented languages can be viewed as
patterns of higher-order imperative programs.

Over the years, object-oriented programmers have documented many of
idioms which repeatedly arise in practice, calling them \emph{design
  patterns}~\cite{gof}. While originally intended to help practitioners
communicate with each other, design patterns also offer a nice collection 
of small, but realistic and widely-occurring, programs to study for
verification purposes. 

In Chapter 4, I translate many common design patterns into my
programming language\footnote{Unsurprisingly, they turn into idioms
  that many ML programmers will find familiar.} and then give
specifications and correctness proofs for these programs. I want to
emphasize that I view the specifications as even more important a
contribution as the correctness proofs themselves: programmers have
informal reasons for believing in the correctness of their programs,
which are often surprisingly subtle to formalize. 

\subsection{The Union-Find Algorithm}

One of the reasons for the success of separation logic (and related
substructural logics like linear logic) in program verification is the
fact that aliasing turns out to be only used lightly in many
programs. 

In Chapter 5 of my dissertation, I study the
union-find~\cite{union-find} algorithm. This algorithm is very
challenging to verify, because its correctness and efficiency relies
intimately upon the ability to use shared mutable data to broadcast
updates to the whole program.

To deal with this problem, I make use of the expressive power of
higher-order separation logic, and introduce a \emph{domain-specific
  logic} to describe the interface between a union-find library and
its clients. This allows the implementation to keep a global view of
the union-find data structure, while still permitting clients to be
proved via local reasoning.

Furthermore, those operations which have genuinely global effects
(such as taking the union of two disjoint sets) can be specified using
custom modal operators I call \emph{ramifications}. These operators
commute with the custom separating conjunction, and so allow local
reasoning even when global effects are in play.

\subsection{Verifying Event-Driven Programs}

In Chapter 6, I give the culminating examples of the proof methodology
I have developed in support of my thesis. This chapter divides into 
two main parts. 

In the first part, I implement and prove the correctness of an
imperative dataflow graph implementation. In a dataflow graph, we have
a collection of cells, which each contain a piece of code, which is
executed when the cell is asked for a value. Cells memoize their
values and remember their dependencies, so that when they are asked
again for a value, they will not unnecessarily recompute a
value. Conversely, if a cell is modified, it will invalidate its
memoized value and transitively invalidate anything that depended on
that value.  The specification of the dataflow graph relies crucially
on ramifications in order to modularly specify dependencies and the
effects of modifications to the graph.

Then, in the second part, I use this dataflow graph implementation to
in turn implement a library for functional reactive
programming~\cite{frp}. I prove the correctness of the imperative
implementation relative to a mathematically simple (but unusably
inefficient) semantics somewhat in the style of realizability, which
allows programmers to reason about the imperative implementation as if
it were a pure one. 

Furthermore, this correctness proof relies only on the interface to
the the dataflow graph library, and not to its implementation. This
demonstrates that very sophisticated libraries can still be factored
into modular parts, and moreover that the correctness proofs can be
cleanly factored as well. 

\section{Related Work}

\subsection{Immediate Predecessors}

There are two immediate predecessors to the proof system in this
dissertation. 

First, there is the work of Reynolds on specification logic for
Algol~\cite{spec-logic}. Idealized Algol~\cite{idealized-algol}
combines a call-by-name functional language, together with a type of
imperative computations whose operations correspond to the
while-language (i.e., it does not have higher-order store). This
stratification ensures that Algol features a powerful equational
theory which validates both the $\beta$- and $\eta$-rules of the
lambda-calculus. 

Specification logic extends Hoare logic~\cite{hoare-logic} to deal
with these new features, by viewing Hoare triples as the atomic
propositions of a first-order logic of specifications. Then, the user
of the logic can specify the behavior of a higher-order function using
an implications over triples, using the hypothesis of an implication
to specify the behavior of arguments of command type. In short,
specification logic can be viewed as a synthesis of LCF~\cite{lcf} with
  Hoare logic.

One of the motivations for the language design in my dissertation was
to see if the analogy between the computation type in Algol and the
monadic type discipline could be extended to a full higher-order
programming language, with features like higher-order store. This has
worked fantastically well. The semantics of the logic of this
dissertation is (suprisingly) \emph{simpler} than original
specification logic, even though the programming language itself is a
much more complex one than Idealized Algol.

In particular, one of the main sources of complexity in usages of
specification logic has simply vanished from this logic. Algol has
assignable variables (like C or Java, though of course this reverses
the chronology), and so specification logic had to account for their
update with assertions about so-called ``good variable''
conditions. Since ML-like languages do not have assignable variables,
this means that all aliasing is confined to the heap. As a result, the
entire machinery of good variables is simply absent from my system.
So all aliasing and interference is confined to the heap, and
separation logic works quite well for specifying and reasoning about
this interference.

The other line of work I make use is on algebraic models of separation
logic. I give a very concrete model of separation logic in Chapter 3,
and am able to interpret higher-order quantification due to the fact
that the lattice of propositions is complete. This is probably best
understood as an instantiation of Biering~\emph{et al.}'s
hyperdoctrine model of higher-order separation logic. Likewise, my
semantic domain for specifications makes use of the techniques
introduced by Birkedal and Yang~\cite{birkedal-yang} to model
higher-order frame rules.

Happily, though, most of this machinery stays ``under the hood'' to
ensure that the logic looks simple to the user.  As an example of this
phenomenon, we use TT-closure~\cite{tt-closure} to force the
admissibility of Hoare triples. This lets us give a simple rule for
fixed-point induction, without having to specify conditions on
admissible predicates (which is especially problematic in a setting
with predicate variables).

Finally, nested triples are very useful for specifying the properties
of higher-order programs. It is very useful to be able put the
specification of the contents of a reference into an assertion.  In my
dissertation, I have given a ``cheap and cheerful'' embedding of
assertions in specifications and vice-versa, where the embedding (in
either direction) sends the topmost element of the source lattice to
the topmost element of the target lattice, and maps all other lattice
elements to the bottom element. This approach has the virtues of
first, being very technically easy to implement, and second,
sufficient for all of the examples I have considered.

However, more sophisticated approaches are certainly
possible. \citet{nested-hoare-triples} describe how to unify the
assertion and specification languages into a single logic.  This
offers the technical benefit that it extends the power of the
higher-order frame rule by letting it operate in a hereditary way, to
act on nested triples contained within assertions. In contrast, the
frame rule I give stops at the boundary of an assertion. 

Despite looking, though, I have not yet found any programs that seem
to need this extra generality to verify. Most of the programs I have
considered which might benefit from these features seem to also need
further generalizations, such as Liskov and Wing's monotonic
predicates. Indeed, this is the case even for the higher-order frame
rule: most of the results in this dissertation could have been proven
without it. However, the Kripke view made it no harder to support than
the first-order frame rule.

\subsection{Program Logics and Type Theories}

While there is a vast literature on verification of imperative
programs, three lines of research in particular stand out as most
closely related to my own work.

\subsubsection{Separation Logic for Java}

In his dissertation, Parkinson~\cite{parkinson-thesis} gives a program
logic for Java, which, on the surface, closely follows the standard
methodology of separation logic --- that is, it is a pure Hoare logic
with triples as the fundamental device for structuring program proofs
(and no logic of triples, as in specification logic). However, a
deeper look reveals a number of innovations which permit an elegant
treatment of many aspects of object-oriented programming. Since
object-oriented programming is fundamentally higher-order imperative
programming, there are many ideas in Parkinson's dissertation which
are either related to or simply reminiscent of the ideas in my work.

As in this work, Parkinson builds a high-level separation logic. That
is, his model of the heap consists of a collection of structured
objects, rather than as a simple giant byte array. As in my work,
pointer arithmetic is impossible, and pointer point to arbitrary
values.

Unlike in my work, all specifications are Hoare triples (there is no
specification logic which treats Hoare triples as atoms), and
furthermore his program logic supports the semantic subtyping rule
(aka the Liskov substitution principle). Now, it is well-known that
the straightforward treatment of semantic subtyping essentially
reduces object-oriented programs to first-order programs and prevents
writing essentially higher-order functions (such as $\mathsf{iter}$ or
$\mathsf{map}$). 

To avoid this trap, Parkinson made two key innovations. First, he
makes use of abstract predicates (i.e., second-order quantification
over predicates), which permits writing method specifications which
relate the footprint of a method to the footprint of the methods of
the objects it receives as an argument.

Second, he gives his second-order predicates a very unusual semantic
interpretation. The idea, described in Parkinson and
Bierman~\cite{parkinson-bierman-05}, is to index each predicate symbol
by an object, and to permit the concrete definition of a predicate to
\emph{dynamically dispatch} based on the class of the receiver
object. That is, each class in an object hierarchy may define the
implementation of an abstract predicate differently, and the
definition used actually depends on the concrete class of the
object. As a result, semantic subtyping can be formally respected,
without blocking the use of objects as higher-order parameters: a
family of objects used as higher-order parameters can simply define
different concrete definitions of a common predicate symbol.

This design is both clever and elegant, since it puts the natural
style of specifications in alignment with the style of object-oriented
programming. Unfortunately, the use of dynamic dispatch means that it
is difficult to figure out what higher-kinded predicate symbols ought
to mean.  This turns out not to be a fundamental problem for
Parkinson, since the natural style of Java programming is rather like
writing the defunctionalized version of a higher-order program. As a
result, he rarely needs fully higher-order specifications --- he can
attach specifications to each class implementing an interface for a
higher-order method.

However, I work with a genuinely higher-order programming language,
and this solution does not work for me. Instead, I make use of a
standard semantics of predicates in separation logic, which makes
interpreting predicates of arbitrary order straightforward. As a
result, I have no problems with higher-order specifications. A good
example of this difference can be seen by contrasting our respective
proofs of the subject-observer pattern, which we simultaneously and
independently gave correctness proofs for.

From a distance, the specifications look very similar, with the
differences appearing on close inspection. Namely, in Parkinson's
version of the subject-observer pattern~\cite{parkinson-iwaco-07}, for
each subject class, there is a common observer interface that all
concrete observer classes must implement, and the subject predicate is
indexed by a list of observers of this interface. He then uses his
predicate dynamic dispatch to allow each observer to have a different
real invariant. In my version of the subject-observer pattern (in
Chapter 4), the subject predicate is directly indexed by a list of
observer actions plus their invariants and specifications, directly as
a higher-order predicate.

The ability to make use of full higher-order logic turns out to be
very important for more complex examples. This initial verification of
the subject-observer pattern turned out to be inadequate for real use,
which lead to my development of embedded separation logics for
verifying programs making use of heavily aliased mutable state (as in
Chapter 5 and 6). There, I make free use of the full machinery of
higher-order logic to define custom logics as index
domains. Furthermore, the specification in chapter 6 relies
essentially on the ability to work compositionally with higher-order
programs, without having to defunctionalize a whole program.

\subsubsection{Hoare Type Theory}

Nanevski, Morisett and Birkedal have developed Hoare Type
Theory~\cite{htt, nanevski08}, a sophisticated dependently-typed
functional language that, like our system, uses a monadic discipline
to control effects.  Unlike my work, HTT takes advantage of type
dependency to directly integrate specifications into the types of
computation terms, by using an indexed family of monads to represent
computations. A single term of computation type has a type $e :
\spec{P}{-}{a:A}{Q(a)}$, where $P$ and $Q$ are predicates on the heap.
Similarly to my own use of existentials, Nanevski, Ahmed, Morisett and
Birkedal~\cite{abstract-htt} have also proposed using the existential
quantification of their type theory to hide data representations,
giving examples such as a malloc/free style memory
allocator. \citet{nanevski-victor-10} also give a proof of a modern
congruence closure algorithm, illustrating that this system can be
used to verify some of the most complex imperative algorithms known.

The key difference between the two approaches can be summarized by
saying that my work in this thesis is to construct a higher-order
predicate logic \emph{about} programs in a non-dependently programming
language, whereas HTT \emph{embeds} imperative programs into a
dependently typed language. That is, the distinction lies in whether
or not specifications are considered part of the language or not.
(Note that the other two combinations are also reasonable: embedding
computations into a non-dependent language basically amounts to a
monadic type discipline. Specification languages without rich support
for quantifiers or higher-order features takes us into the domain of
program analysis, in which automated tools prove weak theorems about
programs. However, neither of these allow rich specifications of
program behavior.)

In principle, embedding computations into a dependently-typed language
should offer greater flexibility than separating specifications from
programs. However, the specific choices made in the design of HTT mean
that this promise goes unrealized: there are natural classes of
higher-order imperative program which are impossible to write in
HTT. Furthermore, I give correctness proofs of such programs, with not
especially difficult proofs, which illustrates that the difficulty is
not fundamental.

To understand the issue, recall that computations in HTT have types $e
: \spec{P}{-}{a:A}{Q(a)}$, where $P$ and $Q$ are predicates on the
heap. Furthermore, heaps map locations to values, and heap predicates
map heaps to propositions. Now, any higher-order imperative program
--- i.e., any program which manipulates pointers to code --- needs to
refer to terms of computation type in its pre- and post-conditions.
In the original formulation of HTT, which is predicative, this causes
size issues to arise, since a heap at universe level $n$ can only
contain commands which manipulate heaps at universe level $n-1$ or
lower. As a result, examples such as the subject-observer example in
Chapter 4 or the dataflow network example of Chapter 6 cannot be
written in predicative HTT. 

This difficulty has been overcome by Petersen~\emph{et al.}, who give
an impredicative model of Hoare Type Theory. However, Svendsen
[personal communication] says that in an impredicative setting, the
weaker elimination rules for impredicative existentials made porting
the proofs in my system to HTT much more difficult. I do not presently
know whether the problem is fundamental, or whether there is a more
intricate encoding which can avoid the obstacles.
 
These issues simply do not arise in my system, since specifications
are strictly separated from the types. As a result, there are never
any size issues arising from the problem of storing computations in
the heap. 

(This also suggests that an interesting future direction would be to
study a version of Hoare Type Theory in which there is an ordinary
monadic type of computations, together with a predicate on terms of
monadic type which indexes them with pre- and post-conditions.)

\subsubsection{Regional Logic and Ownership}

In addition to systems based on separation, there is also a line of
research based on the concept of object invariants and ownership.  The
Java modeling language (JML)~\cite{jml} and the Boogie
methodology~\cite{boogie} are two of the most prominent systems based
on this research stream. In Boogie, each object tracks its owner
object with a ghost field, and the ownership discipline enforces that
the heap have a tree structure. This allows the calculation of frame
properties without explosions due to aliasing, even though the
specification language remains ordinary first-order
logic. \citet{banerjee-naumann-regions} give a logic, which they name
``regional logic'', which formalizes these ideas in a small core logic
more tractable than industrial-strength systems like JML or Boogie.

For ``by-hand'' proofs, approaches based on ownership tend to be more
onerous than proofs using separation logic, since footprints have to
be tracked explicitly in pre- and post-conditions. However, the flip
side of this is that there are no substructural quantifiers, and this
can enable an easier use of existing theorem provers for automation. 

From the semantic point of view, one of the most interesting features
of this line of work is that \citet{banerjee-naumann-rep} were able to
use an ownership discipline to prove a representation independence
(i.e., relational parametricity) result for Java-like programs.  This
is something that neither my system, nor any of the other ones
described earlier is capable of.  The equality relation I use is
simply the equality inherited from the (highly non-abstract)
denotational semantics I give.

\subsection{Program Proofs for Design Patterns}

\subsubsection{Iterators}

In his dissertation~\cite{parkinson-thesis}, Parkinson gave as an
example a simple iterator protocol, lacking the integration with
composites we have exhibited.  Subsequently, we formalized a similar
account of iterators~\cite{iterator}, again lacking the integration
with composites. 

Jacobs, Meijer, Piessens and Schulte~\cite{iterators-revisited} extend
Boogie with new rules for the coroutine constructs C\# uses to define
iterators. Their solution typifies the difficulties ownership-based
approaches face with iterators, which arise from the fact that
iterators must have access to the private state of a collection but
may have differing lifetimes. This work builds on Barnett and
Naumann's generalization of ownership to friendship~\cite{friends},
which allows object invariants to have some dependency on non-owned
objects. 

Unlike the work in my thesis, this required an extension to the core
logic, rather than being a formula provable within the logic. 

\subsubsection{Flyweights}

Pierik, Clarke and de Boer~\cite{creational-invariants} formalize
another extension to the Boogie framework which they name
\emph{creation guards}, specifically to handle flyweights. They
consider flyweights an instance of a case where object invariants can
be invalidated by the allocation of new objects, and add guards to
their specifications to control allocation to the permitted cases.

Again, this required an extension to the core logic, though in this
case the extension was quite modest, since footprints are already
explicitly tracked. 

\subsubsection{The Subject-Observer Pattern}

The subject-observer pattern has been the focus of a great deal of
effort, given its prominence in important applications. Simultaneously
with our own initial formulation, Parkinson gave an example of
verifying the subject-observer
protocol~\cite{parkinson-iwaco-07}. Recently, Parkinson and
Distefano~\cite{jstar-parkinson-distefano} have implemented a tool to
verify these programs, and have demonstrated several examples
including a verification of a subject-observer pattern specified along
these lines. The tool includes automatic generation of loop
invariants.

The style of invariant in our work and Parkinson's is very similar,
and subject to similar limitations. Since each subject needs to know
what its observers are, verifying programs with chains of
subject-observers is extremely cumbersome. This is especially
problematic given that GUI programs --- which are one of the primary
uses of the subject-observer pattern --- rely upon chains of subjects
and observers.

The work of Barnett and Naumann is also capable of reasoning about the
subject-observer pattern, but only if all of the possible observers
are known at verification.  Leino and Schulte~\cite{boogie-sub-obs}
extended Boogie with Liskov and Wing's concept of history invariants
or monotonic predicates~\cite{liskov-wing} to give a more modular
solution. The idea in this work is to require the changes that a
subject makes to ``increase the truth'' of the observer's predicate
along some partial order. This is less flexible than the approach we
took, though perhaps a little easier to use when it is
applicable. Unfortunately, this work is not merely heavyweight, but
entirely inapplicable for event-driven programs such as GUIs, since
there is no natural partial order on user actions.

More recently, Shaner, Naumann and Leavens~\cite{ShanerLN07} gave a
``gray-box'' treatment of the subject-observer pattern.  Instead of
tracking the specifications of the observers in the predicate, they
give a model program that should approximates the behavior of any
actual notification method. This works as long as the runtime
dependencies are known statically enough to include them in the model
programs --- again, a limitation which is problematic in the case of
GUIs.

In Chapter 6, I give a better specification of callbacks which is
modular and handles chains of notifications gracefully. This
specification is very much unlike any of the other specifications for
subject-observer that I have seen. 




% \subsection{Imperative Modularity}
% 
% While separating modules and clients in this way is obviously useful,
% for imperative programs it is not entirely a sufficient mechanism on
% its own.
% 
% In a purely functional program, all communication between an
% implementation and a client of a module is mediated by the signature
% of the module. The addition of state makes it possible to create an
% extra channel of communication through the heap, and in our formalism
% this is captured by adding pre- and post-conditions to the
% specifications of imperative actions.
% 
% However, suppose we have a client program with specification $C$,
% which makes use of a particular module implementation with a
% specification $S$. Furthermore, suppose that \emph{both} the client
% program implemeting $C$, and the module implementing the $S$
% interface, rely upon yet another module with interface $F$. As an
% example, imagine an $F$ that has a purely functional interface, but
% which makes use of an imperative cache for efficiency reasons (for
% example, to implement memoization). Now, even though there is aliased
% state between $C$ and $I$, they cannot interfere with one one another
% through it.
% 
% Therefore, a properly modular specification \emph{should not require}
% $S$ to mention the use of the caching module. Otherwise we will be
% forced to mention more and more extraneous details, as we layer
% modules on top of one another, with the total size of the
% specifications growing with the size of the reachable module graph.
% 
% The language I am proposing is able to properly describe this
% situation, thanks to our ability to use the frame rule. Concretely,
% suppose we have the following interface for our third module.
% 
% \begin{tabbing}
% $\forall n:\N.\; \spec{cache}{\mathtt{fib}(n)}{a:\N}{cache \land a = fibonacci(n)}$ \\
% \end{tabbing} 
% 
% Here, we can imagine \texttt{fib} produces fibonacci numbers using the
% naive exponential-time recursive implementation, but adds caching to
% ``optimize'' it to linear time. The $cache$ predicate appears in the
% pre- and post-conditions of $\mathtt{fib}$, so at first it would seem
% that a $C$ and $S$ that used this function would have to mention that
% they require the heap associated with $cache$.
% 
% However, note that $cache$ is the same in both the pre- and
% post-condition. This means that we can prove $C$ and $I$ using a
% specification 
% $$F \equiv \spec{\emp}{\mathtt{fib}(n)}{a:\N}{\emp \land a =
%   fibonacci(n)}$$
% and then we can frame $cache$ on at the very end, after
% completing the proofs of $C$ and $I$. Concretely, the proof tree 
% looks like this:
% 
% \begin{mathpar}
% \inferrule*[right=Cut]
%            {\vdash F \otimes cache \\ 
%             \inferrule*[right=Frame]
%                        {\inferrule*{F \vdash S \\
%                                   F, S \vdash C}
%                                  {F \vdash C}}
%                       {F \otimes cache \vdash C}
%           }
%           {\vdash C \otimes cache}
% \end{mathpar}
% 
% Here, I write $R \otimes p$ to suggest that $p$ is framed onto the
% triples in the specification $R$. Note that in the subderivation above
% the use of the Frame rule, we do not need to carry around $cache$ in
% our specification of $\mathtt{fib}$, which means that when we
% introduce $S$, we will not need to mention $cache$ in its specification.
% 
% Simple caching schemes are very easy to hide -- all we need is to make
% use of the frame rule~\cite{sep-inf}. A stronger test of our ability
% to hide irrelevant detail will be if we can hide Liskov and Wing's
% \emph{monotonic constraints}~\cite{liskov-wing}.
% 
% Monotonic invariants are program invariants that hold monotonically
% throughout a program's execution. Suppose we have a partial order $(S,
% \sqsubseteq)$ and a predicate $P$, such that if $s' \sqsubseteq s$
% then $P(s') \implies P(s)$. Now, if we then ensure that any triples
% that mention $P$ are of the form $\spec{P(s)}{c}{a:\tau}{\exists s'.\;
%   P(s') \land s' \sqsubseteq s}$, then we know that no unknown uses of
% this module can break our invariant, since they can only make $P$ ``more
% true''. 
% 
% This property is used in several verification methodologies to prove
% programs with potentially unknown calls~\cite{boogie-sub-obs}, but 
% using it for information hiding purposes may be novel. 
% 
% Related to monotonic effects are \emph{commutative effects}~\cite{idioms}. 
% These are side-effects in which the order they happen in does not affect
% correctness. For example, a gensym operation is a commutative effect, since 
% all we care about is the fact that two different invocations return distinct
% symbols. 

% \section{Interesting Higher-Order Imperative Programs}
% 
% Validating my thesis means that I have to demonstrate that it will
% work on interesting imperative programs. So what do I mean with the
% word ``interesting''? There are several senses in which I use it:
% 
% \begin{itemize}
% \item One kind of interesting program are programs that pose
%   particular difficulties for verification, such as ``Landin's knot'',
%   which implements recursion by backpatching a function pointer. These
%   kinds of procedures are typically small, and may not be found in
%   actual software very often\footnote{Recently, I found a natural use
%     of this technique in a real program, so I do not mean ``not very
%     often'' as a polite way of saying ``never''.}, but help
%   demonstrate the capabilities and limits of a verification
%   methodology.
% 
% \item A second kind of interesting program are example programs that
%   capture a particular stylized patterns of use that arise frequently
%   in practice. Design patterns~\cite{gof} are a good example of this
%   kind of program -- they represent what might be called engineering
%   wisdom, and formalizing these patterns can help demonstrate that our
%   verification methodology can accomodate typical good design, and
%   moreover its formal version is not overly cumbersome to use. 
% 
% \item A third kind of interesting program are big programs. Even if a
%   verification methodology works successfully on small examples, it
%   remains to be seen whether it will work on larger examples, because
%   in a larger example we will have to show that the different patterns
%   of use can coexist peacefully, and that they compose gracefully. 
% 
%   Also, I only mean ``big'' relative to the size of typical examples
%   (which are usually in the tens of lines of code). I will be happy
%   with verifying a program that is on the order of high hundreds to
%   low thousands of lines of code. This is, of course, very small by
%   industrial standards, but it is large enough to learn whether or
%   not the different pieces of this methodology fit together. 
% \end{itemize}
% 
% 
% \subsubsection{Design Patterns}
% 
% I will begin by analyzing individual design patterns, giving a
% specification, an implementation, and a small client program for each
% of the design patterns with a substantially stateful part.
% 
% The intuition here is that 1) object-oriented software is the biggest
% repository of higher-order imperative code out there, and 2) the Gang
% of Four design patterns are well-respected engineering wisdom about
% how to manage such designs. So if I can show that I can formalize that
% practice, I'll have taken a big step towards showing I can formally
% capture the sorts of reasoning that software engineers use to manage
% such programs.
% 
% Furthermore, there's also a \emph{negative} component here. I want to
% figure out what patterns of reasoning are difficult to capture in my
% framework, and systematically analyzing the design patterns should
% help me search the design space.
% 
% One further thing that's worth noting is that a design pattern
% \emph{isn't} a particular formula of specification logic. Any
% specification we write represent particular balances between
% generality (that is, more possible implementations) and specificity
% (that is, more invariants for a client to use). For example, suppose
% we have an iterator specification which supports deletion. For some
% collection types (such as stretchable arrays), deleting an element
% invalidates only those iterators that have are further along the array
% than the one that did the deletion. If we put this fact into the
% specification, then clients can use this fact to not invalidate some
% iterators. But the price is that this specification rules out
% collection implementations such as binary trees, where any deletion
% will invalidate all other iterators.\footnote{This means that any spec
% will be criticized by both those who think it is too liberal, and
% those who think it is too restrictive. I think this is a feature,
% not a bug -- making assumptions explicit in a way that permits
% detailed argument is a big win!}
% 
% 
% \begin{itemize}
% \item[Iterator]
% \item[Subject Observer]
% 
% These two I have written about extensively. I need to pull in the text
% from those here.
% 
% \item[Flyweight]
% 
% The basic idea here is similar to memoization: cache the construction
% of (immutable) objects, and on an object request only return a new one
% if a satisfactory one is not in the cache. The cache, of course, is a
% global mutable data structure that \emph{we don't want to see} in the
% proof of any clients. This makes it a good use case for the frame
% rule, and the issues that arise here will be a good test of whether we
% can actually scale proofs.
% 
% \item[Memento]
% 
% This pattern is commonly used to support undo. It's basically an
% opaque token representing an object's old state, and the object has a
% method that the client can use (with the memento) to reset the state.
% We can implement this with higher-order state -- the token can be a
% command the user can invoke, and there are some nice sequencing issues
% in when you can invoke the undo operation or not. We should also look
% at multilevel undo, and compare this to an internal undo operation.
% 
% \item[Chain of Responsibility]
% 
% The Chain of Responsibility pattern is a pattern for composing
% commands from other commands. Rather than building a monolithic
% command, we break it up into processing units that take a message and
% handle the parts of the update they can, delegating the rest to the
% rest of the chain. This doesn't have to be a strict chain, so you can
% have a ``tree of responsibility'' as well.
% 
% There's probably a very slick specification of this in terms of state
% transformers, and I think there might be a nice ownership transfer
% idiom lurking in here that's worth unearthing. Then again, maybe not
% -- I've had trouble finding any really complex uses of aliasing yet.
% 
% \item[State]
% 
% This is a pattern in which an object is made from other objects which
% provide abstract state -- it does not know the precise implementation
% of its components. In fact, it's okay for the implementation of the
% concrete state to \emph{change} as the program runs. Verifying this
% example is primarily useful as a diagnostic test; of course we can
% handle it, but even the slightest hint of ugliness in our development
% indicates trouble in the logic.
% 
% \item[Composite]
% 
% At first, I thought that the Composite pattern didn't have any
% interesting issues, but then I realized I was wrong, because we are
% often composing \emph{stateful} objects. For example, consider an
% iterator library (such as the \texttt{itertools} library in Python)
% which constructs new iterators out of existing ones.
% 
% Concretely, suppose we have \texttt{PairIterator} that takes two
% iterators and returns a new one that produces pairs of values,
% iterating over the two arguments in lockstep. This seems
% straightforward, until you think that this implies certain sharing
% constraints -- in particular, the arguments have to be two
% \emph{different} iterators.
% 
% So we need to be able to say something about sharing here. This may be
% easiest if we use multiple specifications for the same function (for
% example, if the two iterators come from the same underlying
% collection, or not).
% 
% \item[Proxy] 
% 
% The Proxy pattern does not have very much complexity in its
% implementation; it's essentially just a (possibly-imperative) wrapper
% function -- its ML type could be \texttt{proxy : (a -> b) -> (a -> b)}.
% However, analyzing it will still offer some interesting evidence for 
% or against my thesis. That's because at heart a Proxy transforms 
% the specification of its argument, and we can examine how hard it is
% to model that. 
% 
% Note that state can play an important in implementing a real
% Proxy, because it provides us with a ``back channel'' for
% communication.  For example, a security Proxy that checks to see
% whether the owner of a piece of data is authorized before invoking a
% method might have a data structure it consults for that authorization
% information.
% 
% The Decorator pattern is similar to this, only we can add methods
% to our input in addition to transforming old ones. 
% 
% \item[Strategy]
% 
% In a language with first-class functions, there's not much to the
% Strategy pattern -- we just need a pointer to a function. But the very
% simplicity of this pattern means that we ought to be able to prove
% some fun programs, like showing the correctness of a program that
% updates its strategy as it runs.
% 
% 
% \item[Facade]
% 
% The Facade pattern takes some existing modules and presents a nice
% abstraction of them to the programmer. The way I conceptualize it, we
% want to compose imperative modules in a way that turns their state
% into a single new predicate. This is pretty straightforward in easy
% cases, but I'm interesting in finding out if there's a way to make
% this \emph{hard} I'd like to stress-test our ability to abstract over
% state.
% 
% In particular, the test of whether this works will be seeing how big
% the predicates grow. If I composing two modules with $n$-place
% predicates describing their heap yields a Facade with a $2n$-place
% predicate, it's pretty clear that information hiding isn't genuinely
% happening. This contrasts with the Flyweight, where the test of
% modularity is seeing how many starred subformulas show up in the
% pre- and post-conditions. 
% 
% \end{itemize}
% 
% \section{Contributions}
% 
% To demonstrate my thesis, I make a number of concrete contributions.
% In Chapter 2, I define a higher-order imperative programming language
% based on a predicate variant of $F_\omega$~\citep{fomega}, augmented
% with reference types which uses a monadic language to encapsulate its
% side-effects (including both modification of the heap, and
% nontermination). I give this language a domain-theoretic denotational
% semantics based on the techniques of \citet{smyth-plotkin}, which lets
% me validate strong equational reasoning principles --- including both
% the $\beta-$ and $\eta$-rules of the lambda calculus ---
% 
% Equational reasoning is less helpful for imperative programs, and to
% support reasoning about this part of the programming language, I
% define and prove the soundness of a program logic in Chapter 3. The
% program logic combines ideas from specification logic and higher-order
% separation logic to give an expressive program logic 
